<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.LG](#cs.LG) [Total: 82]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 论文提出了一种AI系统，通过Truth Sleuth和Trend Bender两个代理，分别进行视频内容的事实核查和评论区互动，以对抗YouTube上的错误信息。


<details>
  <summary>Details</summary>
Motivation: 错误信息在数字世界中快速传播，威胁社会，需要高效工具进行干预。

Method: 系统结合RAG方法提取视频声明并验证，生成报告后由Trend Bender生成互动评论，通过自评估循环优化。

Result: 实验显示系统在事实核查和用户互动方面高效，能影响观点。

Conclusion: AI驱动的干预在对抗错误信息和促进知情讨论方面具有潜力。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [2] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp是一款基于智能手机的离线对话应用，旨在解决心理健康支持中的可访问性和隐私问题，利用LLMs技术实现本地推理。


<details>
  <summary>Details</summary>
Motivation: 数字心理健康平台面临用户可访问性、网络连接和数据隐私的挑战，需要一种离线解决方案。

Method: 使用Torchtune和Executorch对LLaMA-3.2-1B-Instruct模型进行微调和量化，部署到资源受限设备上，并结合自定义心理健康数据集。

Result: 通过学生群体的定性评估，EmoSApp能提供连贯、共情的对话和相关建议；量化评估显示模型在低资源环境下有效。

Conclusion: EmoSApp为便携、安全和定制化的AI心理健康解决方案提供了范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [3] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 该论文提出了一种模块化工具链，用于处理法律、医疗和行政领域的非结构化文本数据，通过本地运行的开放权重模型实现标准化、匿名化和嵌入分析，解决了隐私和异构性问题。


<details>
  <summary>Details</summary>
Motivation: 法律、医疗和行政领域的非结构化文本数据具有丰富的研究价值，但因隐私和语言结构异质性而难以大规模分析。

Method: 使用大型语言模型（LLM）进行文本标准化、摘要和翻译，结合命名实体识别和规则方法实现匿名化，并通过嵌入向量进行文档分析。

Result: 在瑞典法院的10,842份判决书中验证了工具链的有效性，成功去除了敏感信息并保留了语义内容。

Conclusion: 该工具链为隐私敏感领域的大规模文本分析提供了新可能。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [4] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 论文提出了一种针对自然语言解释（NLEs）的更新版XAI分类法，以支持透明AI系统的治理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，NLEs成为解释模型行为的关键工具，需要对其特性和治理影响进行系统研究。

Method: 基于可解释AI（XAI）文献，构建了一个适应提示型NLEs的三维分类法，涵盖上下文、生成与呈现、评估三个维度。

Result: 分类法为研究人员、审计者和政策制定者提供了一个框架，用于描述、设计和改进NLEs。

Conclusion: 该分类法有助于提升AI系统的透明性，支持有效的AI治理。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [5] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA是一个模块化框架，通过轻量级LoRA适配器和KL正则化训练减少大语言模型中的幻觉问题，同时保持模型效率和模块化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自然语言任务中表现出色，但容易产生幻觉（事实错误），影响实际部署中的可信度。

Method: 结合自动提示重写、混合检索和低秩适配器调优，通过KL正则化训练和反馈校正循环来减少幻觉。

Result: AutoRAG-LoRA显著减少了事实漂移，同时保持了模型的效率和模块化。

Conclusion: 该框架有效解决了LLMs中的幻觉问题，提升了生成内容的可信度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [6] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 论文探讨了如何通过语言模型（LLMs）向用户传达不确定性，以增强信任和合法性，提出了“拟人化不确定性”的概念，并呼吁未来研究关注这一方向。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型输出过于自信，即使准确性存疑，这削弱了其可信度。需要通过语言手段传达不确定性，以促进人机协作并减少潜在危害。

Method: 通过综述人类不确定性沟通的研究，分析现有数据偏见，并提出拟人化不确定性的概念，即模仿人类沟通方式传达不确定性。

Result: 研究发现当前自然语言处理（NLP）研究忽视了人类不确定性沟通的细微差别和数据偏见，需进一步探索拟人化不确定性的实现。

Conclusion: 论文呼吁未来研究关注人机不确定性沟通的独特性，并将拟人化不确定性分解为具体研究方向。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [7] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: PLEX是一种无需扰动的局部解释方法，通过利用LLM的上下文嵌入和Siamese网络，显著提高了解释效率，与LIME和SHAP效果相当但计算成本更低。


<details>
  <summary>Details</summary>
Motivation: LLMs在文本分类中表现优异，但缺乏可解释性，现有XAI方法（如LIME和SHAP）计算成本高。

Method: 提出PLEX方法，利用LLM的上下文嵌入和Siamese网络，避免扰动，实现高效解释。

Result: 在四个分类任务中，PLEX与LIME和SHAP的一致性超过92%，且计算效率提升显著。

Conclusion: PLEX为LLM的文本分类提供了一种高效且可解释的解决方案。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [8] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）能自然形成与人类心理模型一致的分层情感树，且模型越大，情感层次越复杂。同时，模型在情感识别中存在社会经济角色的系统性偏见，对交叉性、少数群体的误分类更严重。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在对话代理中的广泛应用，理解其如何建模用户情感状态对伦理部署至关重要。

Method: 基于情感轮（一种心理学框架），分析模型输出中情感状态的概率依赖关系。

Result: LLMs形成分层情感树，与人类心理模型一致；大模型情感层次更复杂；情感识别存在社会经济偏见。

Conclusion: 研究揭示了LLMs的情感推理能力，并建议基于认知理论开发更好的模型评估方法。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [9] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 该论文研究了成人服务网站（ASW）广告文本的语言建模方法，以帮助识别性交易受害者。通过定制化Transformer模型，在资源有限的情况下实现了优于预训练模型的性能，并应用于ASW数据分析任务。


<details>
  <summary>Details</summary>
Motivation: 成人服务网站（ASW）广告文本是识别性交易受害者的重要数据来源，但其文本分析面临挑战（如表情符号、语法混乱和故意混淆）。研究旨在开发高效的文本分析方法。

Method: 比较了多种语言建模方法，包括信息检索、预训练Transformer和定制化Transformer模型。定制化模型在有限GPU资源下训练，并用于ASW数据分析任务。

Result: 定制化Transformer模型在准确性、召回率、F1分数和ROC AUC上优于BERT-base、RoBERTa和ModernBERT等预训练模型。

Conclusion: 定制化Transformer模型显著提升了ASW文本分析能力，可用于多种下游应用和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [10] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练文本嵌入模型增强属性图的语义分析，提升节点分类和关系预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 属性图中的丰富文本属性未被充分利用，希望通过语义分析增强其分析能力。

Method: 嵌入文本节点和边属性，将语言模型嵌入集成到图流程中，不改变图结构。

Result: 文本语义显著提高了属性图分析的准确性和可解释性。

Conclusion: 文本嵌入模型能有效增强属性图的语义分析能力。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [11] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA是首个评估模型解读科学文献中示意图能力的基准，包含1,500个专家标注示例，覆盖465篇论文。评估了18种前沿多模态基础模型，发现与人类专家存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的示意图包含重要信息，但现有模型对其理解能力有限，需专门基准评估和改进。

Method: 构建MISS-QA基准，包含1,500个标注示例，测试模型对示意图的解读能力，并评估18种多模态模型。

Result: 模型表现显著低于人类专家，尤其在无法回答的问题上，详细错误分析揭示了当前模型的局限性。

Conclusion: MISS-QA为提升模型理解多模态科学文献能力提供了关键见解，未来需进一步优化模型。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [12] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 研究发现，社交平台上的仇恨言论并不总是因获得社会认可而加剧，且不同时间尺度下社会认可与仇恨言论的关系存在差异。


<details>
  <summary>Details</summary>
Motivation: 探讨社交认可如何影响在线仇恨言论，验证Walther的社会认可理论。

Method: 分析2018-2021年Parler平台的1.1亿条帖子，研究点赞数与后续仇恨言论的关系。

Result: 点赞数与后续仇恨言论无显著关联，且不同时间尺度下关系复杂。

Conclusion: 社交认可对仇恨言论的强化机制在特定平台上可能表现不同。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [13] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）在司法系统中的公平性问题，提出了一个评估框架，并通过实验揭示了LLMs普遍存在的不一致性、偏见和不平衡的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在司法决策中的公平性，以确保其可信度和社会正义。

Method: 构建了一个包含65个标签和161个值的评估框架，开发了三个评估指标（不一致性、偏见和不平衡准确性），并测试了16个LLMs。

Result: 实验发现LLMs普遍存在不公平性，尤其在人口统计标签上偏见更明显。调整温度参数可影响公平性，但模型大小、发布时间和来源国无显著影响。

Conclusion: 论文提出了一个公开工具包，支持未来研究和改进LLMs的公平性。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [14] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 论文探讨了对话生成中主观与客观风格相似性对用户偏好的影响，发现主观相似性与用户偏好强相关，且与客观相似性存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨用户与系统风格相似性对用户印象的影响，尤其是区分主观与客观相似性的作用。

Method: 方法包括构建新数据集，涵盖用户偏好、主观风格相似性（用户自评）和客观风格相似性（第三方标注），并在开放域对话中分析。

Result: 结果显示主观风格相似性与用户偏好强正相关，且主观与客观相似性存在显著差异。

Conclusion: 结论强调区分主观与客观评价的重要性，以更全面理解风格相似性与用户偏好的关系。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [15] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: HanjaBridge通过注入多义汉字的候选字符，结合持续预训练框架，显著提升了韩语低资源语言模型的性能，并在跨语言迁移中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决韩语中同音异义汉字词在韩文脚本中的语义模糊问题，提升低资源语言模型的性能。

Method: 提出HanjaBridge技术，在持续预训练框架中为同音词提供所有可能的汉字候选，结合知识蒸馏防止灾难性遗忘。

Result: 在KoBALT基准上相对提升21%，并观察到积极的跨语言迁移效果。

Conclusion: HanjaBridge不仅提升了韩语理解能力，还无需推理时的额外成本，具有实际应用价值。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [16] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在类比推理任务中与人类表现的对比，重点关注语义表示和显式提示的效果。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在类比推理任务中的表现，探索其是否具备类似人类的推理能力。

Method: 通过故事类比的映射任务，分析LLMs的语义表示和显式提示的效果，并比较不同模型规模和架构的表现。

Result: 研究发现LLMs在类比推理中表现接近人类，但仍有差距，模型规模和架构对性能有显著影响。

Conclusion: LLMs在类比推理方面有潜力，但仍需改进以更接近人类推理能力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [17] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: DS@GT团队在eRisk 2025挑战中采用提示工程策略，利用LLMs进行BDI-II评估，生成结构化JSON输出，并在无真实标签情况下评估模型一致性和内部一致性。最佳提交排名第二。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在对话式抑郁检测中的应用，并通过提示工程策略优化模型输出。

Method: 采用提示工程策略，让不同LLMs基于BDI-II标准生成结构化JSON输出，并评估模型间一致性和内部一致性。

Result: 最佳提交在官方排行榜上排名第二，指标为DCHR = 0.50，ADODL = 0.89，ASHR = 0.27。

Conclusion: 提示工程策略有效，模型输出与BDI-II标准对齐，并能分析影响症状预测的对话线索。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [18] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign通过微调大语言模型（LLM），将其视为另一种自然语言来处理手语生成，利用逐步提示策略提取LLM中的手语知识，有效对齐手语与口语的分布和语法规则。


<details>
  <summary>Details</summary>
Motivation: 手语生成因复杂性和独特规则受限于大语言模型的应用，研究旨在探索LLM在手语生成中的潜力。

Method: 通过微调LLM并采用逐步提示策略，提取模型中的手语知识，支持文本与手语的对应学习和生成。

Result: 在How2Sign和Phoenix14T数据集上的实验表明，该方法能有效利用LLM的知识和推理能力，对齐手语与口语的差异。

Conclusion: TEAM-Sign为LLM在手语生成中的应用提供了有效途径，展示了模型处理复杂语言规则的能力。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [19] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于分层低秩适应（LoRA）的方法，用于检测英语和西班牙语推文中的性别歧视，通过条件适配器路由显式建模标签依赖关系，并在多个子任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决文本性别歧视检测任务，特别是针对多语言和多子任务的复杂场景，同时提升参数效率和性能。

Method: 采用分层LoRA适配器，应用于所有线性变换，显式建模标签依赖关系，并通过统一的多语言训练策略实现跨语言迁移。

Result: 在仅使用1.67%可训练参数的情况下，性能接近全微调，训练时间减少75%，存储需求降低98%，并在多个子任务中表现优异。

Conclusion: 分层LoRA适配器是一种高效且性能优越的方法，适用于多语言和多任务性别歧视检测。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [20] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2是HUMANE团队在FEVER-25研讨会上的改进系统，通过文档摘要和答案重构提升证据质量，优化真实性预测，并在计算限制下实现高效运行。


<details>
  <summary>Details</summary>
Motivation: 改进去年的最佳开源模型HerO，提升证据质量和系统性能，以适应实际事实验证需求。

Method: 采用文档摘要、答案重构、后训练量化和更新语言模型主干。

Result: 在排行榜上排名第二，同时运行时间最短，表现出高效性和实用性。

Conclusion: HerO 2展示了在事实验证任务中的高效性和潜力，代码已开源。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [21] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 论文介绍了首个韩语文章级立场检测数据集K-News-Stance，并提出了一种基于新闻结构的分段立场检测框架JoA-ICL，以提升长文本立场检测效果。


<details>
  <summary>Details</summary>
Motivation: 在线新闻推荐系统可能加剧信息茧房和政治极化，而现有立场检测研究多限于短文本和高资源语言。

Method: 提出JoA-ICL框架，利用语言模型代理预测新闻关键结构段落的立场，并聚合为整体文章立场。

Result: JoA-ICL在立场检测任务中表现优于现有方法，并能促进新闻推荐的观点多样性和媒体偏见分析。

Conclusion: 分段立场检测方法有助于提升长文本立场检测效果，并支持更全面的新闻分析和推荐。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [22] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 论文提出了一种研究LLMs中毒的框架，揭示了多个后门触发器可以共存且互不干扰，并提出了一种基于层间权重差异的选择性重训练防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs中毒攻击的触发机制和多触发器交互理解有限，论文旨在填补这一空白。

Method: 提出框架研究多触发器共存现象，并通过高嵌入相似性的触发器展示其鲁棒性；提出基于层间权重差异的选择性重训练防御方法。

Result: 发现多触发器可共存且鲁棒激活，暴露LLMs更广泛的漏洞；防御方法能高效移除触发器行为。

Conclusion: 论文揭示了LLMs的多触发器中毒风险，并提出了一种高效防御方法，为未来研究提供了方向。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [23] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 该研究提出了一种基于LLM增强的临床NLP流程，用于从非结构化临床笔记中提取心血管疾病早期指标，并在性能评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病的及时识别和准确风险分层对降低全球死亡率至关重要，现有预测模型主要依赖结构化数据，而临床笔记中的非结构化数据包含重要信息。

Method: 采用领域适应的大语言模型进行症状提取、上下文推理和相关性分析，结合心血管特异性微调、基于提示的推理和实体感知推理。

Result: 在MIMIC-III和CARDIO-NLP数据集上评估，精度、召回率、F1分数和AUROC均有提升，临床相关性高（kappa = 0.82）。

Conclusion: 该研究展示了LLM在临床决策支持系统中的潜力，可推动早期预警系统发展，并将患者叙述转化为可操作的风险评估。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [24] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 该研究提出了一种基于混合Transformer的情感分析框架，用于分析孟加拉国七月革命期间社交媒体上的公众情绪，使用新数据集和多种模型，最终XMB-BERT与投票分类器取得了83.7%的准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过机器学习技术解码低资源语言（如孟加拉语）在重大社会事件（如七月革命）中的公众情绪，填补相关领域的研究空白。

Method: 采用混合Transformer框架（包括BanglaBERT、mBERT、XLM-RoBERTa和XMB-BERT），结合PCA降维和11种机器学习分类器进行情感分析。

Result: XMB-BERT与投票分类器组合表现最佳，准确率达83.7%，优于其他模型。

Conclusion: 研究表明机器学习能有效分析低资源语言的社交媒体情绪，为类似研究提供了新方法。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [25] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 论文探讨了在跨境金融活动中使用大型语言模型（LLMs）进行实体匹配的优越性，相比传统方法，LLMs在准确性和减少误报方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 跨境金融活动的增加使得准确识别和分类外国实体变得至关重要，但传统匹配方法因语言变体和语义复杂性存在局限性。

Method: 研究比较了传统方法（如Jaccard、余弦和Levenshtein距离）、基于Hugging Face的LLMs和接口型LLMs（如Microsoft Copilot、Qwen 2.5），使用65个葡萄牙公司案例数据集进行评估。

Result: 传统方法准确率超过92%，但误报率高（20-40%）；接口型LLMs表现更优，准确率超过93%，F1分数超过96%，误报率显著降低（40-80%）。

Conclusion: LLMs在跨境金融实体匹配中具有显著优势，能够更好地处理语言变体和语义复杂性，提升匹配准确性和效率。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [26] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: DIJA是一种针对扩散式大语言模型（dLLMs）的越狱攻击框架，揭示了其独特的安全漏洞，通过构造对抗性掩码文本提示，利用双向建模和平行解码机制绕过现有对齐机制。


<details>
  <summary>Details</summary>
Motivation: 尽管dLLMs在代码生成和文本填充方面表现优异，但其安全对齐机制无法抵御上下文感知的掩码输入对抗提示，暴露了新的安全威胁。

Method: DIJA通过构造对抗性交错掩码文本提示，利用dLLMs的双向建模和平行解码机制，绕过动态过滤和拒绝采样，导致对齐机制失效。

Result: DIJA在Dream-Instruct上实现了100%的关键词ASR，在JailbreakBench上比ReNeLLM高出78.5%的评估ASR，并在StrongREJECT分数上提升了37.7分。

Conclusion: 研究揭示了dLLMs的安全对齐机制存在严重缺陷，亟需重新设计以应对此类新兴威胁。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [27] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 轻量级OCR-VLM集成系统在ImageCLEF 2025 EXAMS V挑战赛中表现优异，通过多模态推理和多语言增强，结合精确提示策略，取得领先成绩。


<details>
  <summary>Details</summary>
Motivation: 解决多语言多模态推理任务中的性能问题，探索轻量级集成系统在高风险教育场景中的潜力。

Method: 集成Gemini系列模型进行视觉描述、标题优化和推理，结合少样本和零样本提示策略，并进行多语言数据增强。

Result: 系统在官方排行榜中总体准确率达81.4%，并在13种语言中的11种中领先，最高达95.07%（克罗地亚语）。

Conclusion: 轻量级集成系统结合精确提示和多语言增强，可超越重量级端到端模型，适用于多语言教育场景。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [28] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 论文提出了一种新方法WikiMem，用于量化大型语言模型（LLMs）中个人数据的记忆情况，支持动态构建遗忘集以满足GDPR的“被遗忘权”要求。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs可能记忆并泄露个人数据的问题，特别是在GDPR的“被遗忘权”背景下，现有方法无法有效识别模型中的个人数据关联。

Method: 引入WikiMem数据集和模型无关的度量方法，通过校准负对数似然对真实值与反事实值进行排序，评估15种不同规模的LLMs。

Result: 研究发现记忆程度与个体网络存在感和模型规模相关，验证了方法的有效性。

Conclusion: 为LLMs中个人数据的识别提供了基础，支持动态构建遗忘集以满足合规要求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [29] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 研究探讨了多智能体系统（MAS）在定性研究中的表现，发现温度和智能体角色显著影响共识达成，但对编码准确性提升有限。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统（MAS）在定性研究中的潜力，尤其是其在编码和数据标注任务中的表现。

Method: 通过实验研究，使用六种开源LLM和18种配置，分析了77,000多个编码决策，比较了单智能体与多智能体系统的表现。

Result: 温度和智能体角色显著影响共识达成，但未显著提升编码准确性；单智能体表现通常优于多智能体系统。

Conclusion: 研究揭示了LLM在定性研究中的局限性，挑战了多样MAS角色能带来更好结果的假设，并开源了实验代码。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [30] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 论文介绍了西班牙语和加泰罗尼亚语的偏见评估数据集EsBBQ和CaBBQ，用于评估LLMs在西班牙社会背景下的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 现有资源主要针对英语和美国社会背景，缺乏其他语言和地区的偏见评估工具。

Method: 基于BBQ数据集，设计了西班牙语和加泰罗尼亚语的平行数据集，采用多选题QA形式评估10类社会偏见。

Result: LLMs在模糊场景中易选错答案，高QA准确率常与依赖社会偏见相关。

Conclusion: 需要更多针对非英语和非美国背景的偏见评估工具，以全面评估LLMs的偏见问题。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [31] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM是一种基于大型语言模型（LLM）的新型框架，通过提示链和思维链推理从RFC文档中提取精确的有限状态机（FSM），解决了现有技术中的可扩展性和覆盖不全问题。


<details>
  <summary>Details</summary>
Motivation: 现有的FSM提取技术在可扩展性、覆盖范围和自然语言规范模糊性方面存在局限性，FlowFSM旨在解决这些问题。

Method: FlowFSM结合LLM、提示链和思维链推理，系统处理协议规范，识别状态转换并构建结构化规则书。

Result: 在FTP和RTSP协议上的实验表明，FlowFSM提取精度高，且减少了虚假转换。

Conclusion: FlowFSM展示了基于代理的LLM系统在协议分析和FSM推断中的潜力，适用于网络安全和逆向工程。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [32] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的方法SAE-LAPE，用于识别大语言模型（LLM）中的语言特定特征，并发现这些特征主要位于模型中后层，且对多语言性能和输出有影响。


<details>
  <summary>Details</summary>
Motivation: 理解LLM的多语言机制具有挑战性，现有研究难以从跨语言表示中分离语言特定单元。

Method: 使用稀疏自编码器（SAE）和特征激活概率方法（SAE-LAPE）识别语言特定特征。

Result: 发现语言特定特征主要位于模型中后层，可解释性强，且在多语言性能和语言识别任务中表现优异。

Conclusion: SAE-LAPE方法有效识别语言特定特征，为LLM的多语言机制提供了新见解。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [33] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: KV-Latent通过降采样Key-Value向量维度到潜在空间，显著减少KV Cache占用并提升推理速度，仅需少量额外训练。


<details>
  <summary>Details</summary>
Motivation: Decoder架构的Key-Value缓存逐渐增加成为推理效率瓶颈，影响内存和带宽。

Method: 提出KV-Latent范式，降采样KV向量维度并改进Rotary Positional Embedding的频率采样机制。

Result: 实验显示KV-Latent能有效减少缓存占用并提升推理速度，性能影响小。

Conclusion: KV-Latent为高效语言模型系统提供了新可能，代码已开源。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [34] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的自动形式化方法，通过错误反馈实现无需训练的自动形式化，并构建了一个高质量的数学问题数据集，用于评估自动定理证明器。


<details>
  <summary>Details</summary>
Motivation: 提升形式化数学推理的效率与准确性，通过大规模自然语言数学问题数据集构建形式语言数据集。

Method: 基于大语言模型的自动形式化流水线，结合错误反馈机制，无需额外训练。

Result: 构建了包含3,922个自然语言问题和9,787个Lean形式化问题的数据集，64.46%质量良好；实验表明错误反馈和增加采样数能提升形式化效果。

Conclusion: 该数据集适合作为自动定理证明器的基准，同时验证了大语言模型在形式化与推理任务中的潜力。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [35] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 论文提出首个中文仇恨言论细粒度标注数据集（STATE ToxiCN），并研究了编码仇恨术语及大语言模型（LLMs）的语义理解能力，提出了一种整合标注词典的方法以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 中文仇恨言论检测研究滞后，且现有方法在细粒度语义理解和编码仇恨术语解释性方面存在不足。

Method: 引入STATE ToxiCN数据集，研究编码仇恨术语及LLMs的语义理解能力，提出整合标注词典的方法。

Result: 新数据集和整合词典方法显著提升了仇恨言论检测性能。

Conclusion: 研究为中文仇恨言论检测的可解释性提供了宝贵资源和见解。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [36] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot是一个多代理大型语言模型系统，旨在提升罗马尼亚语医生在文本远程医疗中的沟通质量，而非临床准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医生在远程医疗中沟通质量不足的问题，提升患者体验。

Method: 使用三个LLM代理，通过DSPy自动优化提示，基于低资源罗马尼亚语数据设计，部署开源模型。

Result: 实证评估和实际部署显示，医生响应质量和用户评价显著提升。

Conclusion: Dr.Copilot是罗马尼亚医疗环境中首批实际部署的LLM系统之一，效果显著。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [37] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 论文提出了一种名为ConVA的方法，通过控制LLM内部值的向量激活，直接对齐其潜在表示中的价值观，确保一致性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的发展，如何使其与人类价值观对齐变得尤为重要，以提供清晰、透明且能适应动态场景的能力。

Method: 提出上下文控制的值向量识别方法，以及门控值向量激活方法，确保在不影响模型性能的前提下实现有效的价值观控制。

Result: 实验表明，该方法在10种基本价值观上实现了最高的控制成功率，同时保持了LLM的性能和流畅性，并能抵御恶意输入。

Conclusion: ConVA方法有效实现了LLM与人类价值观的对齐，具有实际应用潜力。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [38] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合人类专家和大型语言模型（LLM）的方法，用于评估学术论文的方法新颖性，通过提取同行评审报告中的新颖性相关句子和LLM总结的方法部分来微调预训练语言模型（PLMs），并设计了文本引导融合模块以提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统的新颖性评估方法（如专家判断或引用组合）存在局限性，专家知识有限，引用组合方法的有效性不确定。LLM拥有丰富知识，但缺乏人类专家的判断能力，因此结合两者以弥补不足。

Method: 从同行评审报告中提取新颖性相关句子，利用LLM总结论文方法部分，用于微调PLMs；设计了基于Sparse-Attention的文本引导融合模块以整合人类和LLM知识。

Result: 实验表明，所提方法在性能上优于大量基线模型。

Conclusion: 结合人类专家和LLM的知识与能力，可以有效评估学术论文的方法新颖性，弥补传统方法的不足。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [39] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次对多种过程模型表示（PMRs）在LLM驱动的过程建模（PMo）中的适用性和性能进行了实证研究，并提出了PMo数据集。研究发现，Mermaid在PMo中表现最佳，而BPMN text在过程元素相似性上表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有PMRs在结构、复杂性和可用性上差异显著，且缺乏系统性比较；同时，PMG方法的评估策略和技术各异，难以直接比较。

Method: 通过构建PMo数据集（包含55个过程描述及九种PMRs的模型），从LLM驱动的PMo适用性和PMG性能两个维度评估PMRs。

Result: Mermaid在六项PMo标准中得分最高，BPMN text在过程元素相似性上表现最佳。

Conclusion: 研究为PMRs的选择提供了实证依据，Mermaid和BPMN text在不同场景下各有优势。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [40] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 论文探讨了加权损失函数在Transformer模型中的应用，用于SemEval-2025共享任务11中的多标签情感检测，旨在解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 通过动态调整类别权重，提升少数情感类别的性能，同时避免传统重采样方法的计算负担。

Method: 在BRIGHTER数据集上评估BERT、RoBERTa和BART模型，使用Micro F1、Macro F1、ROC-AUC、准确率和Jaccard相似系数等指标。

Result: 加权损失函数对高频情感类别性能提升明显，但对少数类别影响有限。

Conclusion: 该方法在多标签情感检测中有效，但仍面临处理不平衡数据的挑战。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [41] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 论文提出了DCR框架，用于检测和量化语言模型在评估数据中的污染问题，通过调整准确率反映真实性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的快速发展引发了对评估数据污染的担忧，污染可能导致性能指标虚高，影响泛化能力的真实评估。

Method: DCR框架通过四个粒度级别（语义、信息、数据和标签）检测污染，并使用模糊推理系统生成统一的DCR因子来调整准确率。

Result: 在9个LLM（0.5B-72B）上的验证显示，DCR能可靠诊断污染严重程度，调整后的准确率与未污染基准相比平均误差在4%以内。

Conclusion: DCR框架高效透明，为LLM评估提供了实用工具，促进公平比较并提升基准测试的可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [42] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0整合了非推理和推理模式，提升了可用性和推理能力，支持多语言（包括西班牙语），并推出两种规模模型（32B和1.2B），性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为迎接AI代理时代，EXAONE 4.0增强了工具使用和多语言支持，同时保持高性能。

Method: 通过整合非推理和推理模式，优化模型规模（32B和1.2B），并扩展多语言能力。

Result: EXAONE 4.0在性能上优于同类开源模型，并可与前沿模型竞争。

Conclusion: EXAONE 4.0为研究提供了高性能且易用的模型，支持多语言和代理功能。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [43] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 论文通过引入Causal CoT Graphs (CCGs)分析语言模型的推理过程，发现推理节点对最终答案有中介作用，且模型内部结构类似CCGs。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维（chain-of-thought）如何提升语言模型在推理任务中的表现。

Method: 提出CCGs（有向无环图），从推理轨迹中自动提取细粒度因果依赖关系，并构建数据集KisMATH（包含1671个数学问题及其CCGs）。

Result: 实验表明：(i) CCG中的推理节点是最终答案的中介；(ii) 语言模型内部结构与CCGs类似。

Conclusion: KisMATH为研究链式思维在语言模型推理中的作用提供了新工具和方法。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [44] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 论文比较了编码器-仅和解码器-仅语言模型的性能，通过相同训练方法证明编码器在分类和检索任务上表现更优，解码器在生成任务上更优。


<details>
  <summary>Details</summary>
Motivation: 解决编码器-仅和解码器-仅模型在不同任务上的性能差异问题，避免以往研究中参数和训练方法不一致的干扰。

Method: 使用相同的训练方法（SOTA Ettin模型套件）训练不同规模的编码器-仅和解码器-仅模型，并进行任务适配比较。

Result: 编码器在分类和检索任务上优于解码器，解码器在生成任务上更优；任务适配训练效果不如直接使用反向目标模型。

Conclusion: 任务适配训练效果有限，建议根据任务类型直接选择编码器或解码器模型。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [45] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 研究探讨如何通过提示控制大语言模型（LLMs）的推理策略，并评估其对逻辑问题解决的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLMs倾向于单一推理策略，可能限制其在多样化推理任务中的表现。

Method: 通过实验验证提示对LLMs推理策略的控制效果，并提出引导模型选择最优策略的方法。

Result: 实验显示单一策略无法持续提升准确性，但若能自适应选择最优策略，性能可提升。

Conclusion: 提出新方法优化LLMs的推理能力，强调策略选择的重要性。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [46] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 论文介绍了HKGAI-V1的开发，这是一个为香港量身定制的基础主权大语言模型，通过多语言支持和区域对齐框架，提升本地化AI应用能力。


<details>
  <summary>Details</summary>
Motivation: 解决香港多语言环境、独特的社会法律背景及本地文化价值观需求，建立符合区域规范的AI基础设施。

Method: 基于DeepSeek架构，通过全参数微调与检索增强生成（RAG）系统，实现区域对齐与实时信息支持。

Result: HKGAI-V1在本地文化敏感查询中优于通用模型，并开发了专有的对抗性香港价值基准测试工具。

Conclusion: 论文提供了技术成果和可复制的区域化AI系统开发蓝图，支持香港在关键领域的数字主权。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [47] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 论文研究了酒店亮点总结的忠实性评估，比较了传统指标、可训练方法和LLM作为评判者的方法，发现简单指标如词重叠与人类判断相关性高，而LLM在评估中不可靠。


<details>
  <summary>Details</summary>
Motivation: 探讨在酒店亮点总结中如何评估生成内容对输入数据的忠实性，以解决实际业务中的风险和挑战。

Method: 通过人工评估活动，包括分类错误评估和跨度级标注，比较传统指标、可训练方法和LLM评判方法。

Result: 简单指标（如词重叠）与人类判断相关性高（Spearman等级相关系数0.63），LLM在评估中表现不可靠。

Conclusion: LLM生成高质量亮点但评估不可靠，简单指标在跨领域数据中表现优异，需注意错误信息风险和众包评估挑战。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet是一种基于小波变换和因果推理的低光图像增强方法，通过全局度量学习和局部语义损失优化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽略实例级语义信息和特征特性，CWNet旨在解决这一问题。

Method: 结合因果推理和小波变换，采用全局度量学习和局部CLIP语义损失，设计小波变换骨干网络。

Result: 在多个数据集上显著优于现有方法，表现鲁棒。

Conclusion: CWNet通过因果推理和小波变换，有效提升了低光图像增强的性能。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [49] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 提出了一种整合外部生物知识的新框架，用于增强显微镜图像分析模型，以解决细胞系异质性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 细胞系间形态和生物异质性使得药物发现中的高通量筛选技术（如显微镜成像）面临挑战。

Method: 结合外部生物知识（如蛋白质相互作用图和转录组特征）解耦扰动特异性和细胞系特异性表征。

Result: 在RxRx数据库上验证，通过少量样本微调显著提升了对新细胞系的图像分析能力。

Conclusion: 该方法有效提升了基于表型的药物发现应用中的显微镜图像分析性能。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [50] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该研究审核了两个先进的FER数据集，发现其中包含大量摆拍图像，且模型对非白人或深肤色人群存在偏见，容易误判负面情绪。


<details>
  <summary>Details</summary>
Motivation: 解决FER算法在检测自发表情时性能下降及对不同种族和肤色人群表现不佳的问题，这些问题与数据集收集方式有关。

Method: 随机抽样审核数据集中的图像，区分自发与摆拍表情，并测试模型对不同肤色人群的表现。

Result: 发现数据集中存在大量摆拍图像，且模型对非白人或深肤色人群更易误判为负面情绪。

Conclusion: 数据集和模型存在偏差，可能导致实际应用中产生有害影响，需改进数据收集和模型训练方法。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [51] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 提出一种无需描述符的兴趣点匹配方法，显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要计算和匹配描述符，内存开销大，新方法旨在消除这一需求。

Method: 在检测阶段直接关联兴趣点，避免描述符的计算和匹配。

Result: 匹配精度略低于传统方法，但内存使用大幅减少。

Conclusion: 新方法在内存效率上具有显著优势，适用于资源受限的定位系统。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [52] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一种新的航天器图像数据集，用于训练和评估实时图像分割模型，以支持航天器的自主检测系统。


<details>
  <summary>Details</summary>
Motivation: 航天器在太空环境中易受损坏，而人工或机器人维修成本高昂，因此需要开发可靠且经济的自主检测系统。

Method: 使用真实航天器模型和合成背景创建了64k标注图像数据集，并添加噪声和失真模拟真实环境。基于YOLOv8和YOLOv11模型进行微调，并在硬件和时间约束下评估性能。

Result: 模型在测试中达到Dice分数0.92、Hausdorff距离0.69，推理时间约0.5秒。

Conclusion: 该数据集和模型为航天器实时图像分割提供了有效解决方案，支持太空自主检测系统的开发。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [53] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种数据高效的方法，通过LLM代理系统增强空间理解能力，解决复杂室内仓库场景中的空间问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在空间理解任务上表现不足，需要更高效的方法提升其能力。

Method: 设计了一个LLM代理系统，结合空间推理工具和API交互，用于复杂空间问题的解答。

Result: 在2025 AI City Challenge数据集上验证，系统在物体检索、计数和距离估计等任务中表现出高准确性和效率。

Conclusion: 该方法为空间理解任务提供了一种高效且实用的解决方案。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [54] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT是一种嵌套ViT架构，通过动态调整计算资源以适应输入复杂度，提升效率。


<details>
  <summary>Details</summary>
Motivation: 固定计算预算的Vision Transformers在异构硬件上部署效率低，嵌套架构虽能扩展但未考虑输入复杂度差异。

Method: 采用渐进式思考阶段和Token Recycling机制，动态激活注意力头并复用前一阶段嵌入。

Result: 在相同吞吐量下，ThinkingViT比基线模型准确率提升2.0 p.p.，在相同计算量下提升2.9 p.p.。

Conclusion: ThinkingViT通过动态计算分配显著提升了效率和性能，可作为ViT的插件升级。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [55] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于大型语言模型（LLM）的自主目标检测框架（LAOD），通过动态生成场景特定对象名称实现零样本检测，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖固定类别集，灵活性不足；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。

Method: 利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，提出CAAP和SNAP两个新指标分别评估定位和命名能力。

Result: 在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，能够高效检测和命名新对象。

Conclusion: LAOD框架提升了开放世界理解的自主性和适应性。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [56] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM是一种基于Grad-CAM的改进方法，通过跨层聚合信息和Winsorization技术生成更鲁棒和连贯的显著性图。


<details>
  <summary>Details</summary>
Motivation: 解释CNN决策过程对高风险领域至关重要，但现有方法（如Grad-CAM）可能掩盖重要语义或放大噪声。

Method: 提出Winsor-CAM，利用Winsorization技术衰减异常值，并通过用户可调阈值实现语义级控制。

Result: 在PASCAL VOC 2012数据集上，Winsor-CAM生成更可解释的热图，并在定位指标上优于Grad-CAM和均匀层平均基线。

Conclusion: Winsor-CAM通过多层解释和人机交互控制，推动了可信AI的发展。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [57] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来改进模型适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法难以解释参数更新的贡献，稀疏编码框架旨在提高模型适应性和可理解性。

Method: 采用稀疏编码方法，将微调特征表示为特征字典原子的稀疏组合，稀疏系数指示原子重要性。

Result: 方法在图像编辑和文本到图像概念定制任务中表现优异，优于基线微调方法。

Conclusion: 稀疏编码框架显著提升了模型的可解释性和任务适应性，为微调提供了新思路。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [58] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 该研究提出了一种结合LOF算法和YOLO-v11n的轻量级框架，用于结直肠息肉检测，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要死因之一，及时准确的息肉检测对诊断和预防至关重要。

Method: 使用LOF算法过滤噪声数据，结合YOLO-v11n模型，通过5折交叉验证和数据增强策略优化模型。

Result: 模型性能显著提升，精度达95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%。

Conclusion: 该方法适合临床实时结肠镜检查，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [59] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super是一种改进的3D医学图像中心线追踪模型，解决了重复分支预测和过早终止的问题，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确追踪管状树结构（如血管和气道）对医学任务至关重要，但现有模型存在重复分支和过早终止的问题。

Method: 提出Trexplorer Super模型，通过新技术改进性能，并开发了三个数据集（一个合成，两个真实）用于评估。

Result: Trexplorer Super在所有数据集上均优于现有方法，但合成数据的表现不一定适用于真实数据。

Conclusion: Trexplorer Super显著提升了中心线追踪性能，同时公开了代码和数据集以促进进一步研究。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [60] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于CNN的轻量级天气预测模型KAI-a，在保持高精度的同时显著降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的AI天气预测模型表现出色，但其高训练复杂性和资源需求限制了实际应用。

Method: 采用现代化的CNN架构，结合尺度不变设计和InceptionNeXt模块，针对地球系统数据优化。

Result: KAI-a在中等范围天气预报中表现与最先进模型相当，且训练时间短、参数少。

Conclusion: KAI-a展示了轻量级模型在极端事件预测中的实用性，为数据驱动天气预报提供了高效解决方案。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [61] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文提出两种正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度标签不一致问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度标签不一致（TsDLI）问题，提升模型泛化性和可解释性。

Method: 提出局部变化损失（LVL）和局部-全局一致性损失（LGCL），结合有界变差函数和通勤时间距离的数学原理。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL在多种架构（如LSTM和Transformer）中表现优于基线方法。

Conclusion: LVL和LGCL有效解决了TsDLI问题，在性能和可解释性之间取得了平衡，LVL表现最佳。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [62] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill是一种几何引导的弱监督自蒸馏框架，通过教师-学生学习和基于视场的掩码提升跨视图定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的全监督学习，需要精确的姿态标注，而GeoDistill旨在通过弱监督减少标注成本并提升定位鲁棒性。

Method: 使用教师模型定位全景图像，学生模型预测有限视场图像的位置，通过特征对齐提升局部特征学习。

Result: 实验表明GeoDistill显著提升定位性能，且适用于全景和有限视场图像。

Conclusion: GeoDistill为跨视图定位提供了一种可扩展且高效的解决方案。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [63] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为GAPL-SCD的框架，通过多任务联合优化和原型学习解决语义变化检测中的任务冲突问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）需要同时优化多个任务，容易因任务冲突导致负迁移。现有方法难以平衡任务间的学习难度和梯度流冲突。

Method: 提出GAPL-SCD框架，结合多任务优化、自适应权重分配和梯度旋转方法，引入图聚合原型学习模块和自查询多级特征交互模块。

Result: 在SECOND和Landsat-SCD数据集上实现了最先进的性能，显著提升了SCD任务的准确性和鲁棒性。

Conclusion: GAPL-SCD通过多任务协同优化和原型学习，有效解决了SCD中的任务冲突问题，为复杂场景下的语义变化检测提供了高效解决方案。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [64] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的ID特定人脸恢复框架，通过内容注入和身份注入模块，结合对齐学习，显著提升了身份保真度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前人脸恢复技术因身份模糊输入和随机生成过程导致身份不确定性，RIDFR旨在解决这一问题。

Method: RIDFR结合预训练扩散模型和两个并行条件模块（内容注入和身份注入），并通过对齐学习抑制ID无关语义干扰。

Result: 实验表明，RIDFR在身份保真度和鲁棒性上优于现有方法，能重建高质量ID特定结果。

Conclusion: RIDFR通过创新框架有效解决了身份不确定性问题，为人脸恢复提供了新方向。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [65] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 提出名为WomenSports的新数据集，用于女性运动分类，并设计了一种结合通道注意力的CNN方法，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有女性运动动作数据集不足，缺乏足够的类内和类间变化，限制了相关研究的发展。

Method: 提出WomenSports数据集，并设计了一种基于CNN的深度特征提取方法，结合通道注意力机制优化特征表示。

Result: 在WomenSports数据集上，使用ResNet-50达到89.15%的Top-1分类准确率。

Conclusion: WomenSports数据集和提出的深度学习方法有效解决了女性运动分类问题，为未来研究提供了资源。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [66] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种基于小波注意力的主干网络和射线编码器架构，用于高效且可靠的人-物交互检测。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互检测器在高效性和可靠性上表现不足，依赖资源密集型训练和低效架构。

Method: 设计了小波注意力主干网络和射线编码器，前者聚合低阶和高阶交互特征，后者通过优化注意力区域减少计算开销。

Result: 在ImageNet和HICO-DET等基准数据集上验证了架构的有效性。

Conclusion: 提出的架构显著提升了人-物交互检测的效率和准确性。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [67] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 论文提出了一种名为RG-Gait的方法，通过残差学习解决遮挡步态识别问题，同时保持对完整步态的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前步态识别方法未充分解决遮挡问题，且现有方法在遮挡和完整步态识别之间存在性能权衡。

Method: 将遮挡步态特征建模为完整步态表示的残差偏差，通过残差学习网络自适应整合残差。

Result: 在Gait3D、GREW和BRIAR数据集上验证，RG-Gait显著提升了遮挡步态识别性能且不影响完整步态识别。

Conclusion: 残差学习是解决遮挡步态识别并保持完整步态性能的有效方法。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [68] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN是一种轻量级架构设计，通过多尺度核和通道聚合模块提升CNN性能，参数效率高。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer的简单性偏置及信息冗余问题。

Method: 采用多尺度核和波基通道聚合模块优化空间和通道信息处理。

Result: 在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数）。

Conclusion: SpaRTAN通过高效设计实现高性能，参数效率显著。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [69] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP利用CLIP模型进行零样本异常检测，通过特征匹配和跨模态对齐，结合批次内图像作为参考点，并利用文本信息过滤噪声特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决零样本异常检测中罕见类别的识别问题，同时适应工业实际需求，提出一种无需训练的CLIP改进方法。

Method: 结合特征匹配与跨模态对齐，利用批次内图像作为参考，通过文本信息过滤噪声，并恢复CLIP的局部语义相关性以优化检测。

Result: 在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，显著超越现有方法。

Conclusion: FiSeCLIP为零样本异常检测提供了更强基线，展示了CLIP模型在细粒度任务中的潜力。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [70] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于语义显著区域的放射报告生成方法（SISRNet），通过关注医学关键区域，提高报告的临床准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据偏差导致生成的报告医学不准确，限制了临床应用。

Method: 利用细粒度跨模态语义识别医学关键区域，并在图像建模和报告生成中系统关注这些区域。

Result: 在IU-Xray和MIMIC-CXR数据集上表现优于同类方法。

Conclusion: SISRNet通过关注医学关键区域，有效缓解数据偏差，生成更准确的临床报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [71] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge的CBCT-to-MDCT转换框架，结合GAN先验和人类引导的条件扩散，通过边界一致性和人类反馈优化生成结果。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN或扩散模型在医学图像转换中解剖保真度和感知可控性的不足，同时整合人类偏好以优化临床结果。

Method: 结合Schrodinger Bridge、GAN先验和条件扩散，通过边界一致性约束和分类器自由引导（CFG）整合人类反馈，采用迭代优化和锦标赛偏好选择。

Result: 在临床数据集上，该方法在RMSE、SSIM、LPIPS和Dice指标上表现优异，仅需10步采样，优于现有方法。

Conclusion: 该框架高效且有效，适用于实时、偏好对齐的医学图像转换。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [72] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务——个性化开放词汇语义分割（personalized open-vocabulary semantic segmentation），并设计了一种基于文本提示调优的插件方法，以识别用户特定的视觉概念，同时保持原始开放词汇语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割（OVSS）无法理解用户个性化的文本描述（如“我的马克杯”）来分割特定兴趣区域，论文旨在解决这一问题。

Method: 提出了一种基于文本提示调优的插件方法，采用“负掩码提议”减少错误预测，并通过将视觉嵌入注入文本提示来丰富表示。

Result: 在FSS$^\text{per}$、CUB$^\text{per}$和ADE$^\text{per}$等新建立的基准测试中，该方法表现出色。

Conclusion: 该方法在不影响原始OVSS性能的前提下，显著提升了个性化开放词汇语义分割的效果。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [73] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: DGFDNet提出了一种双域去雾网络，结合空间和频率域信息，通过物理引导的退化对齐和自适应频率调制，显著提升了去雾性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在单图像去雾中计算成本高，且依赖空间域特征难以应对复杂雾霾条件，频率域与空间域耦合不足。

Method: 提出DGFDNet框架，包含HAFM模块（自适应增强雾霾相关频率成分）和MGAM模块（多尺度特征融合），并引入PCGB分支进行先验迭代修正。

Result: 在四个基准数据集上取得最佳性能，兼具鲁棒性和实时效率。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了去雾效果和实用性。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [74] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D是一个专注于足踝区域的高分辨率点云数据集，用于3D点云补全任务，支持生物力学研究和临床应用。


<details>
  <summary>Details</summary>
Motivation: 足踝在步态中的运动分析对生物力学研究和临床评估至关重要，但现有数据集通常关注全身或下肢运动，缺乏足踝区域的详细数据。

Method: FootGait3D包含46名受试者的8,403帧点云数据，使用五摄像头深度传感系统采集，提供完整和部分视图的点云。

Result: 数据集支持单模态和多模态点云补全方法的评估，为足部几何恢复提供基准。

Conclusion: FootGait3D在生物力学、临床步态分析和机器人应用中具有重要潜力，数据集已公开。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [75] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，通过Swin Transformer和新型模块实现高效特征提取，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率卫星图像中目标检测的挑战，提升检测精度和计算效率。

Method: 采用Swin Transformer替代CNN主干网络，结合UpConvMixer块和Fusion Blocks实现多尺度特征融合，引入CBAM注意力机制和多路径头设计。

Result: 在xView数据集上达到32.95%的准确率，优于现有方法11.46%。

Conclusion: GLOD通过创新的架构设计，显著提升了卫星图像目标检测的性能和效率。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [76] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn框架通过原型驱动学习缓解医学图像分割中对文本输入的依赖，提升无配对文本数据的利用率和临床应用。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中依赖配对文本输入的两大限制：数据利用不足和临床应用受限。

Method: 提出Prototype-driven Semantic Approximation (PSA)模块，通过原型空间近似语义指导，支持无文本输入的图像分割。

Result: 在QaTa-COV19、MosMedData+和Kvasir-SEG数据集上表现优于现有语言引导方法。

Conclusion: ProLearn有效减少对文本输入的依赖，扩展了语言引导分割的适用性。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [77] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP是一种新型的局部3D高斯编辑框架，通过3D-GALP模块和正则化SDS损失实现精确的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经表示和实例级编辑模型在实现精确局部3D编辑时面临挑战，尤其是高斯泼溅技术因多视角2D分割不一致和SDS损失的模糊性而受限。

Method: 提出3D-GALP模块生成鲁棒的3D掩码，并结合正则化SDS损失（包括L1锚定损失和额外正则化项）实现精确编辑。

Result: 实验表明，RoMaP在重建和生成的高斯场景中实现了最先进的局部3D编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更鲁棒和灵活的解决方案。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [78] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于关节角度的标记自由人体姿态估计（HPE）改进方法，通过高质量数据集训练双向循环网络，显著提升了关键点识别和轨迹平滑的性能。


<details>
  <summary>Details</summary>
Motivation: 当前HPE在关键点识别和轨迹分析中存在误差和波动，且现有深度学习模型因训练数据标注不准确而受限。

Method: 提出基于关节角度的建模方法，包括关节角度模型、高阶傅里叶级数近似时间变化，以及双向循环网络作为后处理模块。

Result: 在花样滑冰和街舞等挑战性场景中，JAR方法优于现有HPE改进网络。

Conclusion: 基于关节角度的改进方法有效提升了HPE的准确性和稳定性。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [79] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 论文提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了现有方法在结构对称性和部分遮挡下的不足，并发布了SKD数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器的单目姿态估计对在轨服务任务至关重要，但现有关键点检测器在结构对称性和部分遮挡下表现不佳。

Method: 提出GKNet，利用关键点图的几何约束进行姿态估计，并发布SKD数据集用于验证。

Result: 实验表明GKNet在精度和效果上优于现有方法。

Conclusion: GKNet在非合作航天器姿态估计中表现出高精度和有效性，SKD数据集为未来研究提供了支持。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [80] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于交叉验证策略的深度学习方法，用于从GPR图像中自动识别道路地下病害（RSD），显著提高了识别准确率并减少了人工工作量。


<details>
  <summary>Details</summary>
Motivation: GPR图像中的RSD识别依赖人工且效率低，现有深度学习方法因数据稀缺和网络能力不足而受限。

Method: 构建了高质量的3D GPR数据集，并提出基于YOLO模型的交叉验证策略。

Result: 在实地测试中，召回率超过98.6%，检测系统可减少约90%的人工工作量。

Conclusion: 该方法为RSD自动识别提供了高效解决方案，具有实际应用价值。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [81] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: Atmos-Bench是首个3D大气基准数据集，结合FourCastX网络，通过物理约束和高质量模拟数据，提升了大气结构恢复的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助输入和简化物理近似，缺乏标准化3D基准，导致不确定性和对真实大气效应的捕捉不足。

Method: 提出Atmos-Bench基准数据集和FourCastX网络，结合WRF和COSP模拟器生成高质量数据，嵌入物理约束提升恢复效果。

Result: 在Atmos-Bench数据集上，FourCastX在355 nm和532 nm波段均优于现有基线模型，无需辅助输入。

Conclusion: Atmos-Bench为卫星3D大气结构恢复设定了新标准，有助于更深入的气候研究。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [82] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统综述了视觉识别模型的可解释性研究，提出了一种以人为中心的分类法，并探讨了评估指标和新技术的机遇。


<details>
  <summary>Details</summary>
Motivation: 随着视觉识别模型在关键领域的应用增加，理解其机制和失败原因的需求推动了可解释性研究的发展。

Method: 提出了一种基于意图、对象、呈现和方法论的分类法，系统整理了现有XAI方法。

Result: 建立了系统化的分类标准，总结了评估指标需求，并探讨了新技术（如多模态大模型）带来的机遇。

Conclusion: 本文旨在组织现有研究并启发未来对视觉识别模型可解释性的探索。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [83] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++ 是一种新型多模态大语言模型，专注于通用关键点理解，通过用户指令整合多模态输入，提升细粒度图像分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在捕捉细粒度语义信息（如关键点）方面表现不足，而关键点对图像分析、对象检索和行为识别至关重要。

Method: 采用“识别-检测”范式，先解析关键点语义，再通过链式推理机制定位其精确位置，并扩展训练数据集至 50 万样本。

Result: 在多个关键点检测基准测试中表现优异，展示了卓越的准确性和泛化能力。

Conclusion: KptLLM++ 是细粒度图像理解的统一解决方案，对提升人机交互具有变革性意义。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [84] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于水母物种的检测和分类，结合多种特征提取技术和分类器，最高准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 水母在海洋生态系统中扮演重要角色，但其快速繁殖和生态影响对生物多样性和保护构成挑战，准确识别物种对生态监测和管理至关重要。

Method: 整合了MobileNetV3、ResNet50、EfficientNetV2-B0和VGG16等特征提取技术，结合传统机器学习分类器和前馈神经网络分类器，并使用softmax函数进行直接分类。

Result: 结合MobileNetV3的人工神经网络模型表现最佳，准确率达98%，显著优于其他组合。

Conclusion: 深度学习与混合框架在解决生物多样性挑战和推进海洋物种检测方面具有显著效果。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [85] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出了一种多模态引导的硬样本生成与学习框架（HSGL），用于解决服装变化行人重识别（CC-ReID）中的硬样本问题，通过生成和优化硬样本提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 硬样本在CC-ReID任务中因模糊性或相似性成为瓶颈，限制了学习策略设计和模型鲁棒性。

Method: HSGL包含双粒度硬样本生成（DGHSG）和硬样本自适应学习（HSAL），利用多模态信息生成和优化硬样本。

Result: 在多个CC-ReID基准测试中表现优异，显著加速收敛并在PRCC和LTCC数据集上达到SOTA性能。

Conclusion: HSGL展示了多模态引导的硬样本生成与学习在CC-ReID中的潜力，提升了模型的判别能力和鲁棒性。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [86] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MMOne的通用框架，用于解决多模态场景表示中的模态冲突问题，通过模态建模模块和多模态分解机制，实现了更紧凑高效的多模态表示。


<details>
  <summary>Details</summary>
Motivation: 人类通过多模态线索感知世界，但不同模态间的固有差异（如属性差异和粒度差异）带来了挑战，需要一种方法来统一表示多模态信息。

Method: 提出MMOne框架，包括模态建模模块（使用模态指示器捕获模态特性）和多模态分解机制（将多模态高斯分布分解为单模态高斯分布）。

Result: 实验表明，该方法能显著提升各模态的表示能力，并可扩展到更多模态。

Conclusion: MMOne框架有效解决了多模态场景表示中的模态冲突问题，实现了更高效和紧凑的多模态表示。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [87] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，并在多个数据集上取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 由于极端天气和人类活动导致滑坡灾害频发，但传统观测方法在大范围和复杂地形中难以实现自动化。

Method: 设计了一种新型神经网络架构，用于滑坡检测和分割任务，输入为遥感图像。

Result: 在LandSlide4Sense、Bijie和Nepal数据集上，检测任务的F1分数分别为98.23和93.83，分割任务的mIoU分数为63.74和76.88。

Conclusion: 实验证明该模型具有实际应用潜力，可集成到滑坡观测系统中。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [88] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型的色彩视觉能力，提出了一个测试任务和数据集，并分析了错误类型及优化策略。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的色彩视觉能力尚未被充分探索，填补这一空白是研究的动机。

Method: 定义了色彩视觉测试任务，构建了多类别、多难度级别的数据集，并分析了模型的错误类型。

Result: 提出了针对色彩视觉测试的微调策略，以提升模型性能。

Conclusion: 研究为大型视觉语言模型的色彩视觉能力提供了测试方法和优化方向。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [89] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 提出了一种基于聚类引导的自监督多层对比表示学习算法（CMCRL），用于柑橘病害检测与分类，显著提升了性能并减少了对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 柑橘病害严重影响产量，传统深度学习方法依赖大量标注数据，而CMCRL通过自监督学习减少这一需求。

Method: 引入聚类中心对比和多层对比训练（MCT）范式，利用无标注样本优化模型，适应症状相似性并学习分层特征表示。

Result: 在公开数据集CDD上表现优异，准确率提升4.5%-30.1%，且在其他评估指标（F1分数、精确率、召回率）上表现稳健。

Conclusion: CMCRL在减少标注数据需求的同时，实现了与全监督方法接近的性能，为病害检测提供了高效解决方案。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [90] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 本文评估了开源通用和医学专用视觉语言模型（VLMs）在医疗任务中的表现，发现通用大模型在某些任务上已超越医学专用模型，但推理能力仍是瓶颈，且临床可靠性尚未达标。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在医疗任务中的能力，填补现有研究的空白。

Method: 通过八个医疗基准测试（如MedXpert、OmniMedVQA等）评估3B至72B参数的VLMs，将性能分为理解和推理两部分。

Result: 1. 通用大模型在部分任务上表现优于医学专用模型；2. 推理能力普遍低于理解能力；3. 不同基准测试间性能差异显著。

Conclusion: 当前模型尚未达到临床可靠性标准，需加强多模态对齐和更严格的评估协议。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [91] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA是一种针对不完全多模态学习的参数高效训练框架，通过解耦模态组合的共享信息和动态调整训练比例，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感识别中因模态缺失导致的训练梯度冲突问题。

Method: 提出MCULoRA框架，包含MCLA模块（解耦模态组合的共享信息）和DPFT模块（动态调整训练比例）。

Result: 在多个基准数据集上，MCULoRA显著优于现有方法。

Conclusion: MCULoRA为不完全多模态学习提供了一种高效且性能优越的解决方案。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [92] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 提出了首个用于评估长视频生成模型叙事表达能力的基准NarrLV，通过引入Temporal Narrative Atom（TNA）和自动提示生成管道，结合MLLM框架设计评估指标，实验表明其与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成模型的评估缺乏专门针对叙事表达能力的基准，现有方法主要依赖简单叙事提示的基准（如VBench），无法全面衡量模型的叙事丰富性。

Method: 1. 引入Temporal Narrative Atom（TNA）作为基本叙事单元，量化叙事丰富性；2. 基于电影叙事理论设计自动提示生成管道；3. 结合MLLM框架设计多级评估指标。

Result: 实验结果显示，NarrLV的评估指标与人类判断高度一致，并揭示了当前视频生成模型在叙事表达方面的能力边界。

Conclusion: NarrLV填补了长视频生成模型叙事评估的空白，为未来研究提供了有效的评估工具。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [93] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出了一种基于公平性的分组方法，用于处理连续敏感属性，通过最大化组间歧视差异的新标准来识别关键子群体，并在实验中验证了其有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统公平性评估方法在处理连续敏感属性（如肤色）时可能忽略少数群体的歧视问题，因此需要一种更精细的分组方法。

Method: 提出了一种基于歧视水平的公平性分组方法，通过最大化组间歧视差异的新标准来识别关键子群体，并用于去偏目的。

Result: 实验表明，该方法在合成数据集和真实数据集（如CelebA和FFHQ）上能揭示更细微的歧视模式，并在去偏处理中保持准确性。

Conclusion: 该方法有效提升了公平性评估的精细度，同时不影响模型准确性，具有工业部署潜力。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [94] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的框架，通过预训练分割模型和多模态模型获取烟雾掩码和图像描述，改进修复模型生成效果，并利用新损失函数增强一致性，最终生成高质量烟雾数据集提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决森林火灾烟雾图像数据稀缺及现有修复模型生成烟雾图像质量不足的问题。

Method: 结合预训练分割模型和多模态模型获取烟雾掩码和图像描述，提出基于掩码和掩码图像特征的网络架构及新损失函数（掩码随机差异损失），并利用多模态大语言模型筛选合成图像。

Result: 生成的烟雾图像真实且多样，有效提升了森林火灾烟雾检测模型的性能。

Conclusion: 提出的框架能生成高质量烟雾图像，解决数据稀缺问题，并提升检测任务效果。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [95] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD框架通过结构化多视角分解解决3D视觉定位中的多锚点查询和视角变化问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂多锚点查询和视角变化导致的空间描述不一致问题。

Method: 提出ViewSRD框架，包含Simple Relation Decoupling (SRD)模块、Multi-view Textual-Scene Interaction (Multi-TSI)模块和Textual-Scene Reasoning模块。

Result: 在3D视觉定位数据集上表现显著优于现有方法，尤其在复杂查询中。

Conclusion: ViewSRD通过结构化多视角分解有效解决了3D视觉定位中的关键挑战。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [96] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测与识别（ATR），在复杂环境下表现优异，准确率达99.6%。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在国防和监控领域的ATR任务面临诸多挑战，如数据集有限、硬件限制、天气影响等，现有深度学习模型表现不佳。

Method: 基于改进的YOLOv5s，优化检测头、特征融合和自定义数据增强，提出YOLOatr模型。

Result: 在DSIAC MWIR数据集上测试，YOLOatr在实时ATR任务中达到99.6%的准确率。

Conclusion: YOLOatr在复杂热红外环境下表现优异，为ATR任务提供了高效解决方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [97] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP是一个基于IoT的番茄植物表型数据集，包含64,464张RGB图像和3,616张高分辨率图像，用于精细表型分析。通过深度学习模型验证，其准确性和速度与专家相当。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法存在观察者偏差和不一致性问题，限制了精细分析的准确性和可重复性。

Method: 开发了TomatoMAP数据集，采用IoT成像系统和标准化数据采集协议，结合MobileNetv3、YOLOv11和MaskRCNN的深度学习框架进行验证。

Result: 模型在精细表型分析中的准确性和速度与专家相当，Cohen's Kappa和评分者一致性热图验证了方法的可靠性。

Conclusion: TomatoMAP数据集和自动化方法为植物表型分析提供了高精度和可重复的解决方案。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [98] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 论文提出了一种任务导向的人类抓取合成方法，通过任务感知的接触地图和两阶段流程，显著提升了抓取质量和任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统抓取合成方法缺乏对场景和任务的全面考虑，导致抓取姿势与任务需求不匹配。

Method: 采用两阶段流程：首先生成任务感知的接触地图，随后基于该地图合成任务导向的抓取姿势。

Result: 实验表明，该方法在抓取质量和任务表现上显著优于现有方法。

Conclusion: 任务和场景信息的综合建模对任务导向的抓取合成至关重要。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [99] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 论文提出了一种基于人工智能的方法，利用YOLOv11模型自动识别侵蚀区域并估算面积，开发了交互式网页应用ERO SCAN。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专业知识和人工处理，效率低，需要自动化解决方案。

Method: 调整并微调YOLOv11模型，结合照片和LiDAR图像训练，使用Roboflow平台标注数据。

Result: 模型准确率70%，能精确识别侵蚀区域并计算面积，开发了ERO SCAN系统。

Conclusion: 该方法优化了侵蚀检测和量化，支持风险管理和土地规划决策。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [100] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新的高斯泼溅框架，首次引入多种几何基元以提高表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一基元（高斯椭圆或椭球）表示复杂多样的物体表面，导致重建质量不足。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，以支持多种基元的高效渲染和学习。

Result: 实验证明该框架能显著提升表面重建的准确性。

Conclusion: 通过引入多种基元，新框架在高斯泼溅中实现了更高质量的物体表面重建。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [101] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet结合单目深度估计与多视角立体视觉，通过注意力机制和动态深度候选更新，解决了纹理缺失和反射区域的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MVS方法在纹理缺失和反射区域表现不佳，而单目深度估计无需特征匹配，具有鲁棒性。

Method: 通过注意力机制整合单目特征，动态更新深度候选，并设计相对一致性损失监督深度预测。

Result: 在DTU和Tanks-and-Temples数据集上达到SOTA性能，排名第一。

Conclusion: MonoMVSNet通过结合单目深度先验，显著提升了MVS在挑战性区域的性能。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [102] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 论文提出了UGC-VideoCap，一个针对短视频的多模态字幕生成基准和模型框架，强调音频与视觉的平衡整合，并提供了高质量数据集和高效训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成研究过于视觉中心化，忽视了音频在场景动态、说话者意图和叙事背景中的重要作用，缺乏多模态数据集和轻量级模型。

Method: 提出UGC-VideoCap基准，包含1000个TikTok视频的多模态标注和4000个QA对；开发UGC-VideoCaptioner(3B)模型，采用两阶段训练策略（监督微调+GRPO）。

Result: UGC-VideoCap提供了高质量的多模态数据集，模型在有限数据下实现了高效适应和竞争性能。

Conclusion: UGC-VideoCap为真实场景下的多模态视频字幕生成提供了高质量基准和数据高效解决方案。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [103] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 该论文提出了一种几何方法来描述人脸识别模型对可解释属性的依赖性或不变性，并引入了一种物理启发的对齐度量。


<details>
  <summary>Details</summary>
Motivation: 研究发现人脸识别模型的嵌入空间中存在多尺度几何结构，受可解释的面部和图像属性影响，但现有方法仅关注身份信息。

Method: 提出几何方法和物理启发的对齐度量，评估模型在合成数据微调后的属性不变性。

Result: 模型对不同属性表现出不同程度的不变性，揭示了其优势和弱点。

Conclusion: 该方法为模型提供了更深的可解释性，代码已开源。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [104] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR模型在图像生成领域作为扩散模型的替代方案，本文研究其适应性和差分隐私（DP）应用，发现VAR在非DP任务中表现优于扩散模型，但DP性能较差。


<details>
  <summary>Details</summary>
Motivation: 探索VAR模型的适应性及其在差分隐私下的表现，填补现有研究空白。

Method: 实现并对比多种VAR适应策略，与最先进的扩散模型策略进行基准测试。

Result: VAR在非DP任务中优于扩散模型，但DP性能不足。

Conclusion: 需进一步研究VAR的隐私适应策略。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [105] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI框架利用神经表示技术（NeRV）改进大图像压缩，通过预训练-微调、混合精度训练和并行化目标加速收敛，并引入超压缩技术提升压缩比。


<details>
  <summary>Details</summary>
Motivation: 高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化性差，INR技术虽具潜力但速度慢且压缩比不足。

Method: 采用NeRV技术，通过预训练-微调、混合精度训练和并行化目标加速收敛，并引入超压缩技术优化压缩比。

Result: 在两个医学影像数据集上，COLI在PSNR和SSIM指标上表现优异，压缩比显著提升，训练速度提高4倍。

Conclusion: COLI框架有效解决了INR技术在大图像压缩中的速度和压缩比问题，具有实际应用潜力。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [106] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和扩散生成模型的血管几何合成方法，用于生成高保真度的主动脉几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法受限于线性假设，难以处理复杂血管拓扑结构。

Method: 结合NURBS参数化和分层扩散模型，生成中心线和径向轮廓。

Result: 生成的主动脉几何结构与原始数据集生物标志物分布高度匹配。

Conclusion: HUG-VAS首次通过NURBS和扩散模型统一整合，实现了从图像先验到生成形状建模的桥梁。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [107] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI算法通过组合边缘像素采样和卷积密度估计，在模糊图像中实现高精度圆检测和拟合，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在图像质量差的情况下实现稳健圆检测和拟合的计算机视觉挑战。

Method: 结合高效的组合边缘像素采样和参数空间中的卷积密度估计。

Result: 在真实医学数据和合成数据中表现优异，Jaccard指数达0.896，实时性能40.3 fps，优于RCD等方法。

Conclusion: 3C-FBI在精度、速度和鲁棒性上表现突出，适用于医疗影像、机器人和工业检测。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [108] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类感知的模糊颜色模型COLIBRI，通过模糊集和逻辑构建颜色分类框架，实验验证其与传统颜色模型相比更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 计算机难以模仿人类颜色感知，需要一种能弥合计算表示与人类视觉感知差距的模型。

Method: 采用三阶段实验方法，包括初步实验确定颜色刺激、大规模人类分类调查（1000+受试者），并提取模糊分区生成隶属函数。

Result: 模型在比较评估中显示出与传统颜色模型（如RGB、HSV、LAB）相比更符合人类感知。

Conclusion: COLIBRI模型在设计和人机交互等领域具有重要意义，首次基于大规模样本构建颜色属性规范模型。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [109] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出了一种5阶段框架，用于从EEG信号解码视觉表示，通过跨模态对齐和重新排序实现上下文感知的EEG到图像生成，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号复杂且噪声大，解码视觉表示具有挑战性，因此需要一种新方法来解决这一问题。

Method: 5阶段框架：EEG编码器、跨模态对齐、标题重新排序、加权插值和图像生成。

Result: 生成高质量图像，分类准确率提升13.43%，生成准确率提升15.21%，FID降低36.61%。

Conclusion: 该方法在语义对齐和图像质量上优于现有技术，展示了EEG到图像生成的潜力。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [110] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出CharaConsist方法，通过点追踪注意力和自适应令牌合并，解决文本到图像生成中角色和背景一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在角色大动作变化或背景细节一致性上表现不足，限制了实际应用。

Method: 采用点追踪注意力和自适应令牌合并，结合前景和背景解耦控制。

Result: CharaConsist在连续或离散场景中均能保持细粒度一致性，适用于DiT模型。

Conclusion: CharaConsist扩展了文本到图像生成的应用范围，提升了生成质量。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [111] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种流式4D视觉几何变换器，用于实时4D时空几何感知与重建，借鉴自回归大语言模型的设计理念，支持高效在线处理。


<details>
  <summary>Details</summary>
Motivation: 解决从视频中感知和重建4D时空几何的挑战，满足交互式和实时应用的需求。

Method: 采用因果变换器架构，利用时间因果注意力和历史键值缓存作为隐式内存，实现高效的流式长期4D重建。通过知识蒸馏从密集双向视觉几何变换器（VGGT）中提取知识。

Result: 在多个4D几何感知基准测试中表现优异，提升了在线场景的推理速度，同时保持高质量的空间一致性。

Conclusion: 该方法为可扩展和交互式的4D视觉系统提供了可行路径，代码已开源。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [112] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 该论文综述了深度估计在3D计算机视觉中的重要性，探讨了传统硬件传感器方法的局限性，以及基于视觉方法的挑战和潜力。文章还介绍了深度基础模型的概念，并总结了相关架构、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度估计是3D计算机视觉中的核心任务，但传统硬件传感器方法成本高且受限，而现有视觉方法在泛化和稳定性方面存在不足。深度基础模型的出现为解决这些问题提供了新思路。

Method: 论文通过综述单目、立体、多视角和单目视频设置下的深度学习架构和范式，探讨了深度基础模型的潜力，并总结了大规模数据集和关键训练策略。

Result: 文章总结了深度基础模型的架构和训练方法，强调了其在解决现有挑战中的潜力，并提供了未来研究的路径。

Conclusion: 深度基础模型有望成为解决深度估计问题的关键工具，未来研究应关注其架构优化和大规模数据集的开发。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [113] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: 本文分析了扩散模型（DMs）中毒攻击的影响，并提出了一种新的防御机制Safe-Zone Training（SZT）。


<details>
  <summary>Details</summary>
Motivation: 中毒攻击对扩散模型的鲁棒性构成威胁，尤其是对广泛使用的个性化技术文本反转（TI）。

Method: 提出Semantic Sensitivity Maps可视化中毒影响，发现DMs在低噪声样本上学习行为不均匀，并设计SZT防御机制（JPEG压缩、高时间步限制和损失掩码）。

Result: SZT显著提高了TI对中毒攻击的鲁棒性，生成质量优于现有防御方法。

Conclusion: SZT通过限制学习区域和避免低时间步的对抗信号，有效防御中毒攻击。

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [114] [Breaking a 5-Bit Elliptic Curve Key using a 133-Qubit Quantum Computer](https://arxiv.org/abs/2507.10592)
*Steve Tippeconnic*

Main category: cs.CR

TL;DR: 通过量子攻击成功破解5位椭圆曲线加密密钥，使用IBM的133量子比特设备实现。


<details>
  <summary>Details</summary>
Motivation: 验证量子计算在破解椭圆曲线加密中的实际可行性。

Method: 使用15量子比特电路（10逻辑比特和5辅助比特），在IBM设备上运行，通过量子干涉提取密钥。

Result: 成功提取出密钥k=7，验证了量子攻击的有效性。

Conclusion: 量子计算在密码学攻击中具有实际潜力，代码和数据公开以供复现。

Abstract: This experiment breaks a 5-bit elliptic curve cryptographic key using a
Shor-style quantum attack. Executed on IBM's 133-qubit ibm_torino with Qiskit
Runtime 2.0, a 15-qubit circuit, comprised of 10 logical qubits and 5 ancilla,
interferes over an order-32 elliptic curve subgroup to extract the secret
scalar k from the public key relation Q = kP, without ever encoding k directly
into the oracle. From 16,384 shots, the quantum interference reveals a diagonal
ridge in the 32 x 32 QFT outcome space. The quantum circuit, over 67,000 layers
deep, produced valid interference patterns despite extreme circuit depth, and
classical post-processing revealed k = 7 in the top 100 invertible (a, b)
results. All code, circuits, and raw data are publicly available for
replication.

</details>


### [115] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为LaSM的层间缩放机制，用于增强多模态大语言模型（MLLM）GUI代理对弹出式环境注入攻击的防御能力，无需额外训练即可显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在面对弹出式恶意视觉元素时表现脆弱，现有防御方法要么成本高昂，要么在归纳干扰下效果不佳。

Method: 通过系统研究攻击如何改变GUI代理的注意力行为，发现层间注意力差异模式，并基于此提出LaSM机制，选择性放大关键层的注意力和MLP模块。

Result: 在12种弹出式扰动和4种模型骨干上的实验表明，LaSM显著提升了防御成功率，结合提示级警报后，在强归纳攻击下鲁棒性超过98%。

Conclusion: 注意力错位是MLLM代理的核心漏洞，通过选择性层间调制可有效解决。

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [116] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: 论文探讨了如何结合博弈论和基于大型语言模型（LLM）的智能代理来提升网络安全防御，提出了一种理论与实践结合的新路径。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全方法依赖手动响应和脆弱启发式，需要更智能、主动的防御系统。博弈论和LLM的结合为建模对抗行为、设计战略防御提供了新思路。

Method: 通过整合博弈论框架（如静态、动态、贝叶斯和信号博弈）和LLM驱动的智能代理，将抽象策略转化为实际决策。

Result: 提出了一种新的理论框架和软件设计方法，强调模块化、自适应和信任意识。

Conclusion: 博弈论与LLM代理的结合为网络安全提供了更丰富的理论基础和新型解决方案，推动了智能自适应系统的实现。

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [117] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: 提出了一种结合MFCC和ResNet-18的新方法，用于IoT网络流量中的异常检测，通过自适应频谱特征提取和多分类任务提升性能。


<details>
  <summary>Details</summary>
Motivation: IoT网络的快速扩展导致安全漏洞激增，亟需高效的异常检测技术。

Method: 利用可学习的MFCC和ResNet-18模型，将原始信号转换为高维空间以增强分类效果。

Result: 在CICIoT2023、NSL-KDD和IoTID20数据集上验证了方法的有效性。

Conclusion: 结合自适应信号处理和深度学习架构，为异构IoT网络提供了可扩展的异常检测方案。

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [118] [Crypto-Assisted Graph Degree Sequence Release under Local Differential Privacy](https://arxiv.org/abs/2507.10627)
*Xiaojian Zhang,Junqing Wang,Kerui Chen,Peiyuan Zhao,Huiyuan Bai*

Main category: cs.CR

TL;DR: 论文提出CADR-LDP框架，结合加密技术和差分隐私机制，优化节点度序列发布，解决现有方法在阈值选择和通信成本上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于边删除的图投影方法在阈值参数选择上存在权衡，导致噪声过大或边删除过多，且通信成本高。

Method: CADR-LDP采用加密辅助的最优阈值选择方法（Optimal-θ-Selection）和局部投影边添加方法（LPEA-LOW），优先处理低度节点以减少误差。

Result: 理论证明CADR-LDP满足ε-节点局部差分隐私，实验在八个图数据集上表现优于现有方法。

Conclusion: CADR-LDP通过优化阈值选择和边添加策略，显著提升了度序列发布的准确性和效率。

Abstract: Given a graph $G$ defined in a domain $\mathcal{G}$, we investigate locally
differentially private mechanisms to release a degree sequence on $\mathcal{G}$
that accurately approximates the actual degree distribution. Existing solutions
for this problem mostly use graph projection techniques based on edge deletion
process, using a threshold parameter $\theta$ to bound node degrees. However,
this approach presents a fundamental trade-off in threshold parameter
selection. While large $\theta$ values introduce substantial noise in the
released degree sequence, small $\theta$ values result in more edges removed
than necessary. Furthermore, $\theta$ selection leads to an excessive
communication cost. To remedy existing solutions' deficiencies, we present
CADR-LDP, an efficient framework incorporating encryption techniques and
differentially private mechanisms to release the degree sequence. In CADR-LDP,
we first use the crypto-assisted Optimal-$\theta$-Selection method to select
the optimal parameter with a low communication cost. Then, we use the LPEA-LOW
method to add some edges for each node with the edge addition process in local
projection. LPEA-LOW prioritizes the projection with low-degree nodes, which
can retain more edges for such nodes and reduce the projection error.
Theoretical analysis shows that CADR-LDP satisfies $\epsilon$-node local
differential privacy. The experimental results on eight graph datasets show
that our solution outperforms existing methods.

</details>


### [119] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li,Sharad Mehrota,Shantanu Sharma,Komal Kumari*

Main category: cs.CR

TL;DR: 提出了一种基于密钥的访问控制技术，用于安全外包键值存储，支持关键词检索并防止数据泄露。


<details>
  <summary>Details</summary>
Motivation: 解决键值存储外包中的安全问题，防止数据、用户权限或查询结果大小的泄露。

Method: 采用Shamir的秘密共享技术，提供无条件安全性，支持关键词检索，并检测恶意行为。

Result: 在500,000个文件中处理5,000个关键词耗时231.5毫秒，能有效防止恶意访问和数据篡改。

Conclusion: 该方法在安全性和效率上表现良好，适用于安全外包键值存储场景。

Abstract: This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [120] [3S-Attack: Spatial, Spectral and Semantic Invisible Backdoor Attack Against DNN Models](https://arxiv.org/abs/2507.10733)
*Jianyao Yin,Luca Arnaboldi,Honglong Chen,Pascal Berrang*

Main category: cs.CR

TL;DR: 提出了一种新型后门攻击3S-attack，在空间、频谱和语义域均具有隐蔽性，利用语义特征作为触发机制，并通过实验验证其难以被检测。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究多集中在空间和频谱域，语义域研究较少，3S-attack填补了这一空白。

Method: 利用Grad-CAM和预训练模型提取语义特征作为触发机制，在频谱域嵌入触发，并通过像素级限制确保隐蔽性。

Result: 实验证明3S-attack在多种数据集上具有高隐蔽性，现有防御方法难以检测。

Conclusion: 3S-attack展示了后门攻击的新威胁，强调了需要更强的防御机制以确保AI安全。

Abstract: Backdoor attacks involve either poisoning the training data or directly
modifying the model in order to implant a hidden behavior, that causes the
model to misclassify inputs when a specific trigger is present. During
inference, the model maintains high accuracy on benign samples but
misclassifies poisoned samples into an attacker-specified target class.
Existing research on backdoor attacks has explored developing triggers in the
spatial, spectral (frequency), and semantic (feature) domains, aiming to make
them stealthy. While some approaches have considered designing triggers that
are imperceptible in both spatial and spectral domains, few have incorporated
the semantic domain. In this paper, we propose a novel backdoor attack, termed
3S-attack, which is stealthy across the spatial, spectral, and semantic
domains. The key idea is to exploit the semantic features of benign samples as
triggers, using Gradient-weighted Class Activation Mapping (Grad-CAM) and a
preliminary model for extraction. The trigger is then embedded in the spectral
domain, followed by pixel-level restrictions after converting the samples back
to the spatial domain. This process minimizes the distance between poisoned and
benign samples, making the attack harder to detect by existing defenses and
human inspection. Extensive experiments on various datasets, along with
theoretical analysis, demonstrate the stealthiness of 3S-attack and highlight
the need for stronger defenses to ensure AI security. Our code is available at:
https://anonymous.4open.science/r/anon-project-3776/

</details>


### [121] [Contrastive-KAN: A Semi-Supervised Intrusion Detection Framework for Cybersecurity with scarce Labeled Data](https://arxiv.org/abs/2507.10808)
*Mohammad Alikhani,Reza Kazemi*

Main category: cs.CR

TL;DR: 提出了一种基于半监督对比学习的实时入侵检测系统，利用Kolmogorov-Arnold网络（KAN）解决数据标注不足的问题，并在多个数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 在第四次工业革命时代，物联网和工业物联网环境中的网络安全和入侵检测至关重要，但标注数据稀缺和高成本阻碍了机器学习模型的有效训练。

Method: 采用半监督对比学习框架和KAN网络，利用大量未标注数据区分正常与攻击行为，并在UNSW-NB15、BoT-IoT和Gas Pipeline数据集上进行验证。

Result: 实验表明，该方法在仅使用少量标注样本的情况下，性能优于现有对比学习方法，且KAN在检测准确性和鲁棒性上优于传统多层感知机（MLP）。

Conclusion: KAN能够建模复杂关系并提供可解释性，适用于安全关键环境，支持多分类任务，具有规则提取潜力。

Abstract: In the era of the Fourth Industrial Revolution, cybersecurity and intrusion
detection systems are vital for the secure and reliable operation of IoT and
IIoT environments. A key challenge in this domain is the scarcity of labeled
cyber-attack data, as most industrial systems operate under normal conditions.
This data imbalance, combined with the high cost of annotation, hinders the
effective training of machine learning models. Moreover, rapid detection of
attacks is essential, especially in critical infrastructure, to prevent
large-scale disruptions. To address these challenges, we propose a real-time
intrusion detection system based on a semi-supervised contrastive learning
framework using the Kolmogorov-Arnold Network (KAN). Our method leverages
abundant unlabeled data to distinguish between normal and attack behaviors
effectively. We validate our approach on three benchmark datasets: UNSW-NB15,
BoT-IoT, and Gas Pipeline, using only 2.20 percent, 1.28 percent, and 8 percent
of labeled samples, respectively, to simulate real-world conditions.
Experimental results show that our method outperforms existing contrastive
learning-based approaches. We further compare KAN with a traditional multilayer
perceptron (MLP), demonstrating KAN's superior performance in both detection
accuracy and robustness under limited supervision. KAN's ability to model
complex relationships and its learnable activation functions are also explored
and visualized, offering interpretability and potential for rule extraction.
The method supports multi-class classification and proves effective in
safety-critical environments where reliability is paramount.

</details>


### [122] [Reporte de vulnerabilidades en IIoT. Proyecto DEFENDER](https://arxiv.org/abs/2507.10819)
*Pedro Almansa Jiménez,Lorenzo Fernández Maimó,Ángel Luis Peráles Gómez*

Main category: cs.CR

TL;DR: 该技术报告对工业物联网（IIoT）设备进行了全面研究，分析其场景、漏洞及安全威胁，并提出基于机器学习的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究IIoT设备的场景和漏洞，以提升工业环境的安全性。

Method: 分类IIoT设备，分析其功能、交互方式及漏洞，并结合实际案例和机器学习提出对策。

Result: 揭示了IIoT设备的潜在威胁，并提出了有效的安全措施。

Conclusion: 机器学习在提升IIoT安全性方面具有重要作用。

Abstract: The main objective of this technical report is to conduct a comprehensive
study on devices operating within Industrial Internet of Things (IIoT)
environments, describing the scenarios that define this category and analysing
the vulnerabilities that compromise their security. To this end, the report
seeks to identify and examine the main classes of IIoT devices, detailing their
characteristics, functionalities, and roles within industrial systems. This
analysis enables a better understanding of how these devices interact and
fulfil the requirements of critical industrial environments. The report also
explores the specific contexts in which these devices operate, highlighting the
distinctive features of industrial scenarios and the conditions under which the
devices function. Furthermore, it analyses the vulnerabilities affecting IIoT
devices, outlining their vectors, targets, impact, and consequences. The report
then describes the typical phases of an attack, along with a selection of
real-world documented incidents. These cases are classified according to the
taxonomy presented in Section 3, providing a comprehensive view of the
potential threats to security and assessing the impact these vulnerabilities
may have on industrial environments. Finally, the report presents a compilation
of some of the most recent and effective security countermeasures as potential
solutions to the security challenges faced by industrial systems. Special
emphasis is placed on the role of Machine Learning in the development of these
approaches, underscoring its importance in enhancing industrial cybersecurity.

</details>


### [123] [REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack](https://arxiv.org/abs/2507.10836)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Main category: cs.CR

TL;DR: 论文提出了REAL-IoT框架，用于评估GNN在物联网环境中的鲁棒性，揭示了其在分布漂移和真实攻击下的性能下降，并探索了LLM在增强鲁棒性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有GNN-based NIDS评估通常基于单一数据集，缺乏对分布漂移和真实对抗攻击的鲁棒性测试，导致高估其性能。

Method: 提出REAL-IoT框架，整合经典数据集以评估泛化能力，并引入真实IoT测试床收集的新型入侵数据集。此外，利用LLM分析网络数据并过滤可疑流量。

Result: 评估显示GNN模型在真实场景中性能下降，暴露其对漂移和攻击的脆弱性。LLM过滤展示了提升鲁棒性的潜力。

Conclusion: 研究强调了真实威胁建模和严格评估的必要性，为开发鲁棒的IoT入侵检测系统提供了方向。

Abstract: Graph Neural Network (GNN)-based network intrusion detection systems (NIDS)
are often evaluated on single datasets, limiting their ability to generalize
under distribution drift. Furthermore, their adversarial robustness is
typically assessed using synthetic perturbations that lack realism. This
measurement gap leads to an overestimation of GNN-based NIDS resilience. To
address the limitations, we propose \textbf{REAL-IoT}, a comprehensive
framework for robustness evaluation of GNN-based NIDS in IoT environments. Our
framework presents a methodology that creates a unified dataset from canonical
datasets to assess generalization under drift. In addition, it features a novel
intrusion dataset collected from a physical IoT testbed, which captures network
traffic and attack scenarios under real-world settings. Furthermore, using
REAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze
network data and mitigate the impact of adversarial examples by filtering
suspicious flows. Our evaluations using REAL-IoT reveal performance drops in
GNN models compared to results from standard benchmarks, quantifying their
susceptibility to drift and realistic attacks. We also demonstrate the
potential of LLM-based filtering to enhance robustness. These findings
emphasize the necessity of realistic threat modeling and rigorous measurement
practices for developing resilient IoT intrusion detection systems.

</details>


### [124] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi,Hongwei Li,Jiahao Yu,Xinqian Sun,Wenbo Guo,Xinyu Xing*

Main category: cs.CR

TL;DR: 协作模糊测试结合多个模糊测试工具，动态选择适合不同程序的组合，提供稳定且鲁棒的性能，但面临计算资源需求和资源分配效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决单个模糊测试工具因依赖特定假设而无法适应不同程序的问题，推动通用模糊测试解决方案的发展。

Method: 通过协作模糊测试框架，动态选择和组合多个模糊测试工具。

Result: 协作模糊测试能够提供更稳定和鲁棒的测试性能，但资源消耗和分配效率成为限制因素。

Conclusion: 协作模糊测试是通用模糊测试的有前景方向，但需解决资源管理和分配问题。

Abstract: Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [125] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: 论文介绍了PhreshPhish数据集，解决了现有钓鱼检测数据集的不足，如数据质量低、泄漏和不现实的基准率，并提供了基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击的威胁日益严重，但现有数据集质量低、基准不现实，阻碍了机器学习在钓鱼检测中的进展。

Method: 提出PhreshPhish数据集，规模更大、质量更高，并设计了一套基准测试套件，以减少泄漏、增加任务难度、提高多样性并调整基准率。

Result: PhreshPhish数据集显著优于现有公共数据集，提供了更真实的模型评估基准。

Conclusion: PhreshPhish数据集和基准测试将促进钓鱼检测领域的标准化模型比较和进一步研究。

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [126] [From Alerts to Intelligence: A Novel LLM-Aided Framework for Host-based Intrusion Detection](https://arxiv.org/abs/2507.10873)
*Danyu Sun,Jinghuai Zhang,Jiacen Xu,Yu Zheng,Yuan Tian,Zhou Li*

Main category: cs.CR

TL;DR: SHIELD是一个基于定制化LLM管道的HIDS系统，通过整合多种技术解决了LLM在入侵检测中的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: HIDS在部署中常因高误报率和结果不一致等问题受到批评，而LLM因其语义分析能力有望改进HIDS，但直接使用LLM效果不佳。

Method: SHIELD整合了事件级MAE、攻击证据识别与扩展、确定性数据增强（DDA）和多用途提示等技术。

Result: 在三个日志数据集上的实验表明，SHIELD性能优于5种代表性HIDS。

Conclusion: LLM在入侵检测中具有巨大潜力，SHIELD为未来研究提供了方向。

Abstract: Host-based intrusion detection system (HIDS) is a key defense component to
protect the organizations from advanced threats like Advanced Persistent
Threats (APT). By analyzing the fine-grained logs with approaches like data
provenance, HIDS has shown successes in capturing sophisticated attack traces.
Despite the progresses embarked by the research community and industry, HIDS
still frequently encounters backlash from their operators in the deployed
environments, due to issues like high false-positive rate, inconsistent
outcomes across environments and human-unfriendly detection results. Large
Language Models (LLMs) have great potentials to advance the state of HIDS,
given their extensive knowledge of attack techniques and their ability to
detect anomalies through semantic analysis, anchored by recent studies. Yet,
our preliminary analysis indicates that building an HIDS by naively prompting
an LLM is unlikely to succeed. In this work, we explore the direction of
building a customized LLM pipeline for HIDS and develop a system named SHIELD.
SHIELD addresses challenges related to LLM's token limits, confusion of
background noises, etc., by integrating a variety of techniques like
event-level Masked Autoencoder (MAE) for attack window detection, attack
evidence identification and expansion, Deterministic Data Augmentation (DDA)
for profiling normal activities, and multi-purpose prompting that guides the
LLM to conduct precise and interpretable attack investigations. Extensive
experiments on three log datasets (DARPA-E3, NodLink-simulated-data and
ATLASv2) show that SHIELD consistently achieves outstanding performance in
comparison with 5 representative HIDS. These findings highlight the potential
of LLMs as powerful tools for intrusion detection and pave the way for future
research in this domain.

</details>


### [127] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: MalCodeAI是一种语言无关的多阶段AI管道，用于自主代码安全分析和修复，结合代码分解和语义推理，支持14种编程语言，并在漏洞检测和修复方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测工具的局限性及网络威胁的复杂性促使开发新型软件安全解决方案。

Method: 使用微调的Qwen2.5-Coder-3B-Instruct模型，结合LoRA优化和MLX框架，分两阶段进行代码分解、漏洞检测与修复。

Result: 在验证损失上表现优异（功能分解0.397，漏洞检测0.199），支持零日漏洞检测，开发者评价高。

Conclusion: MalCodeAI为智能、可解释且以开发者为中心的软件安全解决方案提供了重要进展。

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [128] [DVFS: A Dynamic Verifiable Fuzzy Search Service for Encrypted Cloud Data](https://arxiv.org/abs/2507.10927)
*Jie Zhang,Xiaohong Li,Man Zheng,Zhe Hou,Guangdong Bai,Ruitao Feng*

Main category: cs.CR

TL;DR: DVFS提出了一种动态可验证的模糊搜索服务，解决了云存储中加密数据检索的安全与效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有模糊多关键词搜索方案在安全性和效率之间存在矛盾，线性搜索安全但效率低，树状索引效率高但存在分支泄漏风险。

Method: DVFS结合局部敏感哈希与虚拟二叉树实现自适应安全模糊搜索，采用双存储库版本控制机制支持动态更新，并通过区块链验证系统确保正确性。

Result: DVFS将搜索复杂度降至次线性（O(log n)），同时解决了分支泄漏问题，支持动态操作并确保完整性。

Conclusion: DVFS通过创新方法解决了安全与性能的悖论，为加密数据检索提供了高效且可信的动态操作方案。

Abstract: Cloud storage introduces critical privacy challenges for encrypted data
retrieval, where fuzzy multi-keyword search enables approximate matching while
preserving data confidentiality. Existing solutions face fundamental trade-offs
between security and efficiency: linear-search mechanisms provide adaptive
security but incur prohibitive overhead for large-scale data, while tree-based
indexes improve performance at the cost of branch leakage vulnerabilities.
  To address these limitations, we propose DVFS - a dynamic verifiable fuzzy
search service with three core innovations: (1) An \textit{adaptive-secure
fuzzy search} method integrating locality-sensitive hashing with virtual binary
trees, eliminating branch leakage while reducing search complexity from linear
to sublinear ($O(\log n)$ time); (2) A \textit{dual-repository version control}
mechanism supporting dynamic updates with forward privacy, preventing
information leakage during operations; (3) A \textit{blockchain-based
verification system} that ensures correctness and completeness via smart
contracts, achieving $O(\log n)$ verification complexity.
  Our solution advances secure encrypted retrieval by simultaneously resolving
the security-performance paradox and enabling trustworthy dynamic operations.

</details>


### [129] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: NeuralMark是一种基于哈希水印过滤器的神经网络水印方法，旨在解决权重方法易受伪造和覆盖攻击的问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为有价值的数字资产需要所有权保护，而现有权重水印方法易受攻击。

Method: 利用哈希函数从密钥生成不可逆二进制水印，作为过滤器选择嵌入参数，并结合平均池化抵御微调和剪枝攻击。

Result: 在13种卷积和Transformer架构中验证了方法的有效性和鲁棒性，覆盖多种任务。

Conclusion: NeuralMark提供了一种安全、通用且鲁棒的神经网络水印解决方案。

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


### [130] [FacialMotionID: Identifying Users of Mixed Reality Headsets using Abstract Facial Motion Representations](https://arxiv.org/abs/2507.11138)
*Adriano Castro,Simon Hanisch,Matin Fallahi,Thorsten Strufe*

Main category: cs.CR

TL;DR: 研究探讨了混合现实头显中的面部动作捕捉数据可能导致用户身份识别或敏感属性推断的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 随着混合现实系统日益普及和沉浸式体验增强，面部动作数据作为行为生物特征可能引发新的隐私问题。

Method: 研究收集了116名参与者在三种头显设备上的面部、眼部和头部动作数据，分析其用于数字化身动画的抽象表示。

Result: 实验表明，用户身份可从数据中重新识别（准确率高达98%），且情绪状态推断准确率达86%。

Conclusion: 研究结果强调了混合现实环境中面部动作追踪的潜在隐私风险。

Abstract: Facial motion capture in mixed reality headsets enables real-time avatar
animation, allowing users to convey non-verbal cues during virtual
interactions. However, as facial motion data constitutes a behavioral
biometric, its use raises novel privacy concerns. With mixed reality systems
becoming more immersive and widespread, understanding whether face motion data
can lead to user identification or inference of sensitive attributes is
increasingly important.
  To address this, we conducted a study with 116 participants using three types
of headsets across three sessions, collecting facial, eye, and head motion data
during verbal and non-verbal tasks. The data used is not raw video, but rather,
abstract representations that are used to animate digital avatars. Our analysis
shows that individuals can be re-identified from this data with up to 98%
balanced accuracy, are even identifiable across device types, and that
emotional states can be inferred with up to 86% accuracy. These results
underscore the potential privacy risks inherent in face motion tracking in
mixed reality environments.

</details>


### [131] [Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities](https://arxiv.org/abs/2507.11155)
*Yiting Qu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 该论文研究了视觉语言模型（VLM）在识别多模态（文本和图像）不安全概念时的表现，并提出了基于强化学习的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 由于VLM在伦理标准和推理能力上的优势，被广泛用于识别不安全内容，但其在多模态下的表现尚不明确。

Method: 作者构建了UnsafeConcepts数据集，评估了8种VLM的感知和对齐能力，并提出了一种基于PPO的强化学习方法。

Result: 实验表明，大多数VLM能准确感知不安全概念，但存在误分类问题，且开源VLM在视觉和文本模态间存在差距。提出的方法显著提升了VLM的对齐能力。

Conclusion: 论文通过数据集、评估结果和对齐方法，为推进安全VLM的研究提供了支持。

Abstract: Vision-language models (VLMs) are increasingly applied to identify unsafe or
inappropriate images due to their internal ethical standards and powerful
reasoning abilities. However, it is still unclear whether they can recognize
various unsafe concepts when presented in different modalities, such as text
and images. To address this, we first compile the UnsafeConcepts dataset,
featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and
``Assaults,'' along with associated 1.5K images. We then conduct a systematic
evaluation of VLMs' perception (concept recognition) and alignment (ethical
reasoning) capabilities. We assess eight popular VLMs and find that, although
most VLMs accurately perceive unsafe concepts, they sometimes mistakenly
classify these concepts as safe. We also identify a consistent modality gap
among open-source VLMs in distinguishing between visual and textual unsafe
concepts. To bridge this gap, we introduce a simplified reinforcement learning
(RL)-based approach using proximal policy optimization (PPO) to strengthen the
ability to identify unsafe concepts from images. Our approach uses reward
scores based directly on VLM responses, bypassing the need for collecting
human-annotated preference data to train a new reward model. Experimental
results show that our approach effectively enhances VLM alignment on images
while preserving general capabilities. It outperforms baselines such as
supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope
our dataset, evaluation findings, and proposed alignment solution contribute to
the community's efforts in advancing safe VLMs.

</details>


### [132] [LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification](https://arxiv.org/abs/2507.11310)
*Fengxiao Tang,Huan Li,Ming Zhao,Zongzong Wu,Shisong Peng,Tao Yin*

Main category: cs.CR

TL;DR: LRCTI是一个基于大型语言模型（LLM）的框架，用于多步验证网络威胁情报（CTI）的可信度，通过文本摘要、自适应证据检索和自然语言推理模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统CTI可信度验证方法缺乏鲁棒性和透明度，无法有效处理不完整、异构或嘈杂的情报，LRCTI旨在解决这些问题。

Method: LRCTI采用三步框架：文本摘要、自适应多步证据检索和基于提示的自然语言推理，结合LLM反馈提升效果。

Result: 在CTI-200和PolitiFact数据集上，LRCTI的F1-Macro和F1-Micro分数分别达到90.9%和93.6%，优于现有方法。

Conclusion: LRCTI提供了一种可扩展、准确且可解释的自动化CTI可信度验证解决方案。

Abstract: Verifying the credibility of Cyber Threat Intelligence (CTI) is essential for
reliable cybersecurity defense. However, traditional approaches typically treat
this task as a static classification problem, relying on handcrafted features
or isolated deep learning models. These methods often lack the robustness
needed to handle incomplete, heterogeneous, or noisy intelligence, and they
provide limited transparency in decision-making-factors that reduce their
effectiveness in real-world threat environments. To address these limitations,
we propose LRCTI, a Large Language Model (LLM)-based framework designed for
multi-step CTI credibility verification. The framework first employs a text
summarization module to distill complex intelligence reports into concise and
actionable threat claims. It then uses an adaptive multi-step evidence
retrieval mechanism that iteratively identifies and refines supporting
information from a CTI-specific corpus, guided by LLM feedback. Finally, a
prompt-based Natural Language Inference (NLI) module is applied to evaluate the
credibility of each claim while generating interpretable justifications for the
classification outcome. Experiments conducted on two benchmark datasets,
CTI-200 and PolitiFact show that LRCTI improves F1-Macro and F1-Micro scores by
over 5%, reaching 90.9% and 93.6%, respectively, compared to state-of-the-art
baselines. These results demonstrate that LRCTI effectively addresses the core
limitations of prior methods, offering a scalable, accurate, and explainable
solution for automated CTI credibility verification

</details>


### [133] [A Review of Privacy Metrics for Privacy-Preserving Synthetic Data Generation](https://arxiv.org/abs/2507.11324)
*Frederik Marinus Trudslev,Matteo Lissandrini,Juan Manuel Rodriguez,Martin Bøgsted,Daniele Dell'Aglio*

Main category: cs.CR

TL;DR: 论文探讨了隐私保护合成数据生成（PP-SDG）中的隐私度量（PMs）问题，总结了17种不同隐私度量的假设和数学公式。


<details>
  <summary>Details</summary>
Motivation: 由于差分隐私（DP）中的隐私损失（ε）难以直观解释，需要更透明的隐私风险度量方法。

Method: 提出并分析了17种隐私度量的假设和数学公式。

Result: 总结了这些隐私度量的计算方式及其在PP-SDG机制中的应用。

Conclusion: 为隐私保护合成数据生成提供了更全面的隐私度量框架。

Abstract: Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce
synthetic datasets from personal data while maintaining privacy and utility.
Differential privacy (DP) is the property of a PP-SDG mechanism that
establishes how protected individuals are when sharing their sensitive data. It
is however difficult to interpret the privacy loss ($\varepsilon$) expressed by
DP. To make the actual risk associated with the privacy loss more transparent,
multiple privacy metrics (PMs) have been proposed to assess the privacy risk of
the data. These PMs are utilized in separate studies to assess newly introduced
PP-SDG mechanisms. Consequently, these PMs embody the same assumptions as the
PP-SDG mechanism they were made to assess. Therefore, a thorough definition of
how these are calculated is necessary. In this work, we present the assumptions
and mathematical formulations of 17 distinct privacy metrics.

</details>


### [134] [Demo: Secure Edge Server for Network Slicing and Resource Allocation in Open RAN](https://arxiv.org/abs/2507.11499)
*Adhwaa Alchaab,Ayman Younis,Dario Pompili*

Main category: cs.CR

TL;DR: SnSRIC是一个智能网络切片框架，用于在Open RAN环境中抵御DDoS攻击，通过AI动态分配资源并确保安全性。


<details>
  <summary>Details</summary>
Motivation: NGRAN需要满足严格的安全性和延迟要求，但面临基础设施安全和动态资源分配的挑战。

Method: SnSRIC利用AI驱动的xApp动态分配PRB，检测异常行为并通过E2接口限制恶意信号。

Result: 系统能区分恶意与合法设备，保障合法用户的服务连续性。

Conclusion: SnSRIC为Open RAN提供了一种有效的安全与资源管理解决方案。

Abstract: Next-Generation Radio Access Networks (NGRAN) aim to support diverse vertical
applications with strict security, latency, and Service-Level Agreement (SLA)
requirements. These demands introduce challenges in securing the
infrastructure, allocating resources dynamically, and enabling real-time
reconfiguration. This demo presents SnSRIC, a secure and intelligent network
slicing framework that mitigates a range of Distributed Denial-of-Service
(DDoS) attacks in Open RAN environments. SnSRIC incorporates an AI-driven xApp
that dynamically allocates Physical Resource Blocks (PRBs) to active users
while enforcing slice-level security. The system detects anomalous behavior,
distinguishes between benign and malicious devices, and uses the E2 interface
to throttle rogue signaling while maintaining service continuity for legitimate
users.

</details>


### [135] [ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/abs/2507.11500)
*Zhengyue Zhao,Yingzi Ma,Somesh Jha,Marco Pavone,Chaowei Xiao*

Main category: cs.CR

TL;DR: 论文提出ARMOR框架，通过结构化推理增强LLMs的安全性，显著提升对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs安全对齐方法易被恶意指令绕过，需更结构化的人类对齐推理过程。

Method: ARMOR框架分三步：检测越狱策略、提取用户真实意图、应用安全策略分析。

Result: ARMOR在对抗攻击和多个安全基准测试中表现优于现有方法。

Conclusion: ARMOR通过结构化推理显著提升LLMs安全性，为未来安全对齐提供新方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generative
capabilities. However, their susceptibility to misuse has raised significant
safety concerns. While post-training safety alignment methods have been widely
adopted, LLMs remain vulnerable to malicious instructions that can bypass
safety constraints. Recent efforts have introduced inference-time safety
reasoning (system-2 alignment), where LLMs conduct a reasoning process to
perform safety verification before final response. We show, however, that these
checks are driven by ad-hoc reasoning that diverges from the structured human
process, where they first discern a user's true intent, then evaluate the
associated risk based on the true intent. Consequently, these defenses remain
vulnerable to sophisticated jailbreak prompts that cloak harmful goals in
seemingly benign language. To build secure and safe LLMs, we propose a
reasoning-based safety alignment framework, ARMOR, that replaces the ad-hoc
chains of thought reasoning process with human-aligned, structured one. At
inference, ARMOR (1) detects likely jailbreak strategies, (2) extracts the
user's core intent while discarding deceptive instructions, and (3) applies a
policy-grounded safety analysis to the purified request. ARMOR is evaluated on
adaptive jailbreak attacks and multiple safety benchmarks, and a test-time
scaling is conducted to further improve its performance. Results demonstrate
that ARMOR significantly enhances the robustness against state-of-the-art
adaptive jailbreak attacks and outperforms recent reasoning-based aligned
models across various safety benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [136] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: 提出了一种新的工具间匹配（TTTM）分析方法，解决了传统方法在异构设备环境中的局限性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统TTTM方法依赖静态配置或黄金参考数据，难以在商业生产线中实现，且在异构设备环境中表现不佳。

Method: 提出基于数据方差和模式数量的新分析流程，包括单变量和多变量方法。

Result: 最佳单变量方法在方差和模式数量上的相关系数分别超过0.95和0.5；多变量方法与最佳单变量方法的相关系数超过0.75。

Conclusion: 新方法在TTTM问题中表现优异，同时对算法超参数具有敏感性。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [137] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的线性自适应交叉熵损失函数，通过增加一个依赖于真实类预测概率的项，优化分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 标准交叉熵损失函数在分类任务中可能存在优化不足的问题，因此提出改进方案以提升性能。

Method: 设计了线性自适应交叉熵损失函数，增加了一个与真实类预测概率相关的项，并在ResNet模型和CIFAR-100数据集上进行了评估。

Result: 实验结果表明，新损失函数在分类准确率上优于标准交叉熵损失，同时保持了相似的效率。

Conclusion: 该方法为损失函数设计提供了新的研究方向，具有潜在的应用价值。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [138] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched是一种新型自适应学习率调度器，通过动态调整学习率提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 预定义和自适应学习率调度器可能导致次优泛化性能，需更有效的方法。

Method: 基于随机过程波动性概念，计算长短期准确率波动比，动态调整学习率。

Result: 在CIFAR-100数据集上，ResNet-18和ResNet-34的top-1准确率分别提升1.4和1.3个百分点。

Conclusion: VolSched通过更长的探索阶段和更平坦的损失曲面，显著提升模型泛化性能。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [139] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: 本文探讨了深度学习和Transformer的数学基础，提出了一个关于Transformer的通用逼近定理，证明单层Transformer可以逼近任何连续序列到序列的映射。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和Transformer在多个领域取得了成功，但其理论理解仍然有限。本文旨在填补这一理论空白。

Method: 回顾了线性代数、概率和优化的关键概念，详细分析了多头自注意力机制和反向传播算法，并提出了一个通用逼近定理。

Result: 证明了单层Transformer（包含一个自注意力层和一个ReLU激活的前馈网络）可以逼近任何连续序列到序列的映射。

Conclusion: 研究结果推进了对Transformer模型的理论理解，并有助于缩小理论与实践之间的差距。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [140] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: 论文提出了MH-FSF框架，用于解决特征选择研究中基准测试不足和数据集依赖的问题，通过模块化设计支持17种方法的实现和系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前特征选择研究存在基准测试不足和依赖专有数据集的问题，影响可重复性和性能。

Method: 提出MH-FSF框架，模块化设计，实现17种特征选择方法，并在10个公开Android恶意软件数据集上评估。

Result: 结果显示性能在不同数据集上存在差异，强调数据预处理和选择标准的重要性。

Conclusion: MH-FSF框架为特征选择研究提供了统一平台，促进方法一致性和严谨性，推动新研究方向。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [141] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: 论文提出OL-MDISF方法，解决在线学习中混合特征、数据漂移和标签缺失问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线学习中，混合特征类型、数据分布漂移和标签缺失是三大挑战，需要一种灵活且高效的方法来解决。

Method: OL-MDISF通过构建潜在copula表示、检测漂移和结构感知伪标记来处理混合特征、漂移和标签缺失。

Result: 在14个真实数据集上的实验表明，OL-MDISF在CER趋势、消融研究和敏感性分析中表现优异。

Conclusion: OL-MDISF为复杂、弱监督的流数据在线学习提供了可复现的基准。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [142] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: 提出了一种名为DTRGC的新方法，用于解决属性缺失图的深度图聚类问题，通过分阶段填补缺失属性并利用聚类信息优化结果。


<details>
  <summary>Details</summary>
Motivation: 属性缺失图的深度图聚类在实际应用中至关重要，但现有方法未能充分利用节点邻域信息，导致结果不可靠。

Method: DTRGC方法包括动态聚类感知特征传播（DCFP）、分层邻域感知填补（HNAI）和跳数表示增强（HRE），分阶段填补缺失属性并优化聚类。

Result: 在六个广泛使用的图数据集上，DTRGC显著提升了属性缺失图下多种DGC方法的聚类性能。

Conclusion: DTRGC通过分阶段填补和聚类信息优化，有效解决了属性缺失图的聚类问题，提升了性能。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [143] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne是一个针对社交网络服务（SNS）的领域特定大语言模型（LLM），通过三阶段训练策略显著提升了多任务性能，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于孤立任务，难以适应多样化的实际场景，且数据扩展效益递减。RedOne旨在打破单任务基线的性能瓶颈，为SNS提供全面解决方案。

Method: 采用三阶段训练策略：持续预训练、监督微调和偏好优化，使用大规模真实数据集。

Result: RedOne在8个主要SNS任务中平均提升14.02%，在双语评估基准中提升7.56%，并在有害内容检测和搜索点击率方面显著优于单任务基线。

Conclusion: RedOne作为领域特定LLM，展现了强大的泛化能力和实际应用潜力。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [144] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD是一个可扩展的框架，用于生成合成布局热图以加速物理设计中的机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在物理设计任务中因高质量、大规模训练数据集稀缺而受限的问题。

Method: 使用扩散模型快速生成多样化的布局热图（如功耗、IR压降、拥塞等）。

Result: 生成了包含20,000多种布局配置的数据集，显著提升了机器学习任务的准确性。

Conclusion: DALI-PD为物理设计中的机器学习研究提供了高效的数据生成解决方案。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [145] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: 阿联酋依赖海水淡化满足90%的饮用水需求，但该过程能耗高且面临气候不确定性挑战。研究提出两阶段预测模型和智能控制逻辑，准确率达98%，并开发了交互式决策支持系统。


<details>
  <summary>Details</summary>
Motivation: 海水淡化在阿联酋能源消耗和碳排放中占比较大，且受气候因素（如AOD）影响显著，亟需提高其可持续性和效率。

Method: 采用两阶段预测模型：第一阶段预测AOD，第二阶段预测淡化效率损失；结合SHAP分析关键因素，并设计基于AOD的智能控制逻辑。

Result: 模型准确率达98%，揭示了系统退化的关键驱动因素，并开发了交互式决策支持工具。

Conclusion: 研究为海水淡化系统提供了气候适应性的预测和控制方案，提升了其可持续性和管理效率。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [146] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA是一个针对医疗联邦学习中标签噪声问题的新框架，通过全局样本选择器和客户端自适应调整机制提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗联邦学习中因数据异质性和标签噪声导致的模型性能下降问题。

Method: 提出全局样本选择器（Global Sample Selector）和客户端自适应调整机制（CAA），结合伪标签生成和鲁棒损失函数。

Result: 在多种噪声条件下（对称、非对称、极端和异质噪声）表现优于现有方法，尤其在极端和异质噪声场景中。

Conclusion: FedGSCA显著提升模型稳定性和噪声处理能力，适用于真实医疗联邦学习场景。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [147] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: 论文重新审视了传统NLP的扩展定律，发现数据质量和训练策略对模型性能的影响，提出了适用于次优扩展的新定律。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中性能提升减速的现象（次优扩展），探讨数据质量和资源分配的作用。

Method: 通过对400多个模型的实证分析，研究数据密度和资源分配对性能的影响。

Result: 发现高数据密度和次优资源分配是导致次优扩展的关键因素，提出了新的扩展定律。

Conclusion: 数据质量和多样性对模型性能至关重要，新扩展定律能更好地预测次优扩展下的性能。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [148] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了为算法设计定制大语言模型（LLMs）的必要性和方法，通过多样性感知排名采样和直接偏好优化，实验表明定制模型优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖通用LLMs，但算法设计是否需要定制模型及其效果尚不明确。

Method: 采用多样性感知排名采样（DAR）平衡数据多样性和质量，结合直接偏好优化对齐任务目标。

Result: 定制LLMs显著优于通用模型，且在相关任务中表现出良好的泛化能力。

Conclusion: 任务特定适配对LLMs在算法设计中具有重要价值，为未来研究开辟了新方向。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [149] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 比较了强化学习（RL）和监督微调（SFT）在数学问题上的表现，发现RL在数学领域略有提升，而SFT在知识密集型任务上表现更差。


<details>
  <summary>Details</summary>
Motivation: 理解RL和SFT在LLM后训练中的动态差异。

Method: 在同一模型和超参数下，比较RL和SFT在数学问题上的表现，并分析模型参数变化。

Result: RL在数学领域有轻微提升，SFT在知识密集型任务上表现更差；SFT对模型参数的修改更显著。

Conclusion: RL可能增强现有能力，而SFT可能替换旧技能；冻结部分模型的效果不明确。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [150] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: 论文研究了预训练大型语言模型中的算法创新对计算资源的需求，发现即使严格的算力限制也不会显著减缓AI算法进步。


<details>
  <summary>Details</summary>
Motivation: 探索算法创新对计算资源的需求及其对AI发展的影响。

Method: 分析了36种Llama 3和DeepSeek-V3中的预训练算法创新，估算其开发所需的FLOP和硬件利用率。

Result: 算力需求每年翻倍，但即使严格限制算力（如GPT-2的训练算力或8个H100 GPU），仍能实现半数创新。

Conclusion: 算力限制不太可能显著减缓AI算法进步。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [151] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: 提出一种基于元学习的框架，用于5G/6G网络中的动态频谱分配，解决了传统深度强化学习样本复杂性和安全风险问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在动态频谱分配中存在样本复杂性和安全风险，需要更高效、安全的方法。

Method: 采用三种元学习架构（MAML、RNN和注意力增强RNN），在模拟动态IAB环境中与非元学习的PPO基线对比。

Result: 注意力元学习代理峰值吞吐量达48 Mbps，远超PPO的10 Mbps，且SINR和延迟违规减少50%以上，公平性指数为0.7。

Conclusion: 元学习是复杂无线系统中智能控制的高效且安全的选择。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [152] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: 该教程概述了基于大语言模型（LLMs）的跨模态时间序列分析，分类了现有方法，并讨论了其应用和挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在时间序列分析中具有潜力，但由于跨模态差异，需要专门的方法来优化其应用。

Method: 提出分类法，将方法分为转换、对齐和融合三类，并讨论其在下游任务中的应用。

Result: 总结了当前进展和挑战，旨在扩展LLMs在实际问题中的应用。

Conclusion: 教程提供了对跨模态时间序列分析的全面理解，并指出了未来研究方向。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [153] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 该论文将扩散和基于流的生成模型扩展到权重空间学习，通过梯度流匹配理论框架统一轨迹推断技术，优化权重生成和初始化。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型在图像、视频和自然语言领域已取得显著成功，但尚未充分探索其在权重空间学习中的应用。

Method: 提出梯度流匹配框架，将梯度下降轨迹建模为轨迹推断问题，结合自编码器、任务上下文数据和Kaiming均匀分布等技术。

Result: 实验表明，该方法在生成权重、下游训练初始化和微调性能上优于基线，并在安全关键系统中检测有害协变量偏移时表现更优。

Conclusion: 该方法为权重空间学习提供了理论框架和实用工具，尤其在安全关键系统中具有潜在应用价值。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [154] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer是一种基于图增强Transformer的深度学习模型，用于预测足球比赛结果，通过多级交互框架捕捉球员和团队的动态关系，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 足球比赛结果预测具有挑战性，现有方法常忽略球员和团队之间的异质交互，影响建模准确性。

Method: HIGFormer包含三个模块：Player Interaction Network（捕捉球员动态）、Team Interaction Network（建模团队历史关系）和Match Comparison Transformer（综合分析预测结果）。

Result: 在WyScout数据集上，HIGFormer显著优于现有方法，并提供球员表现评估的新视角。

Conclusion: HIGFormer通过多级交互框架有效提升预测准确性，为球员评估和团队策略分析提供新工具。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [155] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: 论文提出了一种名为GHPO的新强化学习框架，通过动态调整任务难度和结合模仿学习与探索学习，解决了LLMs在复杂推理任务中的训练不稳定和效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化学习方法在训练大型语言模型（LLMs）时存在训练不稳定和效率低下的问题，尤其是对于资源有限的小型模型。

Method: 提出了Guided Hybrid Policy Optimization（GHPO），通过自适应提示细化动态调整任务难度，结合模仿学习和探索学习。

Result: 在六个数学基准测试中，GHPO平均性能提升约5%，显著优于现有方法。

Conclusion: GHPO提供了一种可扩展且高效的解决方案，显著提升了训练稳定性和最终推理性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [156] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: 提出了一种基于随机傅里叶特征（RFF）的快速无监督时间序列分割方法RFF-GP-HSMM，显著降低了GP-HSMM的计算成本。


<details>
  <summary>Details</summary>
Motivation: 高斯过程隐半马尔可夫模型（GP-HSMM）在处理大规模时间序列数据时，由于需要计算N×N核矩阵的逆，计算成本高昂。

Method: 通过随机傅里叶特征（RFF）近似高斯过程，将其转化为线性回归问题，避免了核矩阵的逆运算。

Result: 在CMU运动捕捉数据集上，新方法的分割性能与传统方法相当，但速度提高了约278倍（39,200帧数据）。

Conclusion: RFF-GP-HSMM在保持性能的同时显著提升了计算效率，适用于大规模时间序列分割。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [157] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet是一种基于Hopfield增强的稀疏空间注意力网络，用于解决无人机动态站点选址问题，显著提高了计算效率和解质量。


<details>
  <summary>Details</summary>
Motivation: 城市低空无人机经济的快速发展对动态选址提出了新挑战，传统深度强化学习方法在处理大规模问题时存在计算复杂度瓶颈。

Method: 提出GeoHopNet，包含四种创新：距离偏置多头注意力、K最近邻稀疏注意力、Hopfield外部记忆模块和记忆正则化策略。

Result: GeoHopNet将可解问题规模扩大，1000节点实例下仅需0.1秒找到高质量解（0.22%最优性差距），优于现有方法。

Conclusion: GeoHopNet通过创新设计显著提升了动态无人机站点选址问题的计算效率和解质量。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [158] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP是一种简单、低开销的基线方法，结合了ReLUDown和Decreasing Backpropagation两种机制，在Continual ImageNet基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中模型平衡可塑性和稳定性的问题。

Method: 结合ReLUDown（轻量级激活修改）和Decreasing Backpropagation（梯度调度方案）。

Result: 在Continual ImageNet基准测试中表现优于或与现有方法相当，同时降低计算成本。

Conclusion: RDBP为持续学习提供了实用解决方案，并为未来方法设定了基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [159] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier是一种新型分类框架，用对角高斯分布替代传统确定性逻辑，通过最小化KL散度统一不确定性校准和潜在控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统分类器中温度缩放和流形近似的问题，提供更自然的类别置信度和几何一致性解释。

Method: 使用对角高斯分布的逻辑，最小化预测高斯分布与单位各向同性高斯之间的KL散度。

Result: 在CIFAR-10和CIFAR-100上，ZClassifier在鲁棒性、校准和潜在分离方面优于softmax分类器。

Conclusion: ZClassifier通过高斯语义潜力解释逻辑，有效统一了不确定性校准和潜在控制。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [160] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: 提出了一种基于Hopfield神经网络的轻量级AI模型，用于生物声学分析，具有快速训练、低能耗和高准确性的特点。


<details>
  <summary>Details</summary>
Motivation: 解决生物声学分析中数据量大、现有AI模型训练数据有限、能耗高和硬件要求高的问题。

Method: 使用透明且可解释的Hopfield神经网络，通过关联记忆存储信号并检测相似信号以分类物种。

Result: 模型训练仅需3毫秒，分类10384个蝙蝠录音仅需5.4秒，内存占用144.09MB，准确率达86%。

Conclusion: 该模型快速、轻量、可持续且准确，有望成为生物声学分析的突破性工具。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [161] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: 论文探讨了神经网络如何通过对称性学习实现激进泛化，以基加法为例，分析了不同进位函数对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 设计能高效学习支持激进泛化的系统是神经网络的挑战，对称性发现与实现是关键。

Method: 通过群论分析基加法的进位函数，训练神经网络比较不同进位函数的学习效果。

Result: 简单神经网络在合适的输入格式和进位函数下可实现激进泛化，学习速度与进位函数结构相关。

Conclusion: 研究对认知科学和机器学习有重要意义，揭示了对称性学习的关键因素。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [162] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架，用于解决随机Petri网（SPN）中参数估计的挑战，特别是在转移率依赖外部协变量且显式似然不可用的情况下。


<details>
  <summary>Details</summary>
Motivation: 随机Petri网在建模离散事件动态时面临参数估计的挑战，尤其是在协变量依赖和部分观测数据的情况下。

Method: 使用轻量级1D卷积残差网络，通过端到端训练从Gillespie模拟的SPN实现中学习逆系统动态，并在推理时通过蒙特卡洛dropout提供不确定性估计。

Result: 在20%事件缺失的合成SPN上，该方法恢复了速率函数系数（RMSE=0.108），且比传统贝叶斯方法更快。

Conclusion: 数据驱动的无似然替代方法能够在复杂、部分观测的离散事件系统中实现准确、鲁棒和实时的参数恢复。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [163] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种结合分布鲁棒优化（DRO）和对抗数据污染的方法，通过Wasserstein-1 DRO目标优化，解决了训练数据中异常值和分布偏移的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 传统DRO方法在面对训练数据中的异常值时效果受限，本文旨在同时解决数据污染和分布偏移问题。

Method: 提出了一种新的建模框架，结合了对抗数据污染的鲁棒性和分布鲁棒性，并设计了一种受鲁棒统计启发的算法。

Result: 在协方差有界的假设下，方法仅使用污染数据即可实现$O(\sqrt{\epsilon})$的估计误差。

Conclusion: 本文首次为数据污染和分布偏移双重挑战下的学习问题提供了严格的理论保证和高效计算方法。

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [164] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: 论文提出了一种名为Ground-Compose-Reinforce的神经符号框架，用于从数据中学习语言基础，并通过语言直接指导RL代理行为，避免了手动设计领域特定元素，实现了高效的数据泛化。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂感知（如像素）和动作中语言基础的问题，避免手动设计或大规模数据集的依赖。

Method: 提出神经符号框架Ground-Compose-Reinforce，结合数据驱动学习和组合形式语言语义，直接通过语言指导RL代理。

Result: 在图像网格世界和MuJoCo机器人领域的实验中，该方法在有限数据下成功映射语言指令到行为，而端到端数据驱动方法失败。

Conclusion: 该框架通过数据驱动和组合语言语义，实现了高效的语言基础和行为生成，具有泛化能力。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [165] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 本文介绍了一个基于NVIDIA PhysicsNeMo-CFD的开源基准测试框架，用于系统评估AI模型在汽车空气动力学预测中的准确性、性能、可扩展性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为CAE社区提供一个标准化的方法，以增强AI模型性能评估的透明度和一致性，加速该领域的研究与创新。

Method: 框架包含多样化的评估指标，支持表面和体积流场预测的评估，并提供了集成新模型和数据集的指南。

Result: 通过DrivAerML数据集对三种AI模型（DoMINO、X-MeshGraphNet、FIGConvNet）进行了评估，展示了框架的实用性。

Conclusion: 该框架旨在帮助研究者和行业专业人士选择和优化AI驱动的空气动力学建模方法，推动更高效、准确和可解释的解决方案的发展。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [166] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个软件框架，用于通过生成去噪模型对连续变量进行空间推理。


<details>
  <summary>Details</summary>
Motivation: 去噪生成模型已成为图像生成的标准方法，但在多连续变量推理中的应用仍需基础设施支持。

Method: 提供易用接口，支持变量映射、生成模型范式和推理策略的灵活控制。

Result: 框架开源，旨在促进该领域研究。

Conclusion: Spatial Reasoners为生成推理研究提供了高效工具。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [167] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: 提出Phy-SSM方法，结合物理知识与状态空间模型，提升复杂环境中长期动态预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中噪声和不规则采样数据的长期动态预测问题，结合物理知识提升泛化能力。

Method: 将部分已知物理知识分解为已知和未知状态矩阵，融入Phy-SSM单元，并引入物理状态正则化项。

Result: 在车辆运动、无人机状态和COVID-19流行病预测中，Phy-SSM表现优于基线方法。

Conclusion: Phy-SSM通过结合物理知识与状态空间模型，显著提升了长期预测性能。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [168] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出了多臂采样框架，作为多臂老虎机优化问题的采样对应。研究了采样中的探索-利用权衡，定义了遗憾概念并提出了达到最优遗憾的算法。结果表明采样无需探索，并通过温度参数统一了多臂采样与多臂老虎机问题。


<details>
  <summary>Details</summary>
Motivation: 研究采样中的探索-利用权衡，填补多臂老虎机优化与采样之间的理论空白。

Method: 定义了多臂采样的遗憾概念，提出简单算法达到最优遗憾，并通过温度参数统一多臂采样与多臂老虎机问题。

Result: 理论证明采样无需探索，算法达到最优遗憾，并揭示了熵正则化强化学习等领域的启示。

Conclusion: 多臂采样框架为采样研究奠定基础，对神经采样器、预训练模型微调及RLHF等具有指导意义。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [169] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: 提出了一种新的因果框架，用于捕捉外域干预下时间过程中事件间的因果关系变化，并设计了无偏ATE估计器和基于Transformer的模型。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要关注域内事件类型，忽略了外域干预的影响，而现实中这些干预可能显著改变因果动态。

Method: 设计了无偏ATE估计器，并开发了基于Transformer的神经网络模型，以处理长程时间依赖和局部模式，同时整合外域干预信息。

Result: 在模拟和真实数据集上的实验表明，该方法在外域增强点过程中的ATE估计和拟合优度上优于基线。

Conclusion: 该方法有效解决了外域干预下的因果推断问题，为实际应用提供了更准确的因果分析工具。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [170] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: 论文提出语义上下文（SC）是工具编排的基础，通过理论、实验和实际应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究语义上下文在工具编排中的关键作用，以提高样本效率、适应性和可扩展性。

Method: 1. 提出SC-LinUCB理论框架；2. 用大语言模型进行实证验证；3. 设计FiReAct流程并测试。

Result: SC-LinUCB降低遗憾值，SC提升大语言模型学习效率，FiReAct在10,000+工具上表现优异。

Conclusion: 语义上下文是构建高效、自适应和可扩展编排代理的关键。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [171] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: 利用图卷积网络（GCN）解决受限品种优化问题，在小规模实例上训练后，可高效处理大规模问题，性能优于现有启发式方法。


<details>
  <summary>Details</summary>
Motivation: 品种优化是收入管理中的经典问题，但由于其组合和非线性特性，通常是NP难问题。传统方法效率低，难以处理大规模实例。

Method: 开发品种问题的图表示，训练GCN学习最优品种模式，提出两种基于GCN输出的推断策略。

Result: 在小规模实例（20种产品）上训练的GCN，可在秒级时间内处理大规模实例（2000种产品），性能达90%以上最优性。

Conclusion: GCN方法在性能和效率上优于现有启发式方法，且可扩展至模型未知但数据可用的场景。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [172] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein距离的离线强化学习方法，避免了对抗训练，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，分布偏移是一个主要挑战，现有方法多基于密度比度量，但可能对分布外数据不鲁棒。

Method: 利用Wasserstein距离和输入凸神经网络（ICNNs）建模最优传输映射，避免对抗训练。

Result: 在D4RL基准数据集上表现优于或与现有方法相当。

Conclusion: 提出的方法在离线强化学习中有效解决了分布偏移问题，且性能优越。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [173] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: 论文提出了一种基于视觉的认知重评增强方法，利用文本到图像扩散模型生成情感一致的视觉反馈，显著降低了负面情绪。


<details>
  <summary>Details</summary>
Motivation: 传统的认知重评依赖高阶认知和语言能力，对创伤或抑郁患者效果有限，因此需要一种更直观、视觉化的干预方法。

Method: 通过稳定扩散模型和微调的IP适配器，将用户的口头重评转化为支持性的视觉图像，保持与原刺激的结构相似性。

Result: 实验表明，AI辅助的重评显著减少了负面情绪，且情感一致性高的生成图像与情绪缓解相关。

Conclusion: 生成视觉输入能够有效支持认知重评，为生成AI、情感计算和治疗技术的结合提供了新方向。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [174] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于图自动编码器的潜在动力学替代模型（GALDS），用于高效模拟神经元树状网络中的物质运输，显著提升了计算速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经元树状网络的复杂几何结构对物质运输模拟提出了计算挑战，传统方法耗时且资源密集。

Method: GALDS结合图自动编码器和神经ODE概念，通过潜在空间表示和动态模型预测系统行为。

Result: 在8种未见几何结构和4种异常运输案例中，GALDS的平均相对误差为3%，最大误差<8%，速度提升10倍。

Conclusion: GALDS为神经元网络中的物质运输模拟提供了一种高效且准确的替代方法。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [175] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 本文提出了一种基于编码器-解码器架构的小型语言模型（SLM），用于改进产品和服务的税收代码预测，解决了多国税务合规中的复杂问题。


<details>
  <summary>Details</summary>
Motivation: 跨国企业每天处理大量交易，需遵守不同司法管辖区的税务规定，准确预测税收代码（如HSN或SAC）对避免税务罚款至关重要。

Method: 采用编码器-解码器架构的SLM，利用非结构化数据预测层次化税收代码序列，捕捉代码间的依赖关系。

Result: 实验表明，该模型在预测结构化税收代码方面优于扁平分类器，并在HSN等任务中表现优于仅解码器或仅编码器架构。

Conclusion: 该方法可扩展至其他政府规定的税收代码，如UNSPSC或NCM，为税务合规提供了高效解决方案。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [176] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种基于模拟的生成模型SiGMoID，用于从噪声、稀疏或部分可观测数据中精确推断非线性动态系统的ODE参数和未观测组件。


<details>
  <summary>Details</summary>
Motivation: 非线性动态模型的系统推断在噪声、稀疏或部分可观测数据下具有挑战性，需要一种鲁棒且精确的方法。

Method: 结合物理信息神经网络与超网络构建ODE求解器，以及Wasserstein生成对抗网络估计参数并捕捉噪声数据分布。

Result: SiGMoID能量化数据噪声、估计系统参数并推断未观测组件，实验验证了其广泛适用性。

Conclusion: SiGMoID为科学研究和工程系统提供了一种有效的动态系统推断方法。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [177] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 论文研究了对抗性遗忘问题，提出了一种保护模型性能的新方法。


<details>
  <summary>Details</summary>
Motivation: AI模型需要遗忘以满足法律要求（如AI法案或GDPR），同时需移除有毒内容、减少偏见或应对数据分布变化。但遗忘可能导致模型性能下降，尤其是恶意方故意发送遗忘请求时。

Method: 研究了对抗性遗忘现象，分析了影响因素（如主干模型和遗忘数据选择策略），并提出了一种保护模型性能的方法。

Result: 展示了对抗性遗忘现象及其对模型性能的影响，新方法能有效保护模型性能。

Conclusion: 提出的方法能有效应对自发或恶意遗忘行为对模型性能的负面影响。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [178] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: 论文研究了如何预测从库存仓库中满足客户需求的单位数量（“消耗”）及相关出库运输成本，并将其建模为概率预测问题，以支持强化学习（RL）控制策略的开发。


<details>
  <summary>Details</summary>
Motivation: 准确建模库存消耗和运输成本对区域库存规划至关重要，尤其是在使用RL开发控制策略时。现有方法（如模拟内部软件系统）不可微分且效率低，无法满足RL训练需求。

Method: 将问题框架为概率预测问题，建模所有仓库在每个时间段的出库消耗和运输成本的联合分布，条件为库存位置和外部客户需求。提出了一种验证方案，利用生产系统评估RL策略下的反事实库存状态。

Result: 初步结果表明，模型在分布内设置下具有较高的准确性。

Conclusion: 提出的概率预测模型和验证方案为RL环境中的库存规划提供了高效且可微分的解决方案，尤其在处理分布外场景时表现出鲁棒性。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [179] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: 论文提出了一种基于类难度可分性的核心集选择方法，通过类比例变体改进现有方法，在高剪枝率下保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法假设数据难度在类间同质化，忽略了类间难度差异，导致性能下降。

Method: 引入类难度可分性系数（CDSC），并提出类比例变体的采样策略（如CCS-CP）。

Result: 在五个数据集上，类比例方法在99%剪枝率下性能下降更小，优于类无关方法。

Conclusion: 显式建模类难度可分性可提升数据剪枝的有效性和鲁棒性，尤其适用于高风险场景。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [180] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: 本文研究了在肽段从头测序中使用扩散解码器的方法，相比传统的自回归解码器，扩散解码器能够从任意肽段开始生成序列，提高了预测准确性。实验表明，结合DINOISER损失函数的设计在氨基酸召回率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码器在肽段从头测序中存在级联错误和未能有效利用高置信度区域的问题，扩散解码器提供了一种新思路。

Method: 研究了三种扩散解码器设计，结合背包束搜索和多种损失函数进行实验。

Result: 最佳扩散解码器设计结合DINOISER损失函数，氨基酸召回率显著提升0.373，但肽段精确率和召回率仍为0。

Conclusion: 扩散解码器在提高模型敏感性和推动肽段从头测序技术进步方面具有潜力。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [181] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 本文综述了机器学习在半导体薄膜沉积中的应用，重点介绍了物理信息神经网络（PINNs）的潜力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 半导体薄膜沉积过程复杂，需要精确控制以实现均匀性和功能性。机器学习，特别是PINNs，为解决工艺控制和质量保证问题提供了新思路。

Method: 通过主题分析，识别了当前ML应用的趋势、局限性和研究空白，并探讨了PINNs在嵌入物理知识方面的策略。

Result: 研究总结了ML在薄膜沉积中的优势与限制，提出了PINNs的集成方法以提升工艺的准确性、可解释性和鲁棒性。

Conclusion: 本文为未来研究提供了明确方向，旨在通过物理信息ML框架改进半导体制造的精度和效率。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [182] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: StellarF模型通过LoRA和Adapter技术，结合多尺度模式识别，实现了恒星耀斑预测的先进性能。


<details>
  <summary>Details</summary>
Motivation: 恒星耀斑预测领域因数据稀疏和缺乏大规模预测模型而受限，需开发高效方法。

Method: 提出StellarF模型，整合耀斑统计模块和历史记录模块，利用LoRA和Adapter技术进行参数高效学习。

Result: 在Kepler和TESS数据集上，StellarF表现优于现有方法。

Conclusion: StellarF为天体物理研究和跨学科应用提供了新方法框架。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [183] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv是一个轻量级的分布式环境执行接口，通过DETACH模式解耦模拟与训练，提出AAPS机制减少同步开销，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有框架将模拟、学习逻辑和编排耦合为单一系统，限制了模块化和可重用性。

Method: ClusterEnv采用DETACH模式，将reset()和step()操作卸载到远程工作节点，同时保持学习集中化；提出AAPS机制以减少同步开销。

Result: 实验表明，AAPS在离散控制任务中实现了高样本效率，且显著减少了权重更新次数。

Conclusion: ClusterEnv无缝集成到现有RL流程中，支持多种方法，代码改动小，性能优越。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [184] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了奖励函数中终端目标和工具目标的混淆问题，指出这种混淆会导致强化学习性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示奖励函数中终端目标和工具目标的混淆如何影响强化学习的性能，尤其是在真实环境中。

Method: 通过构建一个简单的示例，展示了奖励函数中目标混淆如何导致严重的性能偏差。

Result: 结果显示，即使轻微的混淆也会导致优化后的奖励函数在真实奖励函数下表现不佳。

Conclusion: 结论强调了在奖励学习中区分终端和工具目标的重要性，以避免性能下降。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [185] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM是一种新的后训练量化方法，通过显式结合一阶梯度项改进量化误差补偿，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有补偿式权重校准方法假设一阶项可忽略，但实际中渐进补偿过程会引入累积一阶偏差，导致假设不成立。

Method: FOEM通过直接计算潜在权重与全精度权重的差异近似梯度，避免高成本的反向传播，并利用预计算的Cholesky因子实时恢复Hessian子矩阵的逆。

Result: 在3位权重量化中，FOEM将Llama3-8B的困惑度降低89.6%，并将Llama3-70B的5-shot MMLU准确率从51.7%提升至74.9%，接近全精度性能。

Conclusion: FOEM不仅显著提升量化性能，还能与其他先进技术无缝集成，进一步缩小与全精度基线的差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [186] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: 提出了一种基于混合输入变分自编码器（VAE）的潜在空间扰动框架，用于生成难以察觉的对抗样本，解决了表格数据中异质特征的挑战。


<details>
  <summary>Details</summary>
Motivation: 表格数据的异质性（混合分类和数值特征）导致传统对抗攻击方法难以定义难以察觉的修改，且传统梯度方法生成的对抗样本易偏离原始数据分布。

Method: 使用混合输入VAE将分类嵌入和数值特征统一到潜在流形中，生成统计一致的对抗样本，并引入In-Distribution Success Rate（IDSR）作为衡量指标。

Result: 在六个公开数据集和三种模型架构上验证，该方法显著降低了异常率，性能更一致，优于传统输入空间攻击和其他基于VAE的方法。

Conclusion: 研究表明，基于VAE的潜在空间扰动对表格数据的对抗攻击更实用，强调了流形上扰动的重要性，并提供了实用的部署方法。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [187] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM提出了一种通信感知的LoRA自适应框架，通过分层扩散策略优化秩配置，显著降低传输成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上运行大型语言模型面临通信带宽和计算资源限制，现有LoRA方法的固定秩配置和参数传输效率低。

Method: AirLLM将秩配置建模为结构化动作向量，结合PPO和DDIM生成任务和信道自适应的秩向量。

Result: 实验表明，AirLLM在不同信噪比下均能提升微调性能并显著减少传输成本。

Conclusion: AirLLM通过强化学习和扩散模型优化秩配置，实现了高效、可扩展的远程微调。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [188] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon是一个基于Muon优化器的自适应学习率框架，通过两个模块提升训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 提升Muon优化器在大规模模型训练中的效率和适应性。

Method: 引入两个模块：参数级二阶矩调制和RMS对齐的重新缩放。

Result: 在多模型规模和学习率下表现优于Muon，收敛更快且稳定。

Conclusion: AdaMuon无需额外调参，可直接集成到现有Muon训练流程中。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [189] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: 利用机器学习模型从温度数据预测湍流动能（TKE），揭示了火灾环境中温度与气流的新关系。


<details>
  <summary>Details</summary>
Motivation: 探索从易获取的温度数据预测TKE的可能性，以改进火灾环境研究和模型预测。

Method: 使用多种机器学习模型（如深度神经网络、随机森林回归等）分析温度扰动与TKE的关系。

Result: 尽管预测变量与目标变量相关性较弱，机器学习模型仍能较准确地预测TKE。

Conclusion: 机器学习在火灾环境数据分析中具有潜力，有助于改进火灾研究和操作策略。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [190] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 论文提出了一种基于价值梯度的策略优化方法（REPPO），结合路径策略梯度的样本效率和标准策略学习的简单性，显著提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决得分函数策略梯度的高方差问题以及路径策略梯度对准确动作条件价值函数的依赖问题。

Method: 提出REPPO算法，通过平衡随机策略探索和约束策略更新，结合纯在线数据训练Q值模型。

Result: 实验表明REPPO在样本需求、训练时间、内存占用和超参数鲁棒性方面表现优异。

Conclusion: REPPO成功结合了路径策略梯度的优势与在线学习的简单性，为策略优化提供了高效解决方案。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [191] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: 论文提出了一种名为GATE的新框架，通过自适应图表示和注意力机制，解决了Wi-Fi指纹定位中的噪声和设备异构性问题，显著提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi指纹定位在室内定位中广泛应用，但传统深度学习方法假设数据在欧几里得空间中，忽略了空间关系和非均匀噪声分布，导致泛化能力差。GNNs虽能改进，但仍面临非欧几里得噪声和盲点问题。

Method: GATE框架通过自适应图表示指纹向量，保留室内状态空间拓扑结构，并引入注意力超空间向量（AHV）、多维超空间向量（MDHV）和实时边构建（RTEC）方法。

Result: 在多个真实环境中测试，GATE的平均定位误差降低了1.6x至4.72x，最坏情况误差降低了1.85x至4.57x。

Conclusion: GATE通过创新的图表示和动态适应方法，显著提升了室内定位的精度和鲁棒性。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [192] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: 本文提出了一种基于数学公式的MILP实例距离度量方法，通过离散化和权重变量分布比较，显著提升了实例分类的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: MILP实例缺乏有效的相似性度量方法，现有方法依赖标签数据或精度不足，限制了其应用和泛化能力。

Method: 通过离散化右端项、权重和变量，并借鉴Earth mover's distance，量化权重变量分布的不匹配，提出精确和贪心两种变体。

Result: 在StrIPLIB数据集上验证，贪心版本在保持高精度的同时速度提升200倍，优于现有非学习方法，与监督分类器性能相当。

Conclusion: 提出的无监督方法为MILP实例提供了有效的相似性度量，显著提升了分类和子类分组任务的性能。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [193] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于参数高效微调（如LoRA和适配器）的方法，用于在大规模日志数据中检测上下文异常，相比传统方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或深度学习的日志异常检测方法在处理大规模复杂日志序列时效果不佳，因此需要更高效的检测方法。

Method: 采用参数高效微调技术（LoRA和适配器），并在Thunderbird数据集上比较不同小型大语言模型（LLMs）。

Result: LoRA微调方法比LogBert全微调方法性能提升18-19%，准确率达到97.76%-98.83%，而后者仅为79.37%。

Conclusion: 参数高效微调（尤其是LoRA）在日志异常检测中表现出色，为系统维护和开发提供了更高效的解决方案。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [194] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯在线变化点检测（BOCPD）的方法，通过监测强化学习（RL）评论网络中的时间变化，以检测无人机导航中的微妙行为偏差，从而有效应对漂移规避欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖全球导航卫星系统（GNSS）进行定位和导航，但容易受到漂移规避欺骗攻击，传统检测方法因延迟问题难以快速响应。因此，需要一种更高效的时间尺度检测方法。

Method: 采用贝叶斯在线变化点检测（BOCPD）方法，结合强化学习评论网络的时间变化监测，识别攻击行为。

Result: 实验表明，该方法在检测漂移规避欺骗攻击时，优于传统GNSS欺骗检测器、半监督学习框架和Page-Hinkley测试，具有更高的检测准确率和更低的误报率。

Conclusion: 该研究为无人机导航提供了一种高效的时间尺度检测方法，显著提升了对抗隐蔽欺骗攻击的鲁棒性。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [195] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于梯度正则化的神经格兰杰因果模型（GRNGC），通过单一时间序列预测模型和梯度L1正则化解决现有方法的计算成本高和复杂交互捕捉能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经格兰杰因果模型需要为每个时间序列构建单独模型，计算成本高，且稀疏性惩罚削弱了复杂交互的捕捉能力。

Method: GRNGC仅需一个时间序列预测模型，并在输入与输出梯度上应用L1正则化推断因果关系，支持多种架构（如KAN、MLP、LSTM）。

Result: 在模拟和真实数据集（如DREAM、Lorenz-96、DNA等）上，GRNGC性能优于基线方法并显著降低计算开销。

Conclusion: GRNGC是一种高效、灵活的方法，适用于基因调控网络重建等实际应用。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [196] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 本文综述了混合专家（MoE）架构在大语言模型中的应用，强调其在提升模型性能的同时保持低计算开销的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨MoE架构的理论基础、核心设计及其在大语言模型中的应用，以推动该领域的创新。

Method: 系统分析了专家门控与路由机制、分层与稀疏MoE配置、元学习方法、多模态与多任务学习场景、实际部署案例及深度学习的最新进展与挑战。

Result: MoE架构展现出优于贝叶斯方法的模型容量、提升的任务特定性能及高效扩展能力，同时需关注专家多样性、准确校准和可靠推理聚合。

Conclusion: 本文总结了MoE架构的研究现状、挑战及未来方向，为相关创新提供了基础。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [197] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: 提出了一种通信高效的联邦学习方案，通过低秩近似和量化减少网络负载，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中频繁的模型更新交换导致通信开销大，影响效率。

Method: 采用神经网络梯度的低秩近似和量化技术，减少通信数据量。

Result: 显著降低了网络负载，同时对模型准确性影响最小。

Conclusion: 该方案有效解决了联邦学习中的通信效率问题。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [198] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: 本研究提出了一种结合分类和回归模型的机器学习框架，用于心脏病检测和风险预测，使用SMOTE处理数据不平衡，随机森林和线性回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统心脏病诊断方法准确性不足，机器学习可提升诊断效率和准确性，尤其在资源有限地区。

Method: 使用心脏病数据集（1,035例），应用SMOTE生成10万合成数据点，评估分类和回归模型的性能指标。

Result: 随机森林分类准确率97.2%（真实数据）和97.6%（合成数据）；线性回归R2值最高（0.992和0.984）。

Conclusion: 机器学习可显著改善心脏病诊断和风险预测，支持早期干预和临床决策。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [199] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种隐私保护的分布式学习框架，用于在线协作医疗预测平台，以解决隐私问题和预测质量低的挑战。


<details>
  <summary>Details</summary>
Motivation: 在线协作医疗预测平台面临隐私泄露和预测质量低的双重问题，阻碍患者和医生的参与。

Method: 提出了一种隐私保护机制，并将其集成到一次性分布式学习框架中，以满足隐私和性能需求。

Result: 理论证明该框架在特定隐私要求下可实现最优预测性能，并通过模拟和真实数据实验验证。

Conclusion: 该隐私保护协作医疗预测平台能有效平衡隐私和性能需求。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [200] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: 论文探讨了梯度下降在逻辑回归中的行为，发现数据等幅时在一维空间可全局收敛，但在高维仍可能循环。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降在非可分数据集上的行为，特别是步长低于稳定性阈值时的收敛性。

Method: 分析数据等幅条件下梯度下降的收敛性，比较一维和高维空间的行为。

Result: 一维空间下全局收敛，高维空间仍可能出现循环行为。

Conclusion: 需进一步研究循环行为的普遍性及保证全局收敛的充分条件。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [201] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 提出了一种结合生成模型和判别模型的CTR预测方法，通过两阶段训练提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GPT）在表达能力上超越判别模型，可能提升CTR预测的准确性。

Method: 1) 生成预训练：基于用户行为序列预测下一项；2) 微调：将生成模型融入判别框架。

Result: 实验和在线A/B测试验证了方法的有效性，并已部署于大型电商平台。

Conclusion: 生成模型能显著提升CTR预测性能，未来将公开代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [202] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm是一种新型优化器，结合Adam的自适应矩估计和李雅普诺夫稳定性机制，动态调整学习率以提高收敛鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中的噪声梯度和不稳定收敛问题。

Method: 提出LyAm优化器，结合Adam和李雅普诺夫稳定性理论，动态调整学习率。

Result: 在CIFAR-10和CIFAR-100上表现优于现有优化器，提升准确性、收敛速度和稳定性。

Conclusion: LyAm是深度学习优化中的强有力候选方法。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [203] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: 论文提出了一种利用Neyman-Rubin潜在结果框架改进深度强化学习（DRL）的方法，显著提高了样本效率并减少了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决DRL训练中需要大量训练步骤和经验回放缓冲区的问题，降低计算和资源消耗。

Method: 引入Neyman-Rubin潜在结果框架，建立对事实损失（类似于DRL中的on-policy损失）的因果界限，利用过去的值网络输出数据。

Result: 在Atari 2600和MuJoCo领域的实验中，奖励比率提高了2427%，经验回放缓冲区大小减少了96%。

Conclusion: 该方法显著提升了DRL的样本效率，且成本可忽略不计。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [204] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: 研究了SGD在平滑凸目标函数插值区域中的收敛性，分析了最后迭代的期望超额风险，并优化了步长选择。


<details>
  <summary>Details</summary>
Motivation: 研究SGD在过参数化模型训练、持续学习中的遗忘问题以及随机Kaczmarz方法收敛性中的应用。

Method: 使用SGD在β-平滑凸损失函数上，步长η≤1/β，分析最后迭代的期望超额风险。

Result: 得到最后迭代的期望超额风险为O~(1/(ηT^(1−βη/2)) + ηT^(βη/2)σ_⋆^2)，优化步长后可达O~(1/T + σ_⋆/√T)。

Conclusion: 在σ_⋆=0时，步长η=1/β可实现O(1/√T)的收敛速率，优于现有结果。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [205] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: 提出了一种训练通用公平奖励模型（FRM）的框架，用于减少大型语言模型在高风险决策中的偏见，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高风险决策中的使用可能放大不公平偏见，需要一种可信赖的方法来确保决策的公平性。

Method: 通过训练一个通用的公平奖励模型（FRM），为LLM的推理分配公平性分数，从而减少偏见轨迹的权重。

Result: FRM在跨任务、领域和模型家族中表现出良好的泛化能力，并在实际任务中提高了公平性，同时保持或超过基线准确性。

Conclusion: 提出的FRM框架为高风险决策中LLM的公平使用提供了一种有效方法。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [206] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: 论文探讨了神经符号（NeSy）预测器中符号概念独立假设的局限性，指出其可能导致模型无法正确表示某些概念组合的不确定性，从而引发推理捷径问题。


<details>
  <summary>Details</summary>
Motivation: 神经符号预测器通常假设符号概念独立以加速概率推理，但近期研究质疑这一假设是否限制了模型的学习能力和不确定性建模。本文旨在澄清这一问题。

Method: 通过形式化分析，证明了符号概念独立假设会导致模型无法表示某些概念组合的不确定性。

Result: 研究发现，独立假设使模型无法意识到推理捷径问题，即模型可能因错误原因做出正确预测。

Conclusion: 符号概念独立假设在神经符号预测器中存在根本性缺陷，限制了模型的不确定性建模能力，需重新审视其适用性。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [207] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: 提出一种无需反向传播的神经网络训练方法，通过局部信号和层间损失在RL中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播中梯度消失/爆炸问题及存储中间激活值的需求。

Method: 利用多维尺度匹配原理设计局部层间损失，结合奖励驱动信号，实现前向传播中的局部训练。

Result: 在RL基准测试中表现与BP方法相当，且稳定性、一致性及挑战性环境性能更优。

Conclusion: 该方法为RL提供了一种高效、稳定的替代方案，尤其适用于复杂环境。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [208] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK是一种强化学习框架，通过双目标奖励系统优化答案质量和工具多样性，训练模型探索多样工具使用模式。


<details>
  <summary>Details</summary>
Motivation: 传统的高温采样方法限制了工具使用的多样性，SPaRK旨在通过强化学习探索更广泛的工具使用模式。

Method: 采用离线PPO训练Llama-3.1 8B模型，结合双目标奖励系统和稀有工具优先策略，由GPT-4o评分候选动作。

Result: 在14个MMLU-Pro类别中表现优异，工具选择熵显著高于基线方法，显示多样性探索能提升推理能力。

Conclusion: SPaRK证明通过显式工具多样性探索可以增强推理能力，同时保持准确性。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [209] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: 论文提出了一种结合变分自编码器（VAE）和现代Hopfield网络（MHN）的持续学习模型，以解决神经网络中的灾难性遗忘问题，并在Split-MNIST任务上取得了接近90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 人类能够在不遗忘旧知识的情况下学习新信息，而神经网络模型却存在灾难性遗忘问题。论文旨在通过模拟人脑的互补学习系统（CLS）理论，设计一种神经合理的持续学习模型。

Method: 结合变分自编码器（VAE）的模式完成能力和现代Hopfield网络（MHN）的模式分离能力，构建了一个持续学习模型，并在Split-MNIST任务上进行评估。

Result: 模型在Split-MNIST任务上取得了接近90%的准确率，显著减少了遗忘现象。分析表明VAE负责模式完成，MHN负责模式分离。

Conclusion: 通过结合VAE和MHN的功能，论文为生物和人工系统中的记忆巩固、泛化和持续学习提供了一个功能模板。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [210] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出了一种名为R-MTGB的新型多任务梯度提升框架，通过分块学习共享模式、识别异常任务和微调任务特定预测器，有效处理任务异质性并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的多任务学习常包含异常任务，这些任务可能损害整体模型性能，因此需要一种能自动识别并处理这些异常任务的方法。

Method: R-MTGB采用三阶段学习：学习共享模式、通过正则化参数划分异常任务、微调任务特定预测器。

Result: 实验表明，R-MTGB能有效隔离异常任务、促进知识迁移，并在合成和真实数据集上显著降低预测误差。

Conclusion: R-MTGB在多任务学习中表现出鲁棒性、适应性和可靠收敛性，适用于复杂任务环境。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [211] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: 该研究探讨了激活函数在fNIRS深度学习中的影响，发现对称激活函数（如Tanh和Abs(x）在某些架构中优于ReLU，并强调了选择适合fNIRS信号特性的激活函数的重要性。


<details>
  <summary>Details</summary>
Motivation: fNIRS领域的非线性、低信噪比和信号变异性对模型准确性提出了挑战，但激活函数的影响尚未被系统研究。

Method: 使用多种深度学习架构（如fNIRSNet、AbsoluteNet等）评估传统和领域特定的激活函数，并在单一数据集上进行标准化测试。

Result: 对称激活函数（如Tanh和Abs(x））在某些架构中表现优于ReLU，且对称性分析进一步支持其性能优势。

Conclusion: 选择与fNIRS信号特性匹配的激活函数对提升模型性能至关重要。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [212] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: iTransformer在多元时间序列预测中表现优异，但其倒置框架存在局限性。作者提出了一种新的数据增强方法DAIF，通过频率过滤和跨变量修补策略解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: iTransformer的倒置框架虽然能有效捕捉多元相关性，但会削弱时间依赖性信息并引入噪声。为了解决这些问题，作者提出了DAIF方法。

Method: 定义了倒置序列到序列框架的结构，并提出了两种DAIF策略：频率过滤和跨变量修补。

Result: 在多个数据集和倒置模型上的实验证明了DAIF的有效性。

Conclusion: DAIF是首个针对倒置框架的实时数据增强方法，成功解决了其局限性。

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [213] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR框架通过两阶段LLM方法，结合多模态分析和关系排名，提高了直肠癌淋巴结转移的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 传统MRI评估和现有AI模型在淋巴结转移诊断中表现有限，且缺乏可解释性和患者级上下文。

Method: LRMR框架分两阶段：多模态LLM生成结构化报告，文本LLM进行患者间特征比较和风险排名。

Result: 在117例患者中，LRMR的AUC为0.7917，F1-score为0.7200，优于ResNet50等基线模型。

Conclusion: 两阶段LLM框架为淋巴结转移评估提供了高效、可解释的新范式。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [214] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 论文研究了联邦学习在处理非线性、非平稳时间序列数据时的性能，发现其表现不如集中式方法，但通过适当的去趋势技术可以提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及，其收集的时间序列数据具有非线性和非平稳性，传统集中式分析方法存在延迟和通信成本问题，联邦学习成为替代方案。

Method: 生成合成时间序列数据集，使用LSTM模型在集中式和联邦学习框架下训练，并评估不同去趋势技术的影响。

Result: 联邦学习在非线性数据分布下表现较差，但合适的去趋势技术能显著提升其性能。

Conclusion: 联邦学习在处理复杂时间序列数据时需结合去趋势技术以提高预测准确性。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [215] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: 论文探讨了基于强化学习的TractOracle-RL框架的四种扩展，结合RL最新进展，并在五个扩散MRI数据集上评估性能。提出了一种新的RL训练方案IRT，通过迭代优化奖励机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统纤维追踪方法存在假阳性问题，TractOracle-RL通过引入解剖学先验知识改进性能，但仍需进一步优化。

Method: 扩展TractOracle-RL框架，结合RL最新技术，提出IRT训练方案，利用束过滤方法迭代优化奖励机制。

Result: 实验表明，结合Oracle的RL框架在各种方法和数据集上表现稳健，IRT显著提升了追踪的准确性和解剖学有效性。

Conclusion: RL结合Oracle反馈的方法在纤维追踪中优于传统技术，IRT为未来研究提供了新方向。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [216] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: 提出了一种基于Wendland径向基函数的新型参数化激活函数，用于深度神经网络，结合了线性与指数项，提升了梯度传播和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统激活函数（如ReLU、sigmoid和tanh）的局限性，利用Wendland RBF的紧支撑性、平滑性和正定性改进深度学习性能。

Method: 将标准Wendland组件与线性和指数项结合，设计出可调局部性、平滑且适应性强的激活函数。

Result: 在合成任务（如正弦波逼近）和基准数据集（MNIST、Fashion-MNIST）上表现出竞争力，回归任务中尤其突出。

Conclusion: Wendland激活函数能缓解过拟合并提升泛化能力，未来可探索混合架构和领域适配。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [217] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow是一种基于物理先验的变分自编码器，用于建模神经群体的动态结构和外部未观测影响。


<details>
  <summary>Details</summary>
Motivation: 神经群体表现出潜在的动态结构，驱动时间演化的尖峰活动，需要模型捕捉内在网络动态和外部未观测影响。

Method: 采用Langevin方程驱动的变分自编码器，结合惯性、阻尼、学习势函数和随机力等物理先验，潜在空间采用局部耦合振荡器网络。

Result: 在合成数据和NLB基准测试中表现优于现有方法，准确匹配真实发放率并提升预测和解码性能。

Conclusion: LangevinFlow提供了一个灵活、物理启发的高性能框架，用于建模复杂神经群体动态及其未观测影响。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [218] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 提出了一种名为CLS-DM的模型，通过跨模态特征对比学习，从2D X射线图像中提取3D信息，解决了传统LDM在CT重建中的潜在空间对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统CT重建方法的高时间和辐射成本问题，并改进扩散模型在跨模态任务中的表现。

Method: 提出CLS-DM模型，结合跨模态特征对比学习，实现2D X射线到3D CT的潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM指标上优于现有生成模型。

Conclusion: CLS-DM提升了稀疏X射线重建CT的效果和经济性，并可推广至其他跨模态任务。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [219] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 提出了一种名为3D MIR的新方法，结合深度学习和物理约束，从磁场图像中恢复3D电流信息。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，准确恢复3D信息对无损检测和电路缺陷定位至关重要。

Method: 3D MIR分为三阶段：1) CNN预测参数和分类；2) 物理约束提供初始估计；3) 优化器调整参数以最小化误差。

Result: 3D MIR能高精度恢复3D信息，为磁场图像重建设定了新标准。

Conclusion: 结合深度学习和物理驱动优化在实际应用中具有潜力。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [220] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net是一种新型肝脏和肿瘤分割框架，结合多种技术提升分割精度和泛化能力，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决腹部CT图像中肝脏和肿瘤分割的挑战，如复杂解剖结构、肿瘤外观多变和标注数据有限。

Method: 结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性感知和轻量级时序注意力。

Result: 在LiTS数据集上Dice分数93.26%，IoU 88.09%，ASSD 0.72 mm，VOE 11.91%；在3D-IRCADb-01数据集上Dice 87.45%，IoU 80.30%，ASSD 1.525 mm，VOE 19.71%。

Conclusion: HANS-Net在肝脏和肿瘤分割中表现出高效性、鲁棒性和泛化能力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>
