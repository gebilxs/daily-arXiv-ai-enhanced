<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 88]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.LG](#cs.LG) [Total: 86]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: MALIBU是一个评估多智能体系统中LLM隐含社会偏见的基准，通过场景测试揭示偏见并强调公平策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统可能强化LLM的隐含偏见，需开发工具评估和缓解这一问题。

Method: MALIBU通过两阶段评估：第一阶段对标注人口特征的响应评分，第二阶段比较不同特征的响应。

Result: 研究发现偏见缓解可能偏向边缘化群体，需更细致的检测和公平策略。

Conclusion: 需透明评估基准和平衡公平策略以减少多智能体系统中的偏见。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [2] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 该论文提出了一种通过计算生成摘要、参考摘要和原始新闻文章之间重叠事件来评估摘要质量的方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖人工摘要作为黄金标准，但忽略了事件信息的完整性。

Method: 使用挪威语数据集，计算生成摘要、参考摘要和原文之间的事件重叠。

Result: 方法能更深入地分析摘要中的事件信息。

Conclusion: 基于事件的评估方法为摘要质量提供了更全面的视角。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [3] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本文通过数字化《Nordisk familjebok》百科全书的两版内容，利用语义嵌入和分类器分析地理条目变化，发现从第一版到第二版，地理焦点从欧洲转向北美、非洲等地，反映了第一次世界大战和新势力崛起的影响。


<details>
  <summary>Details</summary>
Motivation: 研究《Nordisk familjebok》百科全书的地理条目变化，以揭示19世纪末至20世纪初瑞典社会的知识演变及其背后的历史因素。

Method: 使用语义句子嵌入匹配两版条目，基于Transformer的分类器提取地理条目并链接到Wikidata，分析地理趋势变化。

Result: 发现地理焦点从欧洲显著转向北美、非洲、亚洲、澳大利亚和北欧，证实了第一次世界大战和新势力崛起的影响。

Conclusion: 研究展示了百科全书内容如何反映历史变迁，为知识传播和社会演变提供了新视角。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [4] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 论文提出了一种结合xLSTM与多头指数门控融合（MEGA）的新框架，用于提升基于方面的情感分析（ABSA）的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法在计算效率与高性能之间难以平衡，xLSTM的长距离依赖建模潜力尚未在ABSA中应用。

Method: 提出MEGA框架，结合双向mLSTM架构与部分翻转反向流（PF-mLSTM），并引入多头交叉指数门控融合机制（MECGAF）。

Result: 在三个基准数据集上，MEGA优于现有方法，实现了更高的准确性和效率。

Conclusion: MEGA框架有效解决了ABSA任务中的性能与效率问题，展现了xLSTM的潜力。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [5] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 论文提出了一种去偏算法，通过从编码器表示中移除观察到的混杂因素信息，显著减少了文本嵌入中的偏差，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 文本序列的嵌入相似性度量可能受到无关属性（如文本来源或语言）的干扰，影响多语料库文本池化等应用。

Method: 采用去偏算法，从编码器表示中移除观察到的混杂因素信息。

Result: 去偏后，文档相似性和聚类度量在所有嵌入变体和任务中均显著提升，且不影响分布外基准性能。

Conclusion: 去偏算法有效减少了嵌入偏差，且未对嵌入质量产生负面影响。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [6] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型在处理非英语和非中文国家的法律问题时提供答案和引用的能力，提出了一种基于波兰民法典的认知LLM代理架构gAIus，其检索机制比嵌入方法更可解释且效果更好。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLM在处理非主流语言法律信息时的局限性，并提升其引用和解释能力。

Method: 提出了gAIus架构，基于波兰民法典设计了一种更可解释的检索机制，并通过法律学徒入学考试的单选问题数据集进行评估。

Result: gAIus显著提升了模型性能，使gpt-3.5-turbo-0125提升419%，超越gpt-4o，并将gpt-4o-mini的得分从31%提升至86%。

Conclusion: 论文展示了gAIus的潜力，并提出了未来研究方向和应用前景。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [7] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4在眼科中模拟临床决策的能力被评估，结果显示其在糖尿病视网膜病变（DR）筛查中表现中等，但在青光眼筛查中表现较差。添加临床元数据对结果无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）在眼科中的应用潜力，特别是在糖尿病视网膜病变和青光眼的筛查中。

Method: 使用300张标注的眼底图像，通过结构化提示描述图像，评估GPT-4的分类和推荐能力，并分析元数据的影响。

Result: GPT-4在DR筛查中表现中等（准确率82.3%），但在青光眼筛查中表现不佳（准确率约78%）。元数据未显著影响结果。

Conclusion: GPT-4能够模拟基本的眼科决策，但不适用于临床使用，可能适用于教育或文档工作。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [8] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: CARE-RAG框架通过冲突驱动的证据整合，提升RAG系统的生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中因知识冲突导致的生成不可靠问题。

Method: 提出CARE-RAG框架，包括参数感知证据、上下文感知证据、冲突驱动总结和QA修复步骤。

Result: 在噪声或冲突证据场景下，CARE-RAG表现优于基线RAG系统。

Conclusion: CARE-RAG通过多源证据的可靠整合，显著提升了RAG系统的可信度。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [9] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 论文提出CompactDS，一个高质量的网页规模数据存储，显著提升了检索增强生成（RAG）在推理密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在推理密集型任务中表现有限，缺乏与预训练数据广度对齐的高质量数据存储。

Method: 引入CompactDS，通过过滤低质量网页内容并结合内存近似最近邻（ANN）与磁盘精确搜索，实现高效检索。

Result: CompactDS显著提升了多个基准测试（MMLU、GPQA等）的准确性，相对增益达10%-33%。

Conclusion: CompactDS展示了高质量数据存储对RAG的重要性，其性能优于复杂代理系统，且易于复现。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [10] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA是一种新的激活稀疏化方法，通过层间正交旋转和Top-K选择，无需额外训练即可提升LLM效率，实现稳定的加速和低性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有激活稀疏化方法需要耗时训练或依赖经验性剪枝，导致稀疏性波动和加速不稳定。LaRoSA旨在解决这些问题。

Method: 利用层间正交旋转将输入激活转换为更适合稀疏化的形式，并通过Top-K选择实现一致的稀疏性和加速。

Result: 在LLaMA2-7B上，40%稀疏度下，LaRoSA仅增加0.17困惑度，加速1.30倍，零样本任务准确率差距仅0.54%。

Conclusion: LaRoSA是一种高效、稳定的激活稀疏化方法，适用于多种LLM，性能损失小且加速显著。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [11] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 论文研究了高级指令调优推理模型（如Deepseek-R1）在解决复杂物理问题中的应用，展示了其在SciBench基准测试中的卓越表现和独特推理模式。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在物理推理中的能力，尤其是在需要深刻概念理解和问题解决技巧的任务中。

Method: 使用指令调优推理模型（如Deepseek-R1）处理SciBench中的多样化物理问题，并评估其表现。

Result: 模型在复杂物理问题上达到了最先进的准确性，并展现出独特的符号推导推理模式。少量样本提示还能进一步提升性能。

Conclusion: 高级推理模型在物理推理任务中表现出色，且通过策略性提示可以持续提升性能。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [12] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM是首个纯逆向语言模型，通过逆向时序处理序列，展示了在通用任务中的潜力，并提出了基于逆向奖励的新应用。


<details>
  <summary>Details</summary>
Motivation: 探索逆向语言模型作为基础模型的潜力，并验证其在任务中的独特优势。

Method: 训练了2B和7B参数的逆向语言模型LEDOM，提出逆向奖励方法优化生成质量。

Result: LEDOM在数学推理任务中显著提升性能，展示了逆向推理的独特能力。

Conclusion: LEDOM具有广泛的应用潜力，将公开模型和代码以促进研究。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [13] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 论文提出SynPref-40M大规模偏好数据集和Skywork-Reward-V2奖励模型，通过人机协同提升数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在评估中表现不佳，主要因偏好数据集范围狭窄或质量不足。

Method: 设计人机协同两阶段数据管道，结合人类标注质量与AI扩展性，训练8个不同规模的奖励模型。

Result: Skywork-Reward-V2在多个基准测试中达到最优，验证数据规模与高质量标注的重要性。

Conclusion: 人机协同数据标注可显著提升奖励模型性能，为开放奖励模型提供新方向。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [14] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力机制的深度学习方法，用于电子健康记录文本的信息提取和多标签疾病预测，并在MIMIC-IV数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录文本的非结构化和高维语义复杂性带来的挑战。

Method: 采用基于Transformer的架构进行临床文本表示学习，利用多层自注意力机制捕捉关键医学实体及其上下文关系，并结合Sigmoid多标签分类器进行疾病预测。

Result: 实验表明，该方法在多个性能指标上优于现有方法，并在不同数据规模、干扰水平和模型深度下表现出强泛化能力。

Conclusion: 该框架为处理真实临床文本提供了高效算法基础，对多标签医学文本建模任务具有实际意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [15] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec是一种无需训练、即插即用的方法，通过利用最后一个token的logit预测下一个及下下个token，并检索相关参考，显著提升了检索式SD的性能，实现了最高2.61倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的检索式SD方法依赖匹配范式检索相关参考作为draft tokens，但常因匹配不准确而失败。LogitSpec旨在通过扩展检索范围，找到更准确的参考。

Method: LogitSpec分两步生成draft tokens：(1) 利用最后一个token的logit预测下下个token；(2) 检索与下一个及下下个token相关的参考。

Result: 实验表明，LogitSpec在多个文本生成基准上实现了最高2.61倍的加速，平均每个解码步骤接受3.28个token。

Conclusion: LogitSpec是一种高效且易于部署的检索式SD方法，显著提升了LLM推理速度。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [16] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 论文提出了一种基于直接偏好优化（DPO）的个性化文本简化方法，通过结合目标群体（如智力障碍者）的反馈，优化LLM生成的文本简化结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本简化系统缺乏个性化，未考虑目标群体的偏好反馈，导致简化结果不符合其需求。

Method: 扩展监督微调（SFT）方法，引入DPO技术，利用目标群体对简化文本的偏好反馈进行后训练。

Result: 提出了一种个性化LLM文本简化系统开发流程，并验证了目标群体参与设计的重要性。

Conclusion: 个性化AI无障碍系统需结合目标群体的反馈，本研究为此提供了初步探索。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [17] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 提出了一种结合不确定性建模和微调大语言模型（LLM）的模块化框架，用于高效准确地检测任务导向对话系统中的超出范围（OOS）意图。


<details>
  <summary>Details</summary>
Motivation: 解决任务导向对话系统中对未见或模糊查询的鲁棒性问题。

Method: 首先对现有分类器进行不确定性估计，然后对高不确定性实例触发微调的LLM进行最终决策。

Result: 在关键OOS检测基准上取得了最优性能，包括实际部署系统中的数据。

Conclusion: 该方法有效平衡了计算效率和性能，结合传统方法和LLM，实现了先进的OOS检测效果。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [18] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 研究发现，在立场检测任务中，外部信息（如维基百科和网络搜索）对大型语言模型（LLMs）的性能产生负面影响，导致F1分数下降高达27.9%。


<details>
  <summary>Details</summary>
Motivation: 探讨外部信息是否对LLMs在立场检测任务中有益，尽管先前研究表明其对BERT类模型有帮助。

Method: 系统评估了8种LLMs在3个数据集和12个目标上的表现，使用维基百科和网络搜索作为外部信息。

Result: 外部信息在多数情况下降低了性能，LLMs倾向于与提供的信息的立场和情感对齐，而非文本的真实立场。

Conclusion: 研究揭示了LLMs在立场检测中的信息偏见风险，与BERT类模型的研究结果形成对比。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [19] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLM）的任务导向对话系统LUSTER，通过端到端强化学习优化任务成功率和情感响应能力。


<details>
  <summary>Details</summary>
Motivation: 任务导向对话系统在自然语言交互中需兼顾任务成功、情感理解和信息传递，现有系统在复杂环境中表现不足。

Method: 提出LUSTER系统，结合LLM能力和结构化奖励模型，通过短长期奖励优化系统表现。

Result: 实验表明，LUSTER在任务成功率和情感响应方面表现更优。

Conclusion: 结合LLM与强化学习的结构化奖励模型是提升对话系统性能的有效路径。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [20] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 新数据集用于图表问答（CQA），基于真实的多视图图表和自然语言问题，反映实际推理流程。现有模型（如GPT-4.1）表现不佳，准确率仅69.3%。


<details>
  <summary>Details</summary>
Motivation: 现有CQA数据集缺乏真实性和多视图图表支持，无法反映实际推理流程，因此构建新数据集以填补这一空白。

Method: 从可视化笔记本中构建数据集，包含真实的多视图图表和自然语言问题，模拟实际分析场景。

Result: 测试现有模型（如GPT-4.1）在新数据集上表现不佳，准确率为69.3%，显示真实CQA任务的挑战性。

Conclusion: 新数据集为CQA研究提供了更真实的基准，揭示了现有模型在复杂场景中的局限性。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [21] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 论文比较了全局评分和成对比较在NLP模型评估中的优缺点，发现全局评分更可靠但可能低估某些模型，而成对比较能更好识别低分模型中的强者。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优神经语言模型的发展，NLP评估从传统全局评分转向成对比较排行榜，需研究两种方法的优劣以指导决策。

Method: 通过合成和真实数据集实验，使用全局指标和Bradley-Terry模型进行成对比较。

Result: 全局评分提供更可靠的整体排名，但可能低估某些模型；成对比较能识别低分模型中的强者，但收敛较慢。

Conclusion: 两种方法各有优劣，选择评估策略需根据具体需求。代码和数据已开源。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [22] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 研究了预训练语言模型在低资源印尼本地语言情感分析任务中的迁移能力，发现模型性能与语言在预训练中的暴露程度密切相关。


<details>
  <summary>Details</summary>
Motivation: 探索预训练语言模型在低资源印尼本地语言中的迁移能力，以支持这些语言的自然语言处理任务。

Method: 评估了零样本性能和基于适配器的迁移方法（MAD-X），并比较了单语印尼BERT、多语言模型（mBERT、XLM-R）的表现。

Result: 多语言模型在预训练中见过的语言表现最佳，部分见过的语言次之，未见过的语言表现最差；MAD-X显著提升了性能。

Conclusion: 模型对语言的预训练暴露程度是迁移成功的最一致预测因素，适配器方法（MAD-X）能有效提升性能。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [23] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 提出AdamMeme框架，通过多智能体协作动态评估多模态大语言模型（mLLMs）对有害表情包的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据集的评估方法无法适应动态变化的网络表情包，需更灵活、全面的评估框架。

Method: 采用多智能体协作的AdamMeme框架，动态更新挑战性样本以测试mLLMs的推理能力。

Result: 实验表明，AdamMeme能系统揭示不同mLLMs的性能差异，并深入分析模型弱点。

Conclusion: AdamMeme为评估mLLMs在有害表情包理解上提供了动态、全面的解决方案。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [24] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 论文提出StereoBias数据集，通过联合学习偏见和刻板印象检测任务提升模型性能，实验表明联合训练显著优于单独训练。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象可能造成危害，尤其在敏感领域如内容审核和决策中。

Method: 引入StereoBias数据集，比较编码器模型和微调解码器模型（使用QLoRA），并联合训练偏见和刻板印象检测任务。

Result: 联合训练显著提升偏见检测性能，且改进源于偏见与刻板印象的关联而非多任务学习本身。

Conclusion: 利用刻板印象信息有助于构建更公平有效的AI系统。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [25] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 论文探讨了NLP在法律文本中的应用，通过扩展数据集和使用LLMs评估德国雇佣合同条款的合法性，发现法律上下文对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 法律工作的文本密集性和资源密集性为NLP研究提供了独特挑战和机会，但现有数据驱动方法缺乏可解释性和可信度。

Method: 与法律专家合作扩展数据集，利用LLMs和上下文学习评估合同条款的合法性，比较不同法律上下文（无上下文、全文法律来源、考试指南）的效果。

Result: 全文法律来源对性能有中等提升，考试指南显著提高了召回率和加权F1-Score（达80%），但LLMs表现仍远低于人类律师。

Conclusion: LLMs在合同合法性审查中有潜力，但方法仍有局限性，需进一步改进。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [26] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 论文探讨了分词对语言数据表示和分析结果的影响，强调了处理表情符号和同形异义词的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究分词差异如何影响语言数据的表示和分析结果的可靠性，特别是在处理表情符号和同形异义词时。

Method: 提出了确保数字文本在语料库中准确表示的方法，以支持可靠的语料分析。

Result: 研究发现，对数字文本数据的语言和技术方面的深入理解对提高语料分析的准确性至关重要。

Conclusion: 研究强调了预处理表情符号和同形异义词的必要性，对语料库研究的定量和定性方法具有重要影响。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [27] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating是一个可扩展的框架，通过将英语数据质量信号转移到17种目标语言中，训练多语言评估器，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据质量选择方法主要针对英语，缺乏多语言支持，MuRating旨在填补这一空白。

Method: MuRating通过聚合多个英语评分器的信号，学习统一的文档质量分数，并通过翻译将这些判断投射到目标语言中。

Result: MuRating在英语和多语言基准测试中均表现优异，尤其在知识密集型任务上提升显著。

Conclusion: MuRating为多语言数据质量评估提供了有效解决方案，并指出了未来研究方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [28] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: 研究发现语言模型能区分测试与部署阶段，称为“评估意识”，这对AI安全评估的可靠性构成挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨评估意识对AI治理框架和行业自愿承诺的潜在影响，确保评估的可信度。

Method: 使用线性探针分析Llama-3.3-70B-Instruct模型，区分真实评估与部署提示。

Result: 模型内部能区分评估与部署，且当前安全评估被模型视为不真实。

Conclusion: 需提升评估的可信度，并利用模型内部信息支持黑盒安全审计。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [29] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究探讨了多模态AI模型在输入信息冲突时的行为，发现模型倾向于优先处理某一模态，且内部结构和注意力机制影响其偏好。


<details>
  <summary>Details</summary>
Motivation: 理解多模态模型如何处理冲突输入，以提升其性能和可控性。

Method: 通过提供不一致的视觉-语言输入（如图像与标题不符），测试模型对不同模态的响应。

Result: 模型倾向于优先处理某一模态，内部结构和注意力机制影响其偏好，并发现可操纵的“路由头”以优化性能。

Conclusion: 研究为识别和控制多模态模型在冲突信号下的行为提供了关键步骤。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [30] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文分析了MDACE数据集，评估了可解释医疗编码系统的合理性，发现真实证据与代码描述部分一致，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 自动医疗编码可简化文档和计费流程，但缺乏透明度和评估数据。MDACE数据集为解决这一问题提供了资源。

Method: 对MDACE数据集进行深入分析，评估现有可解释医疗编码系统的合理性，并提出匹配度量。

Result: 真实证据与代码描述部分一致，先进方法与真实证据高度重叠。

Conclusion: 提出了开发与评估可解释医疗编码系统的建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [31] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 比较分析小型语言模型生成的临床笔记结构化输出的可解析性，JSON表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为隐私敏感的临床环境中部署语言模型提供实用指导。

Method: 评估JSON、YAML和XML三种序列化格式的可解析性，分析错误模式。

Result: JSON可解析性最高，结构鲁棒性随提示优化和模型增大而提升，但对长文档和特定笔记类型下降。

Conclusion: 建议选择JSON作为序列化格式，并优化提示设计以提高模型表现。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [32] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 论文提出了一种系统方法，通过分析低困惑度序列来研究LLMs如何利用和复制训练数据，发现部分序列无法映射到训练语料，为理解LLMs行为提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，了解训练数据如何影响其输出对透明度、责任、隐私和公平性至关重要。

Method: 引入了一种基于低困惑度序列的系统分析方法，可靠地提取并追踪其来源。

Result: 发现大量低困惑度序列无法映射到训练数据，匹配的部分则量化了其在源文档中的分布。

Conclusion: 研究揭示了LLMs对训练数据的利用方式，为理解其行为提供了新途径。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [33] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL是一个统一且支持多语言的LLM评估框架，覆盖35个基准测试，包括10个印度语言数据集，支持分布式推理和多GPU使用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的快速发展，需要超越英语中心的评估框架，满足语言多样化地区（如印度）的需求。

Method: EKA-EVAL整合了35个基准测试，包括10个印度语言数据集，支持分布式推理、量化和多GPU使用。

Result: EKA-EVAL是首个为全球和印度LLM设计的端到端可扩展评估套件，显著降低了多语言基准测试的门槛。

Conclusion: EKA-EVAL开源且公开可用，是EKA计划的一部分，旨在扩展到100多个基准测试，建立强大的多语言LLM评估生态系统。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [34] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: DIY-MKG是一个开源系统，支持多语言学习，通过个性化词汇知识图谱和自适应复习模块解决现有语言学习工具的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言学习工具在多语言词汇联系、个性化学习和认知卸载方面存在不足，DIY-MKG旨在解决这些问题。

Method: DIY-MKG利用LLM构建个性化词汇知识图谱，支持选择性扩展、丰富注释和动态生成个性化测验。

Result: 评估表明，DIY-MKG的词汇扩展在多语言中可靠且公平，生成的测验高度准确。

Conclusion: DIY-MKG是一个有效的多语言学习工具，通过用户反馈和LLM优化提升了学习体验。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [35] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: MiCoTA框架通过中间规模模型作为教师助理，提升小语言模型的长链推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型（SLMs）因容量有限而难以学习长链推理的问题。

Method: 使用中间规模模型作为教师助理，利用中等长度推理序列弥合容量和推理长度差距。

Result: SLMs在多个基准测试中表现显著提升，如Qwen2.5-7B和3B模型分别提高3.47和3.93分。

Conclusion: MiCoTA为SLMs的长链推理数据蒸馏提供了有效方法，并为未来研究奠定基础。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [36] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 提出了一种新颖的剪枝算法，专注于剪枝模型高层注意力头，并引入自适应重缩放参数以校准剪枝后的表示规模。实验证明其在生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法忽视注意力头在网络架构中的位置，导致性能下降。

Method: 提出剪枝高层注意力头的策略，并引入自适应重缩放参数。

Result: 在多种LLMs和27个数据集上验证，生成任务表现显著优于现有方法。

Conclusion: 新方法在剪枝效率和性能上优于传统方法，尤其在生成任务中表现突出。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [37] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文综述了AI在科学研究中的应用（AI4Research），提出了系统分类法，指出了研究空白和未来方向，并整理了丰富的资源。


<details>
  <summary>Details</summary>
Motivation: AI在科学研究中的应用潜力巨大，但缺乏全面综述，阻碍了领域发展。本文旨在填补这一空白。

Method: 提出系统分类法，识别研究空白，整理资源。

Result: 提供了AI4Research的系统分类、未来方向和丰富资源。

Conclusion: 本文为AI4Research领域提供了全面视角，有望推动创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [38] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 论文提出GAPO和P-GAPO方法，通过多目标优化解决LLMs与多样化人类偏好对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs与多样化且可能冲突的人类偏好对齐的问题。

Method: 引入GAPO（梯度自适应策略优化）和P-GAPO，采用多梯度下降法平衡冲突目标。

Result: 理论证明GAPO收敛于帕累托最优解，实验显示在Mistral-7B上表现优于现有方法。

Conclusion: GAPO和P-GAPO能有效对齐多样化人类偏好，提升模型性能。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [39] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 本文研究了如何通过从教师模型中蒸馏高质量推理痕迹（NaturalThoughts）来提升学生模型的推理能力，发现选择多样化推理策略的困难样本更有效。


<details>
  <summary>Details</summary>
Motivation: 探讨教师模型提供的哪种推理示范对学生模型推理能力提升最有效。

Method: 从强教师模型中精选高质量推理痕迹（NaturalThoughts），并分析影响推理能力蒸馏的因素。

Result: 选择困难且多样化的推理样本更高效；NaturalThoughts在多个推理基准测试中表现优于现有数据集。

Conclusion: 精选高质量推理示范能显著提升学生模型的推理能力，尤其在多样化推理任务中表现更优。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [40] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 论文提出了一种基于决策效果的NLG评估框架，通过衡量生成文本对人类和LLM决策的影响，发现传统评估方法与实际决策效果相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 当前NLG的评估方法（如n-gram重叠或句子合理性）与实际决策效果相关性较弱，需要更直接的评估方式。

Method: 提出决策导向的评估框架，通过市场摘要文本（包括客观晨报和主观收盘分析）测试人类投资者和LLM代理的决策质量。

Result: 人类和LLM代理仅依赖摘要时表现不优于随机；但更丰富的分析评论能显著提升人机协作团队的表现。

Conclusion: 评估生成文本应关注其促进人机协同决策的能力，传统评估指标存在局限性。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [41] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 该研究比较了Whisper和Wav2Vec-BERT在低资源语言Bangla上的表现，发现Wav2Vec-BERT在多项指标上优于Whisper。


<details>
  <summary>Details</summary>
Motivation: 探索低资源语言（如Bangla）的自动语音识别（ASR）性能，以支持其发展。

Method: 使用Mozilla Common Voice-17和OpenSLR数据集，通过微调和超参数优化评估Whisper和Wav2Vec-BERT。

Result: Wav2Vec-BERT在WER、CER、训练时间和计算效率上均优于Whisper。

Conclusion: Wav2Vec-BERT更适合低资源语言的语音识别，为相关系统开发提供了实用见解。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [42] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs能进行连贯且有说服力的辩论，但缺乏对对话深层结构的理解。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在敏感领域的快速部署，需要评估其对对话的理解能力。

Method: 通过评估LLMs在辩论中的表现，测量其对对话结构和语用背景的理解。

Result: LLMs能进行有说服力的辩论，但无法展示对深层对话结构的理解。

Conclusion: LLMs作为评估者的局限性源于其对上下文理解的不足，辩论能力不依赖于对内容的真正理解。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出了一种4D视频生成模型，通过跨视图点图对齐监督训练，确保多视角3D一致性，无需相机姿态输入即可预测未来视频序列。


<details>
  <summary>Details</summary>
Motivation: 增强机器人在复杂环境中规划和交互的能力，解决现有视频生成模型在多视角几何一致性和时间连贯性上的不足。

Method: 利用RGB-D观测数据，通过跨视图点图对齐监督训练，学习共享3D场景表示，生成多视角一致的4D视频。

Result: 在模拟和真实机器人数据集上，模型生成的视频在视觉稳定性和空间对齐性上优于现有基线。

Conclusion: 该方法支持从新视角预测视频序列，并能用于机器人末端执行器轨迹恢复，提升机器人操作的鲁棒性和泛化能力。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [44] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 该研究提出了一种结合多源卫星影像和深度学习模型的方法，以提高滑坡识别和预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和人类生命构成严重威胁，需要准确检测和预测。

Method: 利用Sentinel-2多光谱数据和ALOS PALSAR衍生的坡度及DEM层，结合多种地理空间分析技术和深度学习模型（如U-Net、DeepLabV3+和Res-Net）进行滑坡检测。

Result: 研究结果为开发可靠的早期预警系统、改进灾害风险管理和可持续土地利用规划提供了有价值的见解。

Conclusion: 深度学习和多源遥感技术在构建稳健、可扩展和可迁移的滑坡预测模型中具有巨大潜力。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [45] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: cp_measure是一个Python库，将CellProfiler的核心测量功能模块化，便于程序化特征提取，支持机器学习和计算生物学应用。


<details>
  <summary>Details</summary>
Motivation: 当前工具如CellProfiler在自动化、可重复分析方面存在障碍，阻碍了机器学习工作流。

Method: 开发cp_measure库，提取CellProfiler的核心测量功能，设计为模块化、API优先的工具。

Result: cp_measure特征与CellProfiler特征高度一致，并能无缝集成到科学Python生态系统中。

Conclusion: cp_measure支持可重复、自动化的图像分析流程，适用于计算生物学中的机器学习应用。

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [46] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: 提出了一种高效的显著目标检测（SOD）网络设计，结合传统方法和现代CNN，通过像素差异卷积（PDC）和差异卷积重参数化（DCR）提升效率，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 解决现有SOD模型在资源受限设备上计算成本高的问题，实现实时性能。

Method: 结合传统SOD方法和现代CNN，使用PDC编码特征对比，引入DCR减少推理计算量，并扩展至视频SOD（STDC）。

Result: 模型SDNet和STDNet在Jetson Orin设备上分别以46 FPS和150 FPS运行，速度和准确性均优于现有轻量级模型。

Conclusion: 提出的方法在效率和准确性之间取得了显著平衡，适用于实时SOD任务。

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [47] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: 提出了一种基于单模态并行处理的鲁棒框架，用于处理多模态MRI中缺失模态的脑肿瘤分割问题。


<details>
  <summary>Details</summary>
Motivation: 多模态MRI在脑肿瘤分割中至关重要，但传统方法在模态缺失时表现不佳，因此需要一种鲁棒的方法。

Method: 利用Holder散度和互信息设计损失函数，动态调整网络参数以保持模态特异性特征。

Result: 在BraTS 2018和2020数据集上表现优于现有方法，尤其在模态缺失情况下。

Conclusion: 该框架通过动态调整和损失函数设计，显著提升了模态缺失情况下的分割准确性。

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [48] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 论文提出AIGVE-MACS模型，用于AI生成视频的评估，提供分数和多方面语言反馈，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标缺乏解释性，难以与人类评价对齐，需要更全面的评估框架。

Method: 结合视觉语言模型、加权损失和动态帧采样策略，构建AIGVE-BENCH 2基准。

Result: AIGVE-MACS在评分相关性和评论质量上表现最优，并提升视频生成质量53.5%。

Conclusion: AIGVE-MACS为AI生成视频评估提供了新范式，支持更全面的、与人类对齐的评价。

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [49] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 本文综述了杂草绘图领域的最新进展，填补了从数据采集到处理技术的全面文献空白，为未来研究提供了基础参考。


<details>
  <summary>Details</summary>
Motivation: 杂草绘图在精准管理中至关重要，但缺乏全面的文献综述，限制了该领域的进展。

Method: 通过PRISMA指南系统分析数据采集（传感器与平台技术）、数据处理（标注与建模）和绘图技术（时空分析与决策支持工具）的最新方法。

Result: 综述提供了杂草绘图领域的全面理解，支持高效、可扩展和可持续的杂草管理系统开发。

Conclusion: 本文为未来杂草绘图研究提供了基础，并推动了该领域的进一步发展。

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [50] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [51] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: ET-Fuser是一种新颖的方法，通过利用基于预训练模型任务先验的注意力机制，学习集成令牌，以改进面部分析任务的特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单任务学习中缺乏统一的特征表示，ET-Fuser旨在通过集成令牌和自注意力机制解决这一问题。

Method: 提出了一种基于预训练编码器的先验统一学习方法，生成集成令牌，并在自注意力机制中共享互信息。

Result: 实验结果显示，该方法在多种面部分析任务中均表现出显著的特征表示改进。

Conclusion: ET-Fuser通过高效且计算成本低的方法，提升了面部分析任务的特征表示能力。

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [52] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的单张低动态范围图像光照估计方法，通过迭代修复和LoRA技术优化生成高动态范围光照探针。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，泛化能力不足，扩散模型在生成HDR格式的铬球时存在内容不一致或错误的问题。

Method: 使用Stable Diffusion XL进行铬球修复，引入DiffusionLight通过迭代修复生成稳定的低频光照先验，并利用Exposure LoRA生成多曝光图像合并为HDR。进一步提出DiffusionLight-Turbo，通过Turbo LoRA直接预测平均铬球，大幅提升速度。

Result: 实验表明该方法能生成高质量的光照估计，泛化能力强，DiffusionLight-Turbo将运行时间从30分钟缩短至30秒。

Conclusion: 该方法通过扩散模型和LoRA技术有效解决了光照估计的泛化和效率问题，适用于多样化场景。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [53] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: 提出了一种基于物理约束的新方法，直接从运动捕捉数据估计地面反作用力，结合欧拉积分和PD算法，提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖实验室专用设备（如力板）收集地面反作用力，限制了动态学习的应用范围。

Method: 利用欧拉积分方案和PD算法，从运动捕捉数据计算地面反作用力，并结合物理约束优化学习模型。

Result: 在GroundLink数据集上测试，优于基线模型，地面反作用力估计精度和模拟根轨迹精度均有提升。

Conclusion: 该方法通过物理约束直接从运动捕捉数据估计地面反作用力，减少了实验室设备的依赖，提高了估计精度。

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [54] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级方法，通过学习后照明估计映射，将中性白平衡校正转换为美学偏好校正，实现跨相机美学一致性。


<details>
  <summary>Details</summary>
Motivation: 商业自动白平衡（AWB）系统通常追求美学偏好而非中性色彩校正，且现有学习型方法难以泛化到不同相机传感器。

Method: 通过学习一个后照明估计映射，将中性白平衡校正转换为美学偏好校正，并在相机无关空间中实现一致性。

Result: 模型仅含约500参数，运行时间为0.024毫秒，在771张智能手机图像数据集上表现优异。

Conclusion: 该方法在保持与现有跨相机AWB技术兼容的同时，实现了美学一致性和高效性能。

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [55] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: GTTA是一种通用的测试时间增强方法，适用于多种视觉和非视觉任务，通过扰动PCA子空间投影形成鲁棒集成，并结合自监督学习降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间增强方法通用性不足，GTTA旨在提供一种通用且高效的解决方案。

Method: 通过随机扰动PCA子空间投影形成集成，并引入自监督学习阶段优化模型。

Result: 在多种任务和数据集上验证了GTTA的通用性和有效性，包括低能见度水下视频中的鲑鱼分割与检测。

Conclusion: GTTA是一种高效、通用的测试时间增强方法，适用于多种任务，且计算成本低。

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [56] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: 论文提出了一种长尾分布感知路由器（LTDR），用于视觉语言混合专家（MoE）模型中的令牌到专家路由（TER），解决了视觉和语言模态分布差异的问题，并通过过采样策略增强视觉尾部令牌的处理。


<details>
  <summary>Details</summary>
Motivation: 现有MoE框架在视觉语言模型中忽视了视觉和语言模态的分布差异，导致路由策略不够高效。

Method: 提出LTDR，分别针对语言和视觉模态设计分布感知路由策略，并对视觉尾部令牌采用过采样策略。

Result: 在多个基准测试中验证了LTDR的有效性。

Conclusion: LTDR通过模态特异性路由和尾部令牌增强策略，显著提升了视觉语言MoE模型的性能。

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [57] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯溅射（3DGS）的物理攻击框架PGA，通过快速重建和真实感渲染，增强了跨视角鲁棒性和对抗效果。


<details>
  <summary>Details</summary>
Motivation: 现有伪装攻击方法依赖目标对象的网格先验和模拟器构建的虚拟环境，耗时且与现实存在差异，导致对抗效果和鲁棒性不足。

Method: 利用3DGS实现快速精确重建和真实感渲染，通过防止高斯相互遮挡和自遮挡，以及采用min-max优化调整背景，提升鲁棒性。

Result: 实验验证了PGA的有效性和优越性。

Conclusion: PGA在复杂物理环境中表现出更强的对抗效果和跨视角鲁棒性。

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [58] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种新的少样本奖励建模方法Activation RMs，通过激活导向构建对齐的奖励信号，无需额外微调，优于现有方法，并在防止奖励攻击方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统奖励建模难以适应新偏好，需要大量数据和单独模型，因此提出更灵活的少样本方法。

Method: 利用激活导向构建奖励信号，无需额外模型微调，适用于少样本场景。

Result: 在标准奖励建模基准和PreferenceHack基准上表现优异，超越现有方法及GPT-4o。

Conclusion: Activation RMs是一种高效、灵活的奖励建模方法，适用于安全关键应用。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [59] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 提出了一种名为“主动测量”的人机协作AI框架，通过重要性采样和蒙特卡洛估计提高科学测量的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学发现中的应用缺乏足够的准确性和统计保证，需要一种更可靠的方法。

Method: 结合AI预测和人工标注，采用重要性采样和蒙特卡洛估计逐步优化模型和测量结果。

Result: 主动测量框架在多种测量任务中显著降低了估计误差。

Conclusion: 主动测量为科学测量提供了一种高效且精确的解决方案，尤其适用于AI模型不完美或需要统计保证的场景。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [60] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于伪标签增强的音频-视觉Mamba网络（MUG），用于弱监督音频-视觉视频解析，显著提升了段级和事件级预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在弱监督和模型架构限制下，难以同时优化段级和事件级预测。

Method: 通过伪标签增强生成新数据，并采用音频-视觉Mamba网络进行特征处理和交互。

Result: 在LLP数据集上，MUG在各项指标上均优于现有方法（如视觉段级和音频段级指标分别提升2.1%和1.2%）。

Conclusion: MUG通过伪标签增强和Mamba网络设计，有效提升了弱监督音频-视觉视频解析的性能。

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [61] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk框架通过EMI和EDI分别解决身份泄漏和渲染伪影问题，提升说话头生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端情况下存在身份泄漏和渲染伪影问题，影响生成质量。

Method: 提出EMI解耦身份信息与运动特征，EDI利用泄漏身份信息补充细节。

Result: 实验表明FixTalk有效减少身份泄漏和伪影，性能优于现有方法。

Conclusion: FixTalk为高质量说话头生成提供了有效解决方案。

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [62] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 论文提出了一种利用标准地图（SD）信息预测高精地图（HD）元素的方法，通过混合编码和去噪技术提升性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶依赖高精地图，但在线构建高精地图仍面临复杂道路拓扑建模的挑战。

Method: 提出一种网络架构，利用SD地图信息预测车道段及其拓扑关系，结合混合编码和去噪技术，并引入时间一致性。

Result: 实验表明，该方法显著优于现有方法。

Conclusion: 通过利用SD地图信息和改进建模方法，实现了更优的高精地图在线构建。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [63] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 本文首次系统研究了深度学习人脸识别系统中的后门攻击，提出了两种针对人脸检测任务的后门攻击方法，并展示了攻击者如何通过单一后门绕过整个系统功能。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别系统的广泛应用引发了安全隐患，但针对现实场景中无约束系统的后门攻击研究仍为空白。

Method: 通过探索DNN后门在整个人脸识别流程中的可行性，提出了人脸生成和人脸关键点偏移两种攻击方法，并结合大规模边缘损失训练的特征提取器进行实验。

Result: 在20种管道配置和15种攻击案例中，单一后门成功绕过系统功能，验证了攻击的可行性。

Conclusion: 研究揭示了后门攻击的严重威胁，并提出了针对性的最佳实践和防御措施。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [64] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: 提出了一种基于多实例学习（MIL）的方法，用于胎儿腹部异常的病例级分类，无需标准平面定位，通过混合注意力专家模块（MoAE）、医学知识驱动的特征选择模块（MFS）和基于提示的原型学习（PPL）提升性能。


<details>
  <summary>Details</summary>
Motivation: 胎儿腹部畸形是严重的先天性异常，需要准确诊断以指导妊娠管理和降低死亡率。现有AI研究多关注图像级分类和标准平面定位，缺乏病例级诊断方法。

Method: 1. 使用混合注意力专家模块（MoAE）对不同平面的注意力头进行加权；2. 提出医学知识驱动的特征选择模块（MFS），在病例级进行自监督图像标记选择；3. 提出基于提示的原型学习（PPL）增强MFS。

Result: 在大规模胎儿腹部超声数据集（2,419例，24,748张图像，6个类别）上验证，性能优于现有方法。

Conclusion: 该方法在病例级分类中表现出色，为胎儿腹部异常的诊断提供了新思路。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [65] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 论文提出CaptionSmiths方法，通过量化标题属性并插值端点向量，实现单模型灵活控制生成标题的语言模式。


<details>
  <summary>Details</summary>
Motivation: 现有图像标题生成模型难以精细控制生成标题的属性（如长度、描述性），且缺乏平滑过渡能力。

Method: 量化标题长度、描述性和词汇独特性为连续标量，通过插值端点向量实现条件控制。

Result: 模型能平滑调整标题属性，词汇对齐优于基线，长度控制误差降低506%。

Conclusion: CaptionSmiths为单模型实现多样化语言模式控制提供了有效解决方案。

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [66] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度方向一致性的OOD检测方法，通过短路虚假梯度来提升检测效果，同时保持ID分类性能。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中，OOD检测对深度学习模型的安全部署至关重要。观察到ID样本的梯度方向一致，而OOD样本的梯度方向混乱，因此提出改进方法。

Method: 提出一种推理阶段技术，短路虚假梯度，并通过局部一阶近似避免二次前向传播。

Result: 在标准OOD基准测试中表现显著提升，方法轻量且易于集成。

Conclusion: 该方法为实际应用中的OOD检测提供了一种实用且高效的解决方案。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [67] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 提出了一种名为DocShaDiffusion的潜在空间扩散模型，用于文档图像阴影去除，结合阴影软掩模生成模块（SSGM）和阴影掩模引导扩散模块（SMGDM），有效解决了彩色阴影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅处理恒定颜色背景的阴影，忽略了彩色阴影，因此需要一种更全面的解决方案。

Method: 设计潜在空间扩散模型DocShaDiffusion，结合SSGM生成阴影掩模并添加噪声，SMGDM引导扩散和去噪过程去除阴影，并提出阴影鲁棒感知特征损失。

Result: 在三个公共数据集上的实验验证了方法的优越性，且开发了合成文档彩色阴影去除数据集（SDCSRD）支持训练。

Conclusion: DocShaDiffusion在文档阴影去除任务中表现优异，代码和数据集将公开。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [68] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的新型鲁棒水印框架DiffMark，通过改进训练和采样方案，结合面部图像和水印条件生成水印图像，并引入交叉信息融合模块和对抗性指导增强水印对Deepfake操作的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Deepfake技术带来的安全和隐私威胁需要更鲁棒的水印方法进行验证和追踪，现有方法在对抗Deepfake操作时表现不足。

Method: 通过修改扩散模型的训练和采样方案，结合面部图像和水印条件逐步去噪生成水印图像；引入交叉信息融合模块和对抗性指导增强鲁棒性。

Result: 实验证明DiffMark在典型Deepfake操作中表现有效。

Conclusion: DiffMark为对抗Deepfake提供了一种鲁棒的水印解决方案，代码将开源。

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [69] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: TurboReg是一种快速鲁棒的PCR估计器，基于轻量级TurboClique和并行PGS算法，显著提升了速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于极大团搜索的PCR方法虽召回率高，但时间复杂度高，不适用于实时应用。

Method: 提出TurboClique（3-团）和PGS算法，通过高约束兼容图保证鲁棒性，线性时间复杂度提升效率。

Result: 在3DMatch+FCGF数据集上，TurboReg速度提升208.22倍且召回率更高。

Conclusion: TurboReg在速度和性能上均达到SOTA，适用于实时PCR任务。

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [70] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO是一个新颖的多级异常分割框架，通过粗到细的检测策略解决现有像素级方法的局限性，结合不确定性引导的检测模型和像素级分割模型，显著提升异常分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有像素级异常分割方法忽视像素间的空间相关性，且全局阈值策略导致误检或漏检，限制了实际应用效果。

Method: 提出OoDDINO框架，包含正交不确定性感知融合策略（OUAFS）和自适应双阈值网络（ADT-Net），分两阶段实现异常检测与分割。

Result: 在多个基准数据集上验证了OoDDINO的优越性和兼容性，显著优于现有方法。

Conclusion: OoDDINO通过多级策略和动态阈值设计，有效解决了异常分割中的关键挑战，具有广泛适用性。

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [71] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: NOCTIS是一种无需重新训练即可处理多种新物体实例分割的框架，结合Grounded-SAM 2和DINOv2，通过改进匹配算法在BOP 2023挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决无需重新训练即可处理多种新物体实例分割的难题。

Method: 利用Grounded-SAM 2获取物体提议和分割掩码，结合DINOv2的零样本能力生成嵌入，改进匹配算法。

Result: 在BOP 2023挑战的七大数据集上表现优于现有RGB和RGB-D方法。

Conclusion: NOCTIS框架简单高效，适用于新物体实例分割任务。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [72] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为REG的方法，通过将低层图像潜在变量与预训练模型的高层类别标记纠缠，显著提升了生成质量和训练效率，且推理开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如REPA）在扩散模型中通过外部视觉表征对齐缓解训练挑战，但未能充分利用判别表征的潜力。

Method: 提出REG方法，将低层图像潜在变量与预训练基础模型的单个高层类别标记纠缠，实现从纯噪声中生成一致的图像-类别对。

Result: 在ImageNet 256×256上，REG显著加速收敛（比SiT-XL/2快63倍，比SiT-XL/2 + REPA快23倍），且训练400K次即优于REPA训练4M次的结果。

Conclusion: REG通过语义知识主动指导图像生成，显著提升了生成质量和效率，且推理开销极小。

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [73] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 论文提出了一种快速、低功耗的甲烷泄漏检测方法，通过优化算法（Mag1c-SAS和CEM）和机器学习模型（U-Net、LinkNet）实现高效检测，并提出了三种波段选择策略，显著提升了计算速度。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，通过高光谱卫星图像早期检测其泄漏有助于缓解气候变化。现有方法计算量大，难以在资源有限的机载硬件上实现实时检测。

Method: 测试了快速目标检测方法（ACE、CEM），提出了Mag1c-SAS算法，并结合机器学习模型（U-Net、LinkNet）进行检测优化。同时提出了三种波段选择策略。

Result: Mag1c-SAS和CEM在检测强羽流时表现良好，计算速度分别比原Mag1c快约100倍和230倍。其中一种波段选择策略在减少通道数的同时保持了准确性。

Conclusion: 研究为机载甲烷检测提供了高效、低硬件需求的解决方案，提升了数据实时性。代码、数据和模型已开源。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [74] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于主动控制点的6DoF姿态跟踪方法，用于解决工业金属物体因反射特性导致的跟踪难题。


<details>
  <summary>Details</summary>
Motivation: 工业金属物体的姿态跟踪因反射特性在现实环境中具有挑战性，需要一种更有效的方法。

Method: 使用图像控制点主动生成边缘特征进行优化，而非基于6DoF姿态的渲染，并引入最优控制点回归方法以提高鲁棒性。

Result: 在数据集评估和实际任务中均表现有效，为工业金属物体的实时跟踪提供了可行方案。

Conclusion: 该方法为工业金属物体的姿态跟踪提供了一种高效且鲁棒的解决方案，代码已开源。

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [75] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: 论文提出了一种增强多模态融合方法鲁棒性的策略，用于高精地图构建，包括数据增强、新型多模态融合模块和模态丢弃训练策略，并在NuScenes数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模型精度，而忽略了感知模型的鲁棒性，这对实际应用至关重要。

Method: 提出三种关键组件：数据增强、新型多模态融合模块和模态丢弃训练策略。

Result: 实验结果表明，方法显著提升了基线模型的鲁棒性，并在NuScenes数据集上达到最优性能。

Conclusion: 研究为开发更鲁棒可靠的高精地图构建模型提供了有价值的见解，推动了其在现实自动驾驶场景中的应用。

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [76] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: AVC-DPO是一种后训练框架，通过偏好对齐增强视频多模态大语言模型的字幕生成能力，在LOVE@CVPR'25 Workshop Track 1A中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLMs在字幕生成任务中难以根据人类偏好调整焦点，需改进。

Method: 设计针对时空动态和空间信息的增强提示，通过偏好感知训练和对齐优化字幕生成。

Result: 在VDC基准测试中取得第一名的成绩，表现卓越。

Conclusion: AVC-DPO有效提升了视频字幕生成的人类偏好对齐能力。

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [77] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: 综述分析了2018至2025年间37项关于AI害虫分类的研究，探讨了模型架构、数据集及技术挑战，指出从CNN转向混合和Transformer模型的趋势，并总结了当前的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统害虫监测方法效率低且难以扩展，深度学习技术（如CNN、ViT）为自动化害虫检测提供了解决方案。

Method: 通过分析37项研究，按作物类型、害虫种类、模型架构等分类，总结技术趋势和挑战。

Result: 研究发现模型从CNN转向混合和Transformer架构，但仍面临数据集不平衡、小害虫检测难等问题。

Conclusion: 综述提供了该领域的结构化概述，指出了未来AI害虫监测系统的关键挑战和发展方向。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [78] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出了一种无需训练、无需用户提供掩码的ReFlow图像编辑方法，通过分析多模态Transformer块的中间表示，利用中步潜在特征提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 尽管ReFlow在图像质量和文本对齐上优于扩散模型，但其在真实图像编辑中的应用仍具挑战性。

Method: 分析多模态Transformer块的中间表示，提取三个关键特征，利用中步潜在特征进行结构保留，并通过注意力调整提升编辑效果。

Result: 在两个基准测试中优于九种基线方法，并通过人类评估验证了用户偏好。

Conclusion: 该方法在无需训练和用户干预的情况下，显著提升了ReFlow在真实图像编辑中的性能。

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [79] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: 论文提出了一种结合传统方法和深度学习的规则化树冠检测方法，以提高检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 全球变暖、生物多样性丧失和空气污染等问题亟需森林监测，但缺乏自动化监测手段。

Method: 结合传统方法的特征提取与分割和深度学习的树冠检测，提出规则化后处理方法。

Result: 通过邻近树和局部操作增加了检测到的树冠数量，并对比分析了结果。

Conclusion: 新方法提升了树冠检测效果，但仍需改进其优缺点。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [80] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 论文提出了一种跨模态框架cRID，结合视觉语言模型和图注意力网络，用于检测行人数据中的PII并提升行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决街景数据开放中的隐私风险，特别是行人PII泄露问题。

Method: 结合大型视觉语言模型、图注意力网络和表示学习，检测文本可描述的PII线索。

Result: 在跨数据集行人重识别任务中表现优异，如从Market-1501到CUHK03-np。

Conclusion: cRID框架能有效检测语义PII并提升行人重识别性能，具有实际应用价值。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [81] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP是一种基于Segment Anything Model (SAM)的创新方法，通过边界蒸馏模块和1D-2D Mamba适配器，显著提升了息肉分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 息肉分割在结肠镜图像中至关重要，但现有方法在弱边界和泛化性方面表现不足，难以满足临床实时需求。

Method: 提出SAM-MaGuP，结合边界蒸馏模块和1D-2D Mamba适配器，增强全局上下文交互和边界特征学习。

Result: 在五个多样化数据集上，SAM-MaGuP超越了现有方法，实现了更高的分割准确性和鲁棒性。

Conclusion: SAM-MaGuP通过创新的Mamba引导边界先验和1D-2D Mamba块，为息肉分割领域设定了新标准。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [82] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: 研究了基于姿态的数据预处理技术（归一化、插值和增强）对手语翻译性能的影响，使用改进的T5编码器-解码器模型，实验表明这些技术能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索姿态数据预处理技术如何提升手语翻译（SLT）的性能，特别是在连续、无注释的翻译系统中。

Method: 采用改进的T5编码器-解码器模型处理姿态表示，并通过YouTubeASL和How2Sign数据集进行消融实验，分析不同预处理策略的影响。

Result: 适当的归一化、插值和增强技术显著提高了模型的鲁棒性和泛化能力，添加专用寄存器标记也能提升性能。

Conclusion: 姿态数据预处理技术对SLT性能有重要影响，未来可进一步优化模型架构和预处理方法。

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [83] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于Mamba的TrackingMiM架构，解决了Vision Transformer在无人机跟踪任务中的二次复杂度问题，实现了高效且精确的跟踪。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在无人机实时跟踪任务中因二次复杂度问题效率低下，而现有的Mamba方法未能考虑时间连续性。

Method: 提出TrackingMiM，采用嵌套Mamba扫描机制，独立处理时空一致的图像块，模板帧作为查询令牌用于跟踪。

Result: 在五个无人机跟踪基准测试中，TrackingMiM实现了最高精度和显著的速度提升。

Conclusion: TrackingMiM是一种高效且精确的无人机跟踪解决方案，解决了现有方法的局限性。

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [84] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: 本文提出了一个开源基准数据集，用于促进AI在CT分析中的泛化能力，通过减少数据分布偏移。


<details>
  <summary>Details</summary>
Motivation: AI在医学CT分析中因数据分布偏移（如扫描仪差异）导致泛化能力差，需要解决这一问题。

Method: 使用包含1378个CT扫描的开源数据集，涵盖不同扫描仪和设置，并提出评估图像和特征稳定性的方法。

Result: 提供了基线结果和开源代码，支持AI协调策略的开发。

Conclusion: 该数据集和方法为AI协调技术的发展提供了重要资源。

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [85] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 提出了一种基于IIR滤波器矩阵的方法，能去除事件相机数据中约99%的噪声，同时保留有效信号。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据流中存在显著噪声，影响应用效果。

Method: 设计了四种基于IIR滤波器矩阵的算法，并在多个事件数据集上测试。

Result: 方法在1280x720分辨率传感器上仅需约30KB内存，适合嵌入式设备。

Conclusion: 该方法高效去噪且资源占用低，适用于实际嵌入式应用。

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [86] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 论文提出了一种结合判别式学习和生成式学习的方法（IDGBR），用于改进遥感图像语义分割中的边界精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖判别式学习，擅长捕捉低频特征但忽视高频特征，而生成式扩散模型擅长生成高频细节但语义推理不足。

Method: 结合判别式模型生成粗分割图，并通过条件引导网络和扩散去噪过程细化边界。

Result: 在五个遥感数据集上验证了IDGBR能一致改进不同判别式架构的粗分割结果。

Conclusion: IDGBR框架有效结合了两种学习的优势，显著提升了边界分割精度。

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [87] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: SketchColour是一种基于扩散变换器（DiT）的草图到色彩转换方法，显著减少了参数和GPU内存使用，并在SAKUGA数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统2D动画制作需要大量手工绘制和上色，耗时耗力。

Method: 采用DiT架构替代U-Net去噪器，通过轻量级通道连接适配器和LoRA微调注入草图信息。

Result: 在SAKUGA数据集上表现优于现有方法，训练数据仅为竞争模型的一半，且生成动画时间连贯、伪影少。

Conclusion: SketchColour为2D动画上色提供了高效且高质量的解决方案。

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [88] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出了一种基于相机参数的可控去噪框架，通过ISO、快门速度和光圈值调整去噪强度，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏根据噪声水平、相机设置和用户偏好调整去噪强度的灵活性。

Method: 将ISO、快门速度和光圈值转换为向量，控制去噪网络的性能。

Result: 实验表明，该方法为去噪网络增加了可控性并提升了性能。

Conclusion: 提出的框架有效增强了去噪网络的灵活性和性能。

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [89] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 该研究提出了一种多模态课堂监控系统，通过检测学生睡意、手机使用和人脸识别来评估注意力，利用YOLOv8和LResNet Occ FC等技术实现高精度实时监测。


<details>
  <summary>Details</summary>
Motivation: 提升课堂学生注意力评估的精确性和全面性，同时实现自动考勤记录。

Method: 结合YOLOv8模型检测手机和睡意，LResNet Occ FC进行人脸识别，使用RMFD和Roboflow数据集训练，系统通过PHP和ESP32-CAM硬件实现。

Result: 睡意检测mAP@50为97.42%，人脸识别准确率86.45%，手机检测mAP@50为85.89%。

Conclusion: 该系统为教育环境提供了高效、可扩展的监控和考勤解决方案。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [90] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync提出了一种无需训练的框架，通过扩散引导实现长视频深度预测的尺度和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时存在尺度不一致和几何预测不准确的问题，主要依赖2D扩散先验而忽略了3D几何结构。

Method: 引入尺度引导和几何引导，前者同步不同窗口的深度尺度，后者基于3D约束在窗口内实现几何对齐。

Result: 实验表明，DepthSync在多个数据集上显著提升了深度预测的尺度和几何一致性，尤其适用于长视频。

Conclusion: DepthSync通过协同作用的两项引导，有效解决了长视频深度预测中的尺度和几何不一致问题。

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [91] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 介绍首届W-CODA研讨会，聚焦自动驾驶极端场景的多模态感知与理解技术。


<details>
  <summary>Details</summary>
Motivation: 探索下一代自动驾驶极端场景解决方案，推动前沿技术与可靠自动驾驶的融合。

Method: 邀请5位学术与工业界专家分享进展，举办双轨挑战赛（场景理解与生成）。

Result: 汇集研究论文，推动自动驾驶技术在极端场景中的进步。

Conclusion: W-CODA将持续弥合前沿技术与可靠自动驾驶之间的差距。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [92] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: POLC提出了一种感知导向的潜在编码方法，通过丰富潜在特征的语义内容，提升压缩域语义推理性能，同时减少微调参数。


<details>
  <summary>Details</summary>
Motivation: 传统基于MSE优化的图像编码模型在潜在空间中语义丰富性不足，且微调计算成本高。

Method: 提出POLC方法，通过感知导向优化潜在编码，仅需轻量级适配器微调。

Result: POLC在保持率感知性能的同时，显著提升视觉任务性能，且微调开销极低。

Conclusion: POLC为压缩域语义推理提供了一种高效且性能优越的解决方案。

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [93] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为P3HOT的框架，结合提示引导和人类近端感知，用于改进人体-物体接触（HOT）检测任务，解决了现有模型在区域分割和类别一致性上的问题。


<details>
  <summary>Details</summary>
Motivation: 当前HOT检测模型局限于单一图像类型，导致过度分割和类别一致性不足，需要一种更高效的方法。

Method: P3HOT框架结合语义驱动的提示机制和人类近端感知机制，动态感知关键深度范围，并使用新的Regional Joint Loss和AD-Acc.评估指标。

Result: 在HOT-Annotated数据集上，模型在SC-Acc.、mIoU、wIoU和AD-Acc.四个指标上分别提升了0.7、2.0、1.6和11.0。

Conclusion: P3HOT框架显著提升了HOT检测的性能，为相关任务提供了新的解决方案。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [94] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF是一种用于大规模场景3D重建的框架，通过分块处理和优化采样策略，解决了传统NeRF方法的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法因内存限制仅适用于小场景，无法处理大规模卫星图像。本文旨在解决这一问题。

Method: 提出Snake-NeRF框架，采用分块处理、图像重叠裁剪、2×2 3D瓦片渐进策略和分段采样器。

Result: 实验表明，该方法能在单GPU上线性时间处理大规模卫星图像，且不损失重建质量。

Conclusion: Snake-NeRF为大规模场景3D重建提供了一种高效且高质量的解决方案。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [95] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC是一种基础单目深度估计模型，能够在多样环境条件下工作，通过无监督一致性正则化微调和空间距离约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础MDE模型在复杂开放世界环境中表现不佳，尤其是在光照变化、恶劣天气和传感器失真等挑战性条件下。

Method: 提出无监督一致性正则化微调范式，仅需少量未标记数据，并引入空间距离约束以学习补丁级相对关系。

Result: 实验表明，DepthAnything-AC在多样化基准测试中表现出零样本能力，包括真实世界恶劣天气、合成失真和通用基准。

Conclusion: DepthAnything-AC通过创新方法解决了复杂环境下的深度估计问题，具有广泛适用性。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [96] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: SAILViT是一种逐步特征学习增强的ViT，用于解决ViT与LLMs直接协同训练中的参数冲突和模态语义差距问题，提升MLLMs在复杂多模态交互中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViTs通过图像-文本对比学习或自监督机制表现优异，但与LLMs直接协同训练时存在参数初始化冲突和模态语义差距问题。

Method: 提出SAILViT，通过逐步特征细化实现粗到细粒度特征对齐和世界知识注入，适应目标训练需求。

Result: SAILViT在不同参数规模、架构、训练策略和数据规模下表现出强大鲁棒性和泛化能力，显著提升MLLMs在OpenCompass基准上的性能。

Conclusion: SAILViT有效解决了ViT与LLMs协同训练的挑战，为MLLMs在多模态任务中的性能提升提供了新方法。

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [97] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新型注意力机制LASAD，通过保持2D空间关系解决了线性注意力在图像生成中的性能问题，并基于此构建了高效的自回归图像生成器LASADGen。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型依赖Transformer架构，存在计算复杂度和内存开销大的问题，而线性注意力机制在图像生成中因无法捕捉长距离依赖导致质量下降。

Method: 提出LASAD机制，通过基于真实2D空间位置计算位置依赖的衰减因子，保持空间关系，并构建LASADGen生成器。

Result: 在ImageNet上实验表明，LASADGen实现了最先进的图像生成性能和计算效率。

Conclusion: LASADGen成功平衡了线性注意力的高效性和高质量图像生成所需的空间理解能力。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [98] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出RobuSTereo框架，通过扩散模拟和稳健特征编码提升立体匹配模型在恶劣天气下的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的立体匹配模型在恶劣天气下表现不佳，主要因训练数据稀缺和特征提取困难。

Method: 1. 提出扩散模拟管道生成高质量合成数据；2. 设计结合ConvNet和去噪Transformer的稳健特征编码器。

Result: 实验表明，RobuSTereo显著提升了模型在恶劣天气下的鲁棒性和泛化能力。

Conclusion: RobuSTereo通过合成数据和稳健特征提取，有效解决了立体匹配在恶劣天气下的挑战。

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [99] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出了一种名为SPoT的新颖标记化策略，通过连续放置标记来避免网格限制，显著提升性能并减少推理所需的标记数量。


<details>
  <summary>Details</summary>
Motivation: 标准标记化方法将特征限制在离散的网格中，阻碍了模型在稀疏场景下的潜力，需要妥协。

Method: 提出Subpixel Placement of Tokens (SPoT)策略，通过连续放置标记，并结合oracle-guided搜索找到最佳位置。

Result: 实验表明，SPoT能显著提升性能，并大幅减少推理时所需的标记数量。

Conclusion: SPoT为ViT架构提供了灵活、高效和可解释的新方向，将稀疏性转化为战略优势。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [100] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: 论文研究了图像目标导航任务，探讨了是否可以通过端到端强化学习训练完整代理来解决该任务，并分析了架构选择和模拟器设置对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证端到端强化学习是否能够高效解决图像目标导航任务，从而避免依赖专用图像匹配或预训练计算机视觉模块。

Method: 通过大规模研究，分析了多种架构选择（如延迟融合、通道堆叠、空间到深度投影和交叉注意力）及其在导航训练中相对姿态估计器的作用。

Result: 研究发现模拟器设置会影响方法性能，导致模拟中的捷径，但这些能力可以部分迁移到更现实的场景。导航性能与相对姿态估计性能之间存在相关性。

Conclusion: 结论表明端到端强化学习可以部分解决图像目标导航任务，但模拟器设置和架构选择对性能有显著影响。

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [101] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: FACET-VLM是一个用于3D/4D面部表情识别的视觉语言框架，通过多视角表示学习和语义引导实现高性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D/4D面部表情识别中复杂的时空动态问题，推动情感计算在行为理解、医疗监测和人机交互中的应用。

Method: 提出CVSA、MTGF和多视角一致性损失三个关键组件，实现视角一致性和语义对齐。

Result: 在多个基准测试中达到最优性能，并成功扩展到4D微表情识别。

Conclusion: FACET-VLM为多模态面部表情识别提供了高效、可扩展的解决方案。

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [102] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: AdaGCD是一种基于自适应槽注意力的聚类对比学习框架，用于在部分标记数据集中发现已知和未知类别，无需预定义类别数量。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义类别数量的假设，限制了处理真实世界数据的能力。AdaGCD旨在通过动态确定槽数量来解决这一问题。

Method: AdaGCD结合自适应槽注意力（AdaSlot），动态分配表示容量，灵活聚类未标记数据为已知和未知类别。

Result: 在公开和细粒度数据集上的实验验证了AdaGCD的有效性，尤其在利用空间局部信息进行类别发现方面表现优越。

Conclusion: AdaGCD通过自适应机制和动态槽分配，显著提升了开放世界场景下的类别发现能力。

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [103] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: 提出了一种改进的小波域指纹提取方法，避免了传统反演步骤，提高了检测精度和处理速度。


<details>
  <summary>Details</summary>
Motivation: 传统小波去噪方法在提取传感器模式噪声（SPN）时需要图像反演步骤，效率较低。

Method: 提出小波域指纹概念，直接在频域进行指纹提取和比较，避免反演步骤。

Result: 实验表明，该方法在真实数据集上检测精度更高，处理速度显著提升。

Conclusion: 小波域指纹方法简化了流程，提高了效率和准确性。

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [104] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: 论文提出了一种基于软自标记的弱监督分割方法，通过优化CRF/Potts损失，显著提升了基于涂鸦标签的训练效果，甚至优于全像素监督。


<details>
  <summary>Details</summary>
Motivation: 解决弱监督分割中硬伪标签无法表示类别不确定性和错误的问题，提出软自标记方法。

Method: 通过优化CRF/Potts损失，引入软伪标签，并评估不同CRF松弛、邻域系统和连接网络预测与软伪标签的项。提出通用连续子问题求解器。

Result: 软自标记显著提升了基于涂鸦标签的训练效果，优于复杂专用WSSS系统，甚至超越全像素监督。

Conclusion: 软自标记是一种有效的弱监督分割方法，其通用性可应用于其他弱监督问题/系统。

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [105] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: 论文研究了剪枝对视觉模型在可解释性、无监督目标发现和人类感知对齐三个维度的影响，发现稀疏模型存在性能最佳点，但其效果依赖于网络架构和参数规模。


<details>
  <summary>Details</summary>
Motivation: 探讨剪枝如何影响深度视觉模型的可解释性、表示学习能力以及与人类感知的对齐性。

Method: 分析不同视觉网络架构在不同稀疏度下对特征归因可解释性方法的影响，研究剪枝是否能促进更简洁的结构化表示，并评估剪枝是否增强模型与人类感知的对齐。

Result: 发现稀疏模型存在性能最佳点（sweet spots），在这些点上模型具有更高的可解释性、下游泛化能力和人类对齐性，但这些最佳点高度依赖于网络架构和参数规模。

Conclusion: 剪枝对视觉模型的影响复杂，需进一步研究其在何时及如何提升视觉表示能力。

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [106] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn框架通过驱动-响应系统生成3D人-物交互，利用轻量级Transformer模型预测物体响应，并通过动态损失提升一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立处理人与物体运动，导致物理上不合理的行为，需要一种更一致的交互生成方法。

Method: 提出HOI-Dyn框架，将交互建模为驱动-响应系统，使用Transformer预测物体响应，并引入动态损失优化。

Result: 实验表明，HOI-Dyn提升了交互生成质量，并提供了可行的评估指标。

Conclusion: HOI-Dyn为3D人-物交互生成提供了高效且一致的解决方案。

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [107] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: DeRIS框架将Referring Image Segmentation分解为感知和认知两部分，发现认知能力不足是主要瓶颈，并提出Loopback Synergy机制增强模块协同，同时引入数据增强解决长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有RIS框架缺乏对基本瓶颈的系统分析，尤其是多模态认知能力的不足。

Method: 提出DeRIS框架，分解RIS为感知和认知模块，引入Loopback Synergy机制和数据增强。

Result: DeRIS在非和多目标场景中表现优异，无需专门架构修改。

Conclusion: DeRIS通过模块化分析和协同机制，显著提升了RIS性能，具有广泛适用性。

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [108] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: 本文研究了基于Vision Transformers (ViTs)和掩码自编码器(MAE)框架的3D医学图像分割方法，首次将其应用于颅内动脉钙化(IAC)分割，并在临床数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: ViTs在自然图像领域表现优异，但在3D医学图像分割中应用较少。IAC作为神经血管疾病的生物标志物，其自动化量化对大规模风险评估具有重要意义。

Method: 采用MAE框架预训练ViTs，并在IST-3临床试验数据上微调用于IAC分割。研究了低patch大小和插值上采样对模型性能的影响。

Result: 自监督ViT在Dice分数上优于监督nnU-Net基线3.2分，对高切片厚度更具鲁棒性，并在临床风险分类中提升46%。

Conclusion: ViTs结合MAE预训练在IAC分割中表现优异，为医学图像分析提供了新思路。

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [109] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 论文提出两种自监督多模态预训练技术和混合模型架构，用于从SAR图像中提取冰川崩解前沿位置，显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 冰川冰量快速流失，需高精度全年监测以理解崩解过程，现有基于ImageNet预训练的模型因领域差异表现不佳。

Method: 提出自监督预训练技术（SSL4SAR数据集）和混合模型架构（Swin Transformer编码器+CNN解码器）。

Result: 新模型在CaFFe数据集上平均距离误差为293米，优于之前最佳模型67米；集成模型误差75米，接近人类水平（38米）。

Conclusion: 该技术实现了对冰川崩解前沿季节性变化的高精度监测。

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [110] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: DisCon框架通过将离散标记作为条件信号而非生成目标，解决了连续标记建模的优化挑战，同时避免了量化带来的信息损失，显著提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进展激发了将图像编码为离散标记的兴趣，但量化过程导致信息损失和图像保真度下降。连续标记虽能避免量化问题，但其高维无界空间增加了密度估计的难度和生成异常的风险。

Method: 提出了DisCon框架，通过建模连续表示在离散标记条件下的概率分布，既避免了连续标记的优化挑战，又减少了量化带来的信息损失。

Result: 在ImageNet 256×256生成任务中，DisCon取得了1.38的gFID分数，明显优于现有自回归方法。

Conclusion: DisCon通过结合离散和连续标记的优势，为视觉生成任务提供了一种高效且高质量的解决方案。

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [111] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: Vision transformers (ViTs) 在医学影像任务中表现优异，但其语义表示缺乏意义且易受微小变化影响，导致分类结果不可靠。


<details>
  <summary>Details</summary>
Motivation: 研究ViT在医学影像中的语义表示是否具有意义及其对微小变化的脆弱性。

Method: 使用基于投影梯度的算法分析ViT的表示。

Result: ViT的表示缺乏语义意义，微小变化可导致分类准确率下降60%以上。

Conclusion: ViT在医学影像分类中的语义表示问题揭示了其在安全关键系统中部署的挑战。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [112] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 提出了一种新的分段高斯金字塔（SGP）攻击方法，通过多尺度图像增强对抗样本的迁移性，显著提高了对黑盒防御模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的迁移性对深度神经网络构成安全威胁，现有方法通常仅关注单尺度图像，限制了攻击效果。

Method: 采用高斯滤波和三种下采样方法构建多尺度样本，计算各尺度损失函数的梯度并取平均值以确定对抗扰动。

Result: 实验表明，SGP显著提高了攻击成功率，平均提升2.3%至32.6%。

Conclusion: SGP是一种高扩展性的输入变换方法，易于集成到现有攻击中，有效增强对抗样本的迁移性。

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [113] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA提出了一种无需训练的多主体个性化图像生成框架，通过独立调整的LoRA模块融合实现高效多主体生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多主体个性化生成时需复杂调整或联合优化，限制了效率和通用性。

Method: 采用Full Token Tuning策略独立调整每个主体的LoRA模块，并在推理时通过Subject-Aware Inference激活对应模块。

Result: 实验表明FreeLoRA在主体保真度和提示一致性上表现优异。

Conclusion: FreeLoRA为多主体个性化生成提供了一种简单高效的解决方案。

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [114] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为HCNQA的3D VQA模型，通过分层监督方法解决现有答案中心监督方法缺乏推理路径监督的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D VQA模型依赖答案中心监督，缺乏对推理路径的监督，可能导致模型学习表面捷径。

Method: 提出分层聚焦监督方法，模拟人类从广泛到具体的注意力聚焦过程，监督关键推理节点。

Result: 实验表明，该方法能有效确保模型发展合理的推理路径并提升性能。

Conclusion: HCNQA通过分层监督解决了推理路径监督不足的问题，提升了模型性能。

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [115] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 提出了一种自适应动量和解耦对比学习框架（AMD），用于提升自动驾驶中长尾轨迹预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常仅依赖基础模型的预测误差，忽略了长尾轨迹模式的多样性和不确定性。

Method: 结合改进的动量对比学习（MoCo-DT）和解耦对比学习（DCL），设计了四种轨迹随机增强方法和在线迭代聚类策略。

Result: 在nuScenes和ETH/UCY数据集上，AMD在长尾轨迹预测和整体预测精度上均表现优异。

Conclusion: AMD框架显著提升了模型对复杂和罕见轨迹的识别能力，适应了长尾数据的分布变化。

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [116] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: 提出了一种基于多图像的超光谱重建（MI-HSR）框架，利用配备光谱滤镜的三摄像头智能手机系统，显著提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 解决单RGB图像在超光谱重建中因光谱信息严重丢失导致的精度限制问题。

Method: 使用三摄像头智能手机系统，其中两个镜头配备精选光谱滤镜，结合理论和实证分析，提出MI-HSR框架。

Result: 在新建数据集Doomer上验证，比现有方法提升30%的光谱估计精度。

Conclusion: 多视角光谱滤镜与商用硬件结合，可提供更准确、实用的超光谱成像解决方案。

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [117] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: 提出了一种极轻量级的CNN框架，仅约4K参数，实现了移动设备上的实时图像增强，最高达1,100 FPS。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在资源受限平台（如移动设备）上部署的高计算和内存需求问题。

Method: 结合重参数化和增量权重优化策略，引入特征自变换模块和分层双路径注意力机制，并使用局部方差加权损失优化。

Result: 首次实现高达1,100 FPS的实时图像增强，同时在多个任务中保持竞争力的图像质量。

Conclusion: 该框架在速度和性能之间取得了最佳平衡，代码已开源。

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [118] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 提出动态时序槽变换器（DTST）模块，用于解决手术视频中对象中心表示学习的挑战，并在多个手术数据库中实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的异构场景（如手术视频）难以解析为有意义的一组槽，现有方法在手术视频上表现不佳。

Method: 提出动态时序槽变换器（DTST）模块，结合时序推理和预测未来最优槽初始化。

Result: 在多个手术数据库中实现最佳性能。

Conclusion: 无监督对象中心方法可应用于现实世界数据，并成为医疗应用中的常用工具。

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [119] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种新型的自我增强原型演化与双知识合作框架（SPRED），用于解决半监督终身行人重识别（Semi-LReID）问题，通过动态原型引导的伪标签生成和新旧知识协同净化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中标注资源有限，现有方法在利用未标注数据时性能下降严重，因此需要一种能够高效利用未标注数据并减少噪声影响的方法。

Method: 引入可学习的身份原型动态捕捉身份分布并生成高质量伪标签，通过双知识合作方案整合当前模型和历史模型的知识，净化噪声伪标签。

Result: 在Semi-LReID基准测试中，SPRED实现了最先进的性能。

Conclusion: SPRED通过自我增强循环设计，显著提升了半监督终身行人重识别的性能，为未来研究提供了新思路。

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [120] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: 论文提出了Reason50K数据集和ReasonBrain框架，用于解决复杂隐含假设指令的图像编辑问题，通过多模态大语言模型和扩散模型实现推理与编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理需要深度推理的复杂隐含假设指令，且缺乏相关数据集和细粒度细节提取机制。

Method: 提出Reason50K数据集和ReasonBrain框架，结合多模态大语言模型和扩散模型，并引入细粒度推理线索提取模块（FRCE）和跨模态增强器（CME）。

Result: ReasonBrain在推理场景中表现优于现有方法，并在传统IIE任务中展现出强零样本泛化能力。

Conclusion: Reason50K和ReasonBrain为复杂指令的图像编辑提供了有效解决方案，具有广泛的应用潜力。

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [121] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于患者特异性数字孪生（DT）的管道，用于评估可变形图像配准（DIR）方法在胃肠道（GI）器官运动中的准确性，并验证剂量映射的精确性。


<details>
  <summary>Details</summary>
Motivation: 临床实施DIR需要基于体素的空间精度指标，如手动标记点，但对于高度移动的GI器官来说具有挑战性。因此，需要一种更有效的方法来评估DIR的准确性。

Method: 通过半自动化管道，从静态3D患者扫描生成21个模拟GI运动的4D序列数字孪生。使用11个数据集（包括MRI和CT扫描）评估DIR方法的性能，包括目标配准误差、Dice相似系数和Hausdorff距离。

Result: 管道生成的DT模拟了真实的GI运动，运动幅度和Jacobian行列式与真实患者数据相似。同时，能够提取详细的DIR性能指标并验证剂量映射的准确性。

Conclusion: 该管道为动态且解剖复杂的区域提供了严格的DIR工具测试方法，实现了空间和剂量精度的详细评估。

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [122] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: 提出了一种融合多季节结构数据的框架，用于支持果园自动化管理，通过结合休眠期和生长期的RGB-D图像，实现精确的3D重建和模型对齐。


<details>
  <summary>Details</summary>
Motivation: 果园自动化中，生长期的密集树叶遮挡了树结构，限制了机器视觉系统的能力，而休眠期树结构更清晰可见。

Method: 使用YOLOv9-Seg进行实例分割，Kinect Fusion进行3D重建，Fast GICP进行模型对齐，融合多季节数据。

Result: YOLOv9-Seg在休眠期数据集上的MSE为0.0047，mAP@50为0.78；Kinect Fusion的RMSE为5.23 mm（树干直径）；Fast GICP的最小适应分数为0.00197。

Conclusion: 该框架通过融合多季节数据，为机器人系统提供了被遮挡的结构信息，提高了果园自动化操作的精度。

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [123] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom是一个统一的图像定制框架，通过上下文学习整合位置感知和无位置定制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像定制方法缺乏通用框架的问题，提升多样定制的适用性。

Method: 提出IC-Custom框架，结合多模态注意力机制和任务导向的注册令牌，支持多种任务类型。

Result: 在多个基准测试中表现优异，人类偏好提升73%，仅训练0.4%的原始参数。

Conclusion: IC-Custom为工业应用提供了高效、通用的图像定制解决方案。

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [124] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为evMLP的新型视觉模型，结合事件驱动的局部更新机制，通过选择性处理图像或特征图中的变化区域，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索多层感知器（MLPs）在视觉任务中的应用，并解决传统方法在序列图像数据（如视频）中计算冗余的问题。

Method: 提出evMLP模型，利用事件驱动的局部更新机制，仅处理连续帧之间发生变化的图像块（事件）。

Result: 在ImageNet分类任务中表现优异，同时在视频数据集上显著降低了计算成本，同时保持输出一致性。

Conclusion: evMLP通过事件驱动机制有效减少了冗余计算，为视觉模型的高效处理提供了新思路。

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [125] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: 论文介绍了CI-VID数据集，支持从文本和视频到视频的生成（TV2V），解决了现有数据集仅支持孤立文本-视频对的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集仅包含孤立的文本-视频对，无法支持连贯的多场景视频序列建模。

Method: 提出CI-VID数据集，包含34万样本，每个样本包含连贯的视频剪辑序列和文本描述，支持视觉和文本驱动的生成。

Result: 实验表明，基于CI-VID训练的模型在生成视频序列时，准确性和内容一致性显著提升。

Conclusion: CI-VID数据集推动了故事驱动内容的生成，具有高质量的视觉过渡和时间连贯性。

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [126] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: 论文提出了一种名为LongAnimation的新框架，用于实现长动画的自动着色，解决了现有方法在长期颜色一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 长动画着色在动画产业中劳动成本高，现有方法仅适用于短期着色且忽视全局信息，无法保持长期颜色一致性。

Method: 提出动态全局-局部范式，包括SketchDiT、动态全局-局部记忆模块（DGLM）和颜色一致性奖励。DGLM动态压缩全局历史特征并与当前生成特征融合。

Result: 在短期（14帧）和长期（平均500帧）动画上的实验表明，LongAnimation能有效保持颜色一致性。

Conclusion: LongAnimation通过动态全局-局部范式成功解决了长动画着色的颜色一致性问题。

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [127] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: Kwai Keye-VL是一个80亿参数的多模态基础模型，专注于短视频理解，同时保持通用视觉语言能力。通过大规模数据集和创新的训练方法，实现了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在动态、信息密集的短视频理解上表现不足，Kwai Keye-VL旨在填补这一空白。

Method: 采用四阶段预训练和两阶段后训练，包括创新的五模式数据混合和强化学习。

Result: 在公共视频基准测试中达到最优性能，并在通用图像任务中保持竞争力。

Conclusion: Kwai Keye-VL在短视频理解上表现卓越，同时发布了新基准KC-MMBench。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [128] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph是一种无需调优的图像变形方法，适用于不同语义或布局的输入，通过创新的自注意力模块设计和步骤导向的变化趋势，实现了高质量和高效率的图像变形。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练扩散模型的微调，受限于时间和语义/布局差异，而FreeMorph旨在无需实例训练即可实现高保真图像变形。

Method: 1) 提出基于指导的球形插值设计，通过修改自注意力模块解决身份丢失问题；2) 引入步骤导向的变化趋势，混合输入图像的自注意力模块以实现可控过渡。

Result: FreeMorph比现有方法快10~50倍，并在图像变形领域达到新的最高水平。

Conclusion: FreeMorph通过创新的设计解决了无需调优方法的质量问题，为图像变形提供了高效且高质量的解决方案。

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


### [129] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 论文评估了多模态基础模型在标准计算机视觉任务上的表现，发现它们虽不及专业模型，但作为通用模型表现尚可，且语义任务优于几何任务。


<details>
  <summary>Details</summary>
Motivation: 研究多模态基础模型在视觉理解方面的实际能力，填补现有研究的空白。

Method: 通过提示链将标准视觉任务转化为文本可提示和API兼容的任务，建立标准化评估框架。

Result: 模型在语义任务上表现优于几何任务，GPT-4o在非推理模型中表现最佳，推理模型在几何任务上有改进。

Conclusion: 多模态基础模型在视觉任务中表现尚可，但仍有改进空间，尤其是在几何任务和提示敏感性方面。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [130] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: Locality-aware Parallel Decoding (LPD) 通过灵活并行自回归建模和局部感知生成顺序，显著加速自回归图像生成，减少生成步骤并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成依赖逐块预测，导致高延迟。现有方法尝试并行化但效果有限，LPD旨在实现高质量并行生成。

Method: 1. 灵活并行自回归建模：支持任意生成顺序和并行度，通过可学习位置查询令牌确保并行解码一致性。2. 局部感知生成顺序：分组最小化组内依赖，最大化上下文支持。

Result: 在ImageNet类条件生成中，生成步骤从256降至20（256×256分辨率）和1024降至48（512×512分辨率），延迟降低至少3.4倍。

Conclusion: LPD在保持生成质量的同时显著加速自回归图像生成，为高效并行解码提供了新思路。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [131] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: 研究探讨智能家居生态系统的安全威胁，提出后量子加密与AI异常检测的有效性，但面临计算资源挑战。区块链认证和零信任结构增强安全性，但需基础设施调整。


<details>
  <summary>Details</summary>
Motivation: 智能家居集成物联网设备面临日益增长的网络安全风险，需探索有效的安全解决方案。

Method: 分类网络层、设备级及云与AI系统的漏洞，评估后量子加密、AI异常检测、区块链认证和零信任结构的效果。

Result: 后量子加密与AI异常检测有效但资源密集；区块链和零信任结构需基础设施调整。ANOVA等测试验证策略有效性但缺乏扩展性。

Conclusion: 需改进加密技术、AI威胁检测和自适应安全模型，平衡性能与效率及实时适用性。

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [132] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CR

TL;DR: AutoAdv是一个自动化对抗性提示生成框架，用于评估和暴露大型语言模型（LLM）的安全机制漏洞，通过多轮动态攻击方法实现高达86%的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）仍易受越狱攻击，需要系统化评估其安全机制的脆弱性。

Method: 利用参数化攻击者LLM生成语义伪装的恶意提示，结合多轮攻击方法（如角色扮演、误导和上下文操纵）迭代优化攻击。

Result: 在ChatGPT、Llama和DeepSeek等先进模型上，AutoAdv实现了高达86%的越狱成功率。

Conclusion: 当前安全机制对复杂多轮攻击仍脆弱，亟需更强大的防御策略。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [133] [Quasi-twisted codes: decoding and applications in code-based cryptography](https://arxiv.org/abs/2507.01118)
*Bhagyalekshmy S,Rutuja Kshirsagar*

Main category: cs.CR

TL;DR: 本文提出了针对准扭转（QT）码的高效解码方法，并基于QT码构建了一种抗量子攻击的密码系统。


<details>
  <summary>Details</summary>
Motivation: QT码在编码理论中具有重要地位，但目前缺乏高效的解码算法。

Method: 提出了一种基于伴随式的高效解码方法，可纠正最多(d*-1)/2个错误，并构建了一种类似Niederreiter的密码系统。

Result: 解码方法高效且有效，密码系统能抵抗经典攻击和部分量子攻击。

Conclusion: 该研究为QT码的解码和密码应用提供了实用工具。

Abstract: Quasi-twisted (QT) codes generalize several important families of linear
codes, including cyclic, constacyclic, and quasi-cyclic codes. Despite their
potential, to the best of our knowledge, there exists no efficient decoding
algorithm for QT codes. In this work, we propose a syndrome-based decoding
method capable of efficiently correcting up to (d* - 1)/2 errors, where d*
denotes an HT-like lower bound on the minimum distance of QT codes, which we
formalize here. Additionally, we introduce a Niederreiter-like cryptosystem
constructed from QT codes. This cryptosystem is resistant to some classical
attacks as well as some quantum attacks based on Quantum Fourier Sampling.

</details>


### [134] [A Compact 16-bit S-box over Tower Field $\F_{(((2^2)^2)^2)^2}$ with High Security](https://arxiv.org/abs/2507.01423)
*Bahram Rashidi,Behrooz Khadem*

Main category: cs.CR

TL;DR: 本文提出了一种紧凑且安全的16位S盒，基于复合域设计，优化了硬件效率和密码学鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 满足数据敏感应用中对可扩展密码原语的需求，证明更大S盒可在不增加硬件成本的情况下提升安全性。

Method: 通过子域分解操作，利用塔域架构优化域反转和低开销仿射变换。

Result: 硬件资源消耗和关键路径延迟低于其他16位S盒，安全性指标优异（非线性度32512，差分均匀性4等）。

Conclusion: 复合域架构在平衡现代分组密码的安全性和效率方面具有可行性。

Abstract: This paper introduces a compact and secure 16-bit substitution box (S-box)
designed over the composite field $\F_{(((2^2)^2)^2)^2}$, optimized for both
hardware efficiency and cryptographic robustness. The proposed S-box decomposes
operations into subfields, leveraging a tower field architecture. This enables
significant hardware reduction through optimized field inversion and a low-cost
affine transformation. Security evaluations confirm resilience against linear,
differential, algebraic and DPA attacks, validated via metrics including
Nonlinearity (32512), Differential Uniformity (4), Algebraic Degree (15),
Transparency order (15.9875) and SNR (0.34e-08). The hardware results, in 65 nm
CMOS technology, show the proposed 16-bit S-box has lower hardware resources
consumption and lower critical path delay (CPD) than those of other 16-bit
S-boxes. By integrating high algebraic complexity with resource-efficient
structures, this work addresses the growing demand for scalable cryptographic
primitives in data-sensitive applications, demonstrating that larger S-boxes
can enhance security without proportional hardware costs. The results
underscore the viability of composite field-based architectures in balancing
security and efficiency for modern block ciphers.

</details>


### [135] [A new efficient RPKI Design](https://arxiv.org/abs/2507.01465)
*Haya Schulmann,Niklas Vogel*

Main category: cs.CR

TL;DR: 本文分析了RPKI设计的复杂性及其性能瓶颈，提出了改进版iRPKI，显著提升了处理速度、带宽效率和内存占用，同时保持了安全性和向后兼容性。


<details>
  <summary>Details</summary>
Motivation: RPKI作为BGP的关键安全机制，其设计复杂性和性能问题阻碍了大规模部署，因此需要优化。

Method: 通过系统分析RPKI的复杂性根源，提出iRPKI设计，消除冗余证书和签名，优化编码和存储结构。

Result: iRPKI在Routinator验证器中实现了20倍处理速度提升、18倍带宽优化和8倍内存占用减少，并消除了多类漏洞。

Conclusion: iRPKI显著提升了RPKI的可扩展性和部署可行性，适合在受限环境中使用，且可逐步部署。

Abstract: Resource Public Key Infrastructure (RPKI) is a critical security mechanism
for BGP, but the complexity of its architecture is a growing concern as its
adoption scales. Current RPKI design heavily reuses legacy PKI components, such
as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,
all these introduce excessive cryptographic validation, redundant metadata, and
inefficiencies in both storage and processing. We show that these design
choices, although based on established standards, create significant
performance bottlenecks, increase the vulnerability surface, and hinder
scalability for wide-scale Internet deployment.
  In this paper, we perform the first systematic analysis of the root causes of
complexity in RPKI's design and experimentally quantify their real-world
impact. We show that over 70% of validation time in RPKI relying parties is
spent on certificate parsing and signature verification, much of it
unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),
a backwards-compatible redesign that preserves all security guarantees while
substantially reducing protocol overhead. iRPKI eliminates EE-certificates and
ROA signatures, merges revocation and integrity objects, replaces verbose
encodings with Protobuf, and restructures repository metadata for more
efficient access. We experimentally demonstrate that our implementation of
iRPKI in the Routinator validator achieves a 20x speed-up of processing time,
18x improvement of bandwidth requirements and 8x reduction in cache memory
footprint, while also eliminating classes of vulnerabilities that have led to
at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the
feasibility of deploying RPKI at scale in the Internet, and especially in
constrained environments. Our design may be deployed incrementally without
impacting existing operations.

</details>


### [136] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 本文综述了安全洗牌器的核心问题，比较了26种实现方案，统一了安全定义，并提供了选择协议的实际指南。


<details>
  <summary>Details</summary>
Motivation: 安全洗牌器在隐私数据聚合和差分隐私中具有重要作用，但现有研究常将其视为黑盒，忽略了实际漏洞和性能权衡。

Method: 通过识别、分类和比较26种安全协议，统一安全定义，并提供选择指南。

Result: 提出了一套一致的安全属性，总结了依赖安全洗牌器的隐私保护技术，并展望了未来研究方向。

Conclusion: 安全洗牌器的选择需综合考虑安全性和性能，未来研究应进一步优化协议设计。

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


### [137] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: 论文提出了一种名为SafePTR的无训练防御框架，通过选择性修剪有害令牌来提升多模态大语言模型（MLLMs）的安全性，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法未能揭示多模态漏洞的根源，且存在过度防御和训练开销大的问题。

Method: SafePTR框架通过分析有害多模态令牌的来源和作用机制，选择性修剪早期中层的有害令牌，并在后续层恢复良性特征。

Result: 实验表明，SafePTR在三个MLLMs和五个基准测试中显著提升了安全性，且不影响模型效用。

Conclusion: SafePTR是一种高效且无需训练的安全防御方法，能够有效抵御多模态越狱攻击。

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [138] [Cybersecurity Issues in Local Energy Markets](https://arxiv.org/abs/2507.01536)
*Al Hussein Dabashi,Sajjad Maleki,Biswarup Mukherjee,Gregory Epiphaniou,Carsten Maple,Charalambos Konstantinou,Subhash Lakshminarayana*

Main category: cs.CR

TL;DR: 论文分析了本地能源市场（LEMs）面临的网络安全威胁，提出了漏洞影响评估及防护建议。


<details>
  <summary>Details</summary>
Motivation: LEMs依赖智能电网通信标准和易受攻击的IoT设备，存在被利用操纵市场、侵犯隐私和破坏电网稳定的风险。

Method: 通过映射LEM通信流至现有标准，识别关键漏洞并模拟攻击场景，评估其影响。

Result: 研究发现攻击者可扭曲定价和需求模式。

Conclusion: 提出了针对研究者、开发者、政策制定者和LEM利益相关者的安全部署建议。

Abstract: Local Energy Markets (LEMs), though pivotal to the energy transition, face
growing cybersecurity threats due to their reliance on smart grid communication
standards and vulnerable Internet-of-Things (IoT)-enabled devices. This is a
critical issue because such vulnerabilities can be exploited to manipulate
market operations, compromise participants' privacy, and destabilize power
distribution networks. This work maps LEM communication flows to existing
standards, highlights potential impacts of key identified vulnerabilities, and
simulates cyberattack scenarios on a privacy-preserving LEM model to assess
their impacts. Findings reveal how attackers could distort pricing and demand
patterns. We finally present recommendations for researchers, industry
developers, policymakers, and LEM stakeholders to secure future LEM
deployments.

</details>


### [139] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: 研究探讨了标签不平衡对网络入侵警报分类的影响，发现其影响分类性能和解释性，建议通过调整检测规则减少不平衡。


<details>
  <summary>Details</summary>
Motivation: 自动化方法在SOC中用于警报分类和事件升级，但需应对数据不平衡和解释性问题。

Method: 使用DeepCASE评估标签不平衡对警报分类的影响。

Result: 标签不平衡影响分类性能和解释性，调整检测规则可减少不平衡。

Conclusion: 传统方法改进输入数据质量有助于提升自动化效果。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


### [140] [EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity Leakage in Blockchain](https://arxiv.org/abs/2507.01635)
*Chenyu Li,Xueping Liang,Xiaorui Gong,Xiu Zhang*

Main category: cs.CR

TL;DR: 论文揭示了以太坊发现协议（Discv4/Discv5）中公钥重用的广泛问题，导致用户隐私泄露。通过EGNInfoLeaker系统和图算法，研究者成功关联用户身份并生成详细档案。


<details>
  <summary>Details</summary>
Motivation: 尽管以太坊协议设计有强大的加密保护，但实际部署中用户行为（如公钥重用）破坏了隐私保障，亟需研究其影响。

Method: 设计EGNInfoLeaker系统，分析300个网络快照，识别公钥重用，并通过图算法关联用户身份。

Result: 发现83名用户控制483个节点，通过IP关联实现去匿名化，生成详细用户档案（如User27的密钥、IP、位置等）。

Conclusion: 协议安全不仅依赖设计，还需用户严格遵守。EGNInfoLeaker为提升去中心化网络隐私保护提供了基础。

Abstract: While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust
cryptographic designs to protect user privacy, real-world deployment reveals
critical vulnerabilities when users deviate from security guidelines. In this
paper, we design a system called EGNInfoLeaker. Our study is the first work
that uncovers widespread public key reuse across Ethereum's peer-to-peer
networks - a practice that fundamentally undermines the protocol's privacy
guarantees. Through systematic analysis of 300 real-world network snapshots, we
identify 83 users controlling 483 service nodes via public key reuse, enabling
precise de-anonymization through IP correlation. Using evidence collected by
EGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to
network entities and generates comprehensive user profiles. For User27, it
exposes the public key, IP, network ID, location (country/region/city), and
ISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic
misuse transforms theoretical anonymity into practical identity leakage,
exposing users to surveillance and targeted attacks. These findings establish
that protocol security depends not only on sound design but also on strict user
compliance. Going forward, our detection framework provides a foundation for
enhancing real-world privacy preservation in decentralized networks.

</details>


### [141] [Graph Representation-based Model Poisoning on Federated LLMs in CyberEdge Networks](https://arxiv.org/abs/2507.01694)
*Hanlin Cai,Haofan Dong,Houtianfu Wang,Kai Li,Ozgur B. Akan*

Main category: cs.CR

TL;DR: 本文探讨了联邦大语言模型（FedLLMs）在数据隐私保护下的生成能力，但指出其易受模型投毒攻击。文章回顾了现有防御机制的局限性，并提出了一种新型攻击方法（GRMP），最后提出了提升鲁棒性的研究方向。


<details>
  <summary>Details</summary>
Motivation: FedLLMs在保护数据隐私的同时提供强大的生成能力，但其易受模型投毒攻击，尤其是在非独立同分布（non-IID）文本数据下，现有防御机制存在明显不足。

Method: 文章首先回顾了模型投毒技术和现有防御机制，随后提出了一种新型攻击方法（GRMP），该方法利用诚实客户端梯度的高阶相关性合成恶意更新。

Result: GRMP攻击能有效规避高级防御机制，导致显著的准确率损失和性能下降。

Conclusion: 文章强调需要开发图感知的安全聚合方法、FedLLMs特定的脆弱性指标和评估框架，以增强未来联邦语言模型的鲁棒性。

Abstract: Federated large language models (FedLLMs) provide powerful generative
capabilities in CyberEdge networks while protecting data privacy. However,
FedLLMs remains highly vulnerable to model poisoning attacks. This article
first reviews recent model poisoning techniques and existing defense mechanisms
for FedLLMs, highlighting critical limitations, particularly under non-IID text
distributions. In particular, current defenses primarily utilize distance-based
outlier detection or norm constraints, operating under the assumption that
adversarial updates significantly diverge from benign statistics. This
assumption can fail when facing adaptive attackers targeting billionparameter
LLMs. Next, this article investigates emerging Graph Representation-Based Model
Poisoning (GRMP), a novel attack paradigm that leverages higher-order
correlations among honest client gradients to synthesize malicious updates
indistinguishable from legitimate model updates. GRMP can effectively evade
advanced defenses, resulting in substantial accuracy loss and performance
degradation. Moreover, this article outlines a research roadmap emphasizing the
importance of graph-aware secure aggregation methods, FedLLMs-specific
vulnerability metrics, and evaluation frameworks to strengthen the robustness
of future federated language model deployments.

</details>


### [142] [Towards Better Attribute Inference Vulnerability Measures](https://arxiv.org/abs/2507.01710)
*Paul Francis,David Wagner*

Main category: cs.CR

TL;DR: 本文提出了一种结合精确率和召回率的属性推断度量方法，改进了现有方法仅关注精确率的局限性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有属性推断度量方法仅关注精确率，忽略了召回率，导致攻击风险被低估。本文旨在设计一种更全面的度量方法。

Method: 设计并实现了一种结合精确率和召回率的属性推断度量方法，并改进了基线推断的计算方式。

Result: 实验表明，在25%以上的攻击案例中，新方法能正确识别风险，而现有方法则错误标记为安全。

Conclusion: 结合精确率和召回率的度量方法能更准确地评估匿名化数据的攻击风险。

Abstract: The purpose of anonymizing structured data is to protect the privacy of
individuals in the data while retaining the statistical properties of the data.
An important class of attack on anonymized data is attribute inference, where
an attacker infers the value of an unknown attribute of a target individual
given knowledge of one or more known attributes. A major limitation of recent
attribute inference measures is that they do not take recall into account, only
precision. It is often the case that attacks target only a fraction of
individuals, for instance data outliers. Incorporating recall, however,
substantially complicates the measure, because one must determine how to
combine recall and precision in a composite measure for both the attack and
baseline. This paper presents the design and implementation of an attribute
inference measure that incorporates both precision and recall. Our design also
improves on how the baseline attribute inference is computed. In experiments
using a generic best row match attack on moderately-anonymized microdata, we
show that in over 25\% of the attacks, our approach correctly labeled the
attack to be at risk while the prior approach incorrectly labeled the attack to
be safe.

</details>


### [143] [Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range](https://arxiv.org/abs/2507.01768)
*Anis Yusof,Yuancheng Liu,Niklaus Kang,Choon Meng Seah,Zhenkai Liang,Ee-Chien Chang*

Main category: cs.CR

TL;DR: 论文通过模拟两种ICS网络攻击，生成了反映当前威胁环境的数据集，以增强安全系统和分析师应对ICS网络威胁的能力。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统（ICS）网络攻击频发，尤其是在OT系统与IT系统融合的背景下，需要更有效的安全措施和事件响应。

Method: 通过模拟两种ICS网络攻击，设计攻击场景，结合历史ICS事件中的攻击模式，生成数据集。

Result: 生成了包含关键指标的数据集，可作为网络攻击分析的重要资源。

Conclusion: 该数据集有助于提升安全系统和分析师应对当前ICS网络威胁的能力。

Abstract: The prevalence of cyberattacks on Industrial Control Systems (ICS) has
highlighted the necessity for robust security measures and incident response to
protect critical infrastructure. This is prominent when Operational Technology
(OT) systems undergo digital transformation by integrating with Information
Technology (IT) systems to enhance operational efficiency, adaptability, and
safety. To support analysts in staying abreast of emerging attack patterns,
there is a need for ICS datasets that reflect indicators representative of
contemporary cyber threats. To address this, we conduct two ICS cyberattack
simulations to showcase the impact of trending ICS cyberattacks on a railway
cyber range that resembles the railway infrastructure. The attack scenario is
designed to blend trending attack trends with attack patterns observed from
historical ICS incidents. The resulting evidence is collected as datasets,
serving as an essential resource for cyberattack analysis. This captures key
indicators that are relevant to the current threat landscape, augmenting the
effectiveness of security systems and analysts to protect against ICS cyber
threats.

</details>


### [144] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 论文提出了一种隐私保护平台，帮助中小型制造商安全共享数据，以开发创新工具解决实际问题，并以食品晶体质量控制为例展示了工具的实际应用。


<details>
  <summary>Details</summary>
Motivation: 中小型制造商因竞争和隐私问题不愿共享专有数据，但需要创新工具解决实际问题。

Method: 开发隐私保护平台，制造商通过安全方法共享数据，研究人员开发工具后回传平台供其他用户使用。

Result: 开发了自动分析食品晶体大小和数量的工具，提高了效率和准确性，并通过隐私保护平台部署。

Conclusion: 隐私保护平台成功解决了数据共享问题，未来可扩展更多应用场景。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ提出了一种少样本启发的生成零样本学习框架，通过动态调整属性评分和原型估计，减少对大规模特征合成的依赖，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统生成零样本学习方法需要大量计算资源和合成数据，违背了零样本学习的初衷。FSIGenZ旨在通过动态属性评分和原型估计减少对合成数据的依赖。

Method: FSIGenZ引入模型特定属性评分（MSAS）动态调整属性，估计组级原型作为未见类的代表特征，并采用双用途语义正则化（DPSR）训练语义感知对比分类器（SCC）。

Result: 在SUN、AwA2和CUB基准测试中，FSIGenZ使用更少的合成特征实现了竞争性性能。

Conclusion: FSIGenZ通过动态属性评分和原型估计，有效减少了零样本学习对大规模特征合成的依赖，同时保持了高性能。

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [146] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: DBellQuant是一种创新的后训练量化框架，通过双钟形分布转换减少量化误差，实现1位权重压缩和6位激活量化，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在部署中面临的计算和内存挑战，尤其是量化误差和不友好的权重分布问题。

Method: 采用可学习的双钟形转换（LTDB）算法，将单钟形权重分布转换为双钟形以减少二值化误差，并通过逆变换平滑激活值。

Result: 在Wikitext2数据集上，DBellQuant在LLaMA2-13B上实现了14.39的困惑度（6位激活量化），显著优于BiLLM的21.35（无激活量化）。

Conclusion: DBellQuant通过创新的量化方法，为LLMs的实际部署提供了高效的压缩解决方案，性能表现优异。

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [147] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: 论文研究了自监督学习中非对比方法的优化和动态系统视角，证明停止梯度和指数移动平均能避免表示崩溃，并在线性情况下验证其稳定性。


<details>
  <summary>Details</summary>
Motivation: 探讨自监督学习中避免表示崩溃的常用方法（停止梯度和指数移动平均）的理论基础，分析其优化和动态系统特性。

Method: 从优化和动态系统双视角分析停止梯度和指数移动平均的作用，证明其在线性情况下的稳定性。

Result: 停止梯度和指数移动平均虽不优化原始目标函数，但能避免崩溃；线性情况下，无这些方法会导致崩溃，而有这些方法则稳定。

Conclusion: 停止梯度和指数移动平均是避免表示崩溃的有效方法，其动态系统极限点是稳定的，不会退化到平凡解。

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [148] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: 提出PathCoT方法，结合病理学专家知识和自评估，提升多模态大语言模型在病理视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在病理视觉推理任务中表现不佳，缺乏领域知识且推理步骤易出错。

Method: PathCoT整合病理专家知识到推理过程，并引入自评估以减少答案偏差。

Result: 在PathMMU数据集上验证了方法的有效性。

Conclusion: PathCoT通过专家知识和自评估显著提升了病理视觉推理的准确性。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [149] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: 研究利用四种机器学习算法（MLP、随机森林、线性回归、支持向量机）重建甲烷燃料燃烧模拟中的FGM库，最终选择MLP方法并通过超参数调优达到99.81%的准确率。


<details>
  <summary>Details</summary>
Motivation: FGM在燃烧模型中精度高但内存需求大，需为特定燃料开发库。本研究旨在利用机器学习优化FGM库的生成。

Method: 采用四种机器学习算法（MLP、随机森林、线性回归、支持向量机）重建FGM库，通过超参数调优优化MLP模型。

Result: MLP方法表现最佳，优化后模型（四隐藏层，神经元数10-25）准确率达99.81%，误差率2.30%。

Conclusion: MLP是重建FGM库的最优方法，超参数调优显著提升性能，为甲烷燃烧模拟提供高效解决方案。

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [150] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 本文介绍了将基于PyTorch的几何学习框架移植到Gaudi-v2 HPU的经验，提供了核心工具和教程，降低了非CUDA硬件上的研究门槛。


<details>
  <summary>Details</summary>
Motivation: 尽管Nvidia的CUDA GPU主导硬件市场，但新兴加速器如Intel的Gaudi HPU在性能和能效上具有竞争力。然而，使用非CUDA硬件需要大量工程努力和软件适配。

Method: 作者开发了一套核心工具，恢复了Gaudi-v2 HPU上的基本操作（如scatter、稀疏索引、k近邻），并提供了16个教程和11个实际案例，分析了失败原因和解决方案。

Result: 所有经验被整合到一个公开的GitHub仓库中，为研究者在非CUDA硬件上实验几何学习算法提供了基础。

Conclusion: 这项工作降低了在非CUDA硬件上使用几何学习的门槛，并为未来的优化和跨平台移植奠定了基础。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [151] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的多视图动态决策框架，用于多组学数据分类，旨在降低测试成本的同时保持高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 多组学技术成本高且可能导致资源浪费，需一种方法在减少冗余测试的同时保持诊断性能。

Method: 在单组学层面，通过改进神经网络激活函数生成Dirichlet分布参数，量化分类结果的置信度和不确定性；在多组学层面，基于Dempster-Shafer理论融合异构模态，动态决策机制逐步引入数据。

Result: 在四个基准数据集上评估，50%以上病例仅需单组学数据即可准确分类，显著减少冗余测试，同时保持与全组学模型相当的诊断性能。

Conclusion: 该方法有效平衡了诊断准确性和成本，为多组学数据分类提供了实用解决方案。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [152] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: 该研究提出了一种数据驱动的方法，利用历史数据预测2025年利比亚班加西的电力负荷、发电量和缺口，LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 班加西电力供应不稳定，基础设施有限，准确的电力预测对电网稳定和能源规划至关重要。

Method: 使用ARIMA、季节性ARIMA、动态回归ARIMA、指数平滑、XGBoost和LSTM等多种时间序列模型，并对数据进行了缺失值填补、异常值平滑和对数转换。

Result: LSTM模型在预测非平稳和季节性模式方面表现最优，整合了温度和湿度等外生因素。

Conclusion: 优化的LSTM框架为数据稀缺且不稳定的地区提供了实用的电力预测工具，有助于政策制定者和电网运营商进行资源规划。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [153] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: 该研究通过混合GNN和LLM架构优化推荐系统，结合硬件加速和参数高效调优，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在线服务对高效实时推荐系统的需求推动了研究，旨在解决混合GNN-LLM模型的计算瓶颈。

Method: 采用混合GNN-LLM架构，结合量化、LoRA、蒸馏等优化策略，以及FPGA和DeepSpeed硬件加速。

Result: 最优配置（Hybrid + FPGA + DeepSpeed）在NDCG@10上提升13.6%，延迟40-60ms；LoRA减少训练时间66%。

Conclusion: 硬件-软件协同设计和参数高效调优使混合模型优于独立GNN或LLM方法，推荐FPGA和LoRA用于实时部署。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [154] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FSTA的分解技术，通过保留稳定解段并聚合节点来加速迭代求解器，同时引入L2Seg神经网络框架智能识别稳定与不稳定部分。实验表明L2Seg可将求解器速度提升7倍。


<details>
  <summary>Details</summary>
Motivation: 迭代搜索启发式方法在解决VRP问题时存在大量冗余计算，因为解的大部分在迭代中保持稳定。

Method: 提出FSTA分解技术保留稳定解段并聚合节点，同时开发L2Seg神经网络框架（包括非自回归、自回归及其协同变体）智能识别稳定部分。

Result: 在CVRP和VRPTW上的实验显示，L2Seg可将求解器速度提升高达7倍，且协同变体表现最佳。

Conclusion: L2Seg是一种灵活的框架，兼容传统、基于学习和混合求解器，适用于广泛的VRP问题。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [155] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: 使用PPO训练神经模糊控制器，相比DQN方法表现更稳定且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 改进基于DQN的神经模糊控制器训练方法，利用PPO的稳定性提升性能。

Method: 采用PPO作为策略优化方法，替换原有的DQN框架，在CartPole-v1环境中进行实验。

Result: PPO训练的模糊控制器在CartPole-v1中达到500 +/- 0的平均回报，方差更小，收敛更快。

Conclusion: PPO为训练可解释的神经模糊控制器提供了有效途径。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [156] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: Clifford Neural Layers通过引入Clifford代数优化PDE建模，在CPU上实现2/3D卷积层和多向量激活层的快速推理。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络在PDE建模中的效率，特别是在CPU上的推理性能。

Method: 优化2/3D Clifford卷积层和多向量激活层的实现，专注于单核CPU性能。

Result: 在较大数据和网络规模下，比标准PyTorch实现快30%。

Conclusion: Clifford Neural Layers在PDE建模中具有显著性能优势，代码已开源。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [157] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: 提出了一种基于有向无环图（DAG）的快速模型分割算法，用于优化AI模型的分布式训练，降低计算复杂性和训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决复杂AI模型在分布式训练中计算复杂度高的问题，优化模型分割以提高效率。

Method: 将AI模型表示为DAG，并重新定义模型分割问题为最小s-t割搜索问题，提出基于DAG的快速分割算法和块状分割算法。

Result: 算法能在毫秒级找到最优分割，动态边缘网络中训练延迟降低24.62%-38.95%。

Conclusion: 提出的算法在理论和实验中均表现出高效性和优越性，适用于复杂AI模型的分布式训练。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [158] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: 提出了一种动态调整神经网络架构的新方法，结合蒙特卡洛树搜索优化模型结构，适用于多变量时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 传统固定架构的神经网络无法动态优化结构，限制了模型性能。

Method: 通过蒙特卡洛树搜索模拟网络行为，动态调整模型架构（增减层或节点）。

Result: 在多变量时间序列分类任务中表现优异，模型具有强适应性和鲁棒性。

Conclusion: 动态架构优化方法显著提升了模型性能，尤其在时间序列任务中效果突出。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [159] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer架构的心脏感知基础模型（CSFM），通过生成式掩码预训练策略从多模态数据中学习统一表示，显著提升了心脏信号分析的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法依赖同质数据集和静态模型，限制了其在多样化临床环境中的鲁棒性和泛化能力。

Method: 利用Transformer架构和生成式掩码预训练策略，从多模态数据（包括心电图和文本报告）中学习统一表示。

Result: CSFM在多种心脏感知任务中表现优于传统方法，支持跨模态和跨配置的迁移学习。

Conclusion: CSFM是一种多功能、可扩展的心脏监测解决方案，具有广泛的应用潜力。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [160] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: 论文提出了一种变分数字孪生（VDT）框架，通过贝叶斯输出层和高效更新算法，解决了现有数字孪生在实时性、模型不确定性和信息交换方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生技术缺乏实时性、模型不确定性和信息交换的明确框架，限制了其在实际能源系统中的应用。

Method: 提出VDT框架，结合标准神经网络架构和贝叶斯输出层，并开发了一种高效的更新算法。

Result: 在四个能源领域问题中验证了VDT的有效性，包括热通量预测、可再生能源发电预测、核反应堆瞬态冷却和锂离子电池建模，均表现出高性能和鲁棒性。

Conclusion: VDT框架通过轻量级贝叶斯增强和高效更新，将传统模型转化为具有不确定性感知、数据高效和计算可行的数字孪生，为能源系统提供了可靠模型。

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [161] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: 石油行业中的不良事件可能导致经济损失、环境事故和人员伤亡。基于人工智能和机器学习的早期检测解决方案在多个行业中被证明有效。2019年，Petrobras发布了首个公开的3W数据集，用于不良事件检测，并持续更新。本文描述了最新版本的3W数据集，旨在支持研究和开发更强大的检测方法。


<details>
  <summary>Details</summary>
Motivation: 石油行业中的不良事件具有严重的经济和环境后果，但缺乏公开数据集。为解决这一问题，Petrobras开发并公开了3W数据集，以促进研究和开发早期检测技术。

Method: Petrobras开发并公开了3W数据集，这是一个由专家标记的多变量时间序列数据集，并持续更新和改进。

Result: 3W数据集已成为该领域的基础参考，支持了许多研究工作。最新版本包含结构修改和额外标记数据。

Conclusion: 3W数据集的公开和更新为开发更强大的不良事件检测方法提供了支持，有助于减少石油行业中的风险和损失。

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [162] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 提出了一种两阶段训练框架，用于高效去除社交媒体文本中的毒性，同时保留语义，并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上毒性内容的广泛传播对在线环境和公共讨论构成威胁，现有方法在去毒性能、语义保留和数据效率方面存在不足。

Method: 采用两阶段训练框架：先在高质量标注数据上进行监督微调，再利用未标注数据和奖励模型通过Group Relative Policy Optimization训练LLM。

Result: 实验表明，该方法有效平衡了去毒性能、语义保留和泛化能力，减少了对标注数据的依赖。

Conclusion: 该方法在去毒任务中表现优异，具有更高的数据效率和泛化能力。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [163] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 研究发现，在差分隐私训练中，简单的scale-then-privatize技术优于其他方法，且无需追求梯度的无偏二阶矩估计。


<details>
  <summary>Details</summary>
Motivation: 差分隐私训练中的球形噪声降低了自适应优化器的性能，现有研究结论可能不适用于实际模型训练。

Method: 调查多种改进方法，进行理论分析和实证比较。

Result: scale-then-privatize技术表现最佳，且噪声添加更符合实际需求。

Conclusion: 追求无偏二阶矩估计的直觉是错误的，scale-then-privatize是更优选择。

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [164] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: 提出了一种基于密集Hopfield网络的新型能量函数，通过高阶交互实现指数存储容量，并引入时间核以支持长序列记忆的高效检索。


<details>
  <summary>Details</summary>
Motivation: 解决长序列任务中Transformer的局限性，如长上下文建模和时间序列数据的长期依赖问题。

Method: 构建时间核$K(m, k)$以捕捉时序依赖，扩展密集Hopfield网络的存储能力。

Result: 成功应用于电影帧的存储与顺序检索，展示了在高维空间中的有效性。

Conclusion: 该模型为长序列任务提供了新思路，适用于自然语言处理、预测等领域。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [165] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: FlashDP是一种创新的缓存友好型DP-SGD方法，通过单次融合计算梯度，显著减少内存需求和冗余计算，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，训练数据的隐私保护成为关键问题。现有DP-SGD方法在内存和计算效率上存在不足。

Method: FlashDP采用缓存友好的逐层DP-SGD，将操作合并为单一任务，仅需一次融合计算梯度，减少内存移动和冗余计算。

Result: FlashDP减少50%内存移动和20%冗余计算，在Llama-13B预训练中达到非DP方法90%的吞吐量，且准确性不变。

Conclusion: FlashDP为高效且隐私保护的LLMs训练提供了重要进展，代码已开源。

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [166] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: 提出了一种基于元素组成和XRD的多模态框架，无需晶体结构输入，通过自监督预训练策略提升材料发现效率。


<details>
  <summary>Details</summary>
Motivation: 解决现实中原子结构难以获取的问题，利用更易得的元素组成和XRD数据推动材料发现。

Method: 采用多模态框架，结合元素组成和XRD数据，使用掩码XRD建模和对比对齐作为自监督预训练策略。

Result: 预训练加速收敛（最高4.2倍），提升准确性和表征质量，多模态性能随数据规模增长优于单模态基线。

Conclusion: 为材料科学提供了一种无需结构输入、基于实验数据的通用模型路径。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [167] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: 论文探讨了在低成本平台上实时部署汽车以太网入侵检测系统（IDS）的快速神经网络推理技术。


<details>
  <summary>Details</summary>
Motivation: 现代车辆日益互联，汽车以太网技术面临流注入攻击等安全威胁，而传统深度学习IDS需要昂贵硬件支持实时运行。

Method: 采用蒸馏和剪枝等快速神经网络推理技术，优化IDS模型以适配低成本平台（如Raspberry Pi 4）。

Result: 在Raspberry Pi 4上实现了727微秒的入侵检测时间，AUCROC值达0.9890。

Conclusion: 快速神经网络推理技术能有效在低成本平台上实现高性能实时入侵检测。

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [168] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: 研究分析了洪水对路面粗糙度的影响，利用20年数据和XAI技术发现洪水加速路面恶化，建议采取防洪措施。


<details>
  <summary>Details</summary>
Motivation: 洪水对路面基础设施造成严重损害，研究旨在量化洪水对路面粗糙度的影响。

Method: 结合TxDOT的PMIS数据库和洪水事件数据，使用统计分析和XAI技术（如SHAP和LIME）评估影响。

Result: 洪水区域的路面粗糙度增加更快，表明洪水加速了路面恶化。

Conclusion: 建议采取防洪措施（如改进排水系统和使用抗洪材料）以提高路面抗灾能力。

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [169] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: PAE MobiLLM是一种隐私保护且高效的LLM微调方法，通过服务器辅助的侧调优实现移动设备部署，减少通信负担和数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备资源有限与LLM微调需求之间的矛盾，同时避免现有服务器辅助方法的高通信负担和数据隐私问题。

Method: 采用服务器辅助的侧调优，集成激活缓存和一令牌激活快捷方式，减少通信成本；引入加法适配器侧网络设计，保护数据隐私。

Result: PAE MobiLLM实现了高效的LLM微调，减少了通信负担，同时保护了数据隐私。

Conclusion: PAE MobiLLM为移动设备上的LLM微调提供了一种高效且隐私保护的解决方案。

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [170] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: 提出了一种基于深度卷积神经网络的智能优化系统，用于网格生成与优化，核心是Loop2Net生成器和损失函数。


<details>
  <summary>Details</summary>
Motivation: 通过智能优化系统提高网格生成的质量和效率。

Method: 使用Loop2Net生成器和两种关键损失函数进行网格预测和优化训练。

Result: 模型通过惩罚机制实现了网格生成的目标。

Conclusion: 该系统有效提升了网格生成的性能。

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [171] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出双学习假说，揭示大语言模型在上下文学习中同时学习任务相关和潜在后门概念，提出ICLShield防御机制，动态调整概念偏好比例，有效抵御后门攻击。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）因其适应性和无参数特性在大语言模型中表现优异，但也易受后门攻击。研究旨在揭示ICL的脆弱性并提出防御方案。

Method: 提出双学习假说，分析ICL后门效应的上界，设计ICLShield机制，通过置信度和相似度动态调整概念偏好比例。

Result: 实验表明ICLShield在多种大语言模型和任务中表现优异，平均防御效果提升26.02%，对闭源模型（如GPT-4）也有效。

Conclusion: 双学习假说揭示了ICL的脆弱性根源，ICLShield通过动态调整概念偏好比例，显著提升了防御效果，具有广泛适用性。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [172] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 本文优化了一种最先进的基础模型，用于预测高性能机器学习服务中的偶发或尖峰生产中断，并与经典随机模型进行比较。


<details>
  <summary>Details</summary>
Motivation: 基础模型尚未用于预测罕见、尖峰事件，这些事件是极端事件的特殊情况，具有挑战性。

Method: 优化基础模型，并与经典随机模型（如移动平均和自回归）进行预测误差比较。

Result: 分析揭示了基础模型与随机模型在追踪目标数据关键模式上的表现差异，最优参数模型对特定根因的年中断统计估计误差小于6%。

Conclusion: 基础模型在预测偶发或尖峰事件中表现优于经典随机模型，为实际应用提供了更准确的预测工具。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [173] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习（FL）的动态室内定位方法，解决了传统集中式数据收集的隐私、带宽和服务器可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 传统室内定位技术存在误差大和隐私问题，机器学习方法虽能捕捉环境变化，但需要集中数据，引发隐私和效率问题。

Method: 采用联邦学习（FL）结合深度神经网络（DNN）模型，实现动态室内定位。

Result: 实验表明，FL性能接近集中式模型（CL），同时保障数据隐私、带宽效率和服务器可靠性。

Conclusion: FL为隐私增强的室内定位提供了可行方案，推动了安全高效室内定位系统的发展。

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [174] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: 利用IMU数据和可解释AI方法预测帕金森病的冻结步态（FOG），集成学习模型表现最佳，准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 早期检测和预测帕金森病中的FOG症状，以改善患者生活质量。

Method: 使用CatBoost、XGBoost和Extra Trees分类器，结合Stacking Ensemble模型和联邦学习框架。

Result: Stacking Ensemble模型表现最优，准确率达99%，SHAP分析显示时间是关键因素。

Conclusion: 提出的框架结合联邦学习和可解释AI，为FOG预测提供了高效且隐私保护的解决方案。

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [175] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: 本文提出了一种新的3D编码模块，通过旋转采样和SO(3)旋转群计算实现近似旋转不变性，并通过后对齐策略实现严格不变性，显著提升了分子属性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 传统图表示难以有效编码分子的3D空间结构，现有方法在不变性和计算成本上存在不足。

Method: 提出了一种基于旋转采样的3D编码模块，结合SO(3)旋转群计算和后对齐策略，实现近似和严格旋转不变性。

Result: 在QM9和C10数据集上表现出更高的预测准确性、鲁棒性和泛化性能，同时保持低计算复杂度。

Conclusion: 该方法为药物发现和材料设计中的3D分子信息处理提供了高效且有效的解决方案。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [176] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER是一种基于进化算法的黑盒优化方法，用于大型语言模型（LLM）的后训练，通过隐式压缩训练数据引入信息瓶颈，提供理论保证和实际性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统梯度优化依赖大量标注数据，存在隐私、安全和过拟合问题；黑盒优化在数据受限或对抗风险高时是替代方案，但面临可扩展性和计算成本挑战。

Method: 提出BBoxER方法，通过信息流可追踪性提供理论边界，并在预训练LLM上实现轻量级模块化增强。

Result: 实验表明，BBoxER能提升性能并在推理数据集上泛化良好。

Conclusion: BBoxER是梯度优化的有效补充，适用于隐私敏感或受限环境。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [177] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 论文介绍了yProv4ML库，用于收集符合W3C PROV和ProvML标准的JSON格式溯源数据，以优化大规模AI模型的训练效率、执行时间、准确性和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模AI模型需求的增长，如何在计算效率、执行时间、准确性和能耗之间实现平衡成为关键挑战。溯源数据成为解决这一问题的核心工具。

Method: 提出yProv4ML库，支持灵活扩展并通过插件集成其他数据收集工具，与yProv框架完全集成。

Result: yProv4ML库能够帮助研究人员和工程师分析资源使用模式、识别低效问题，并确保AI开发工作流的可重复性和可追溯性。

Conclusion: yProv4ML库为优化大规模AI模型的分布式资源利用和能效提供了有效工具。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [178] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: 提出了一种基于解码器的大型语言模型（LLM）用于检测电子控制单元（ECU）通信日志中的异常，解决了现有方法在专业领域中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在汽车通信系统等专业领域中效果有限，且缺乏针对ECU通信的LLM模型。

Method: 使用解码器LLM学习UDP通信日志，通过时间偏差检测异常，并引入熵正则化技术处理不一致的标注数据。

Result: 提出了一种新的解码器异常检测架构，能够处理不一致标注，并适应不同ECU通信场景。

Conclusion: 该方法通过生成能力减少了手动标注的高成本和错误率，提高了复杂通信环境中的检测准确性。

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [179] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 本文提出yProv4ML框架，用于以PROV-JSON格式记录机器学习过程中的溯源信息，解决现有工具在透明度和数据格式上的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的发展缺乏透明度和严谨性，尤其是超参数和训练轮数的不确定性，导致模型优化困难。现有工具如MLFlow虽能自动化收集数据，但使用专有格式且忽视数据溯源。

Method: 提出yProv4ML框架，通过最小代码修改，以PROV-JSON格式捕获机器学习过程中的溯源信息。

Result: yProv4ML能够有效记录机器学习过程的溯源数据，提升透明度和可追溯性。

Conclusion: yProv4ML为机器学习开发提供了更透明和标准化的溯源记录方法，有助于解决现有工具的局限性。

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [180] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: 研究比较了三种AI模型（NLP、LLM、JEPA）在急诊分诊中的表现，发现LLM模型（URGENTIAPARSE）准确性最高，优于护士分诊。


<details>
  <summary>Details</summary>
Motivation: 急诊分诊中的错误（如过度或不足分诊）是持续挑战，AI的引入可能提升分诊效率和安全性。

Method: 回顾性分析7个月的患者数据，训练并验证三种AI模型，评估其与FRENCH标准的吻合度。

Result: LLM模型（URGENTIAPARSE）表现最佳，预测住院需求（GEMSA）和结构化数据处理上均优于其他模型。

Conclusion: AI（尤其是LLM）可提升急诊分诊效率，但需解决模型局限性和伦理问题。

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [181] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: 本文详细解释了Ziyin等人（2025）关于嵌入式深度线性网络模型（EDLN）的“完美”柏拉图表示假设（PRH）的证明。研究发现，使用SGD训练时，不同宽度和深度的EDLN会学习到相同的表示（仅相差旋转），且这一现象与渐进锐化有共同原因。


<details>
  <summary>Details</summary>
Motivation: 探讨SGD训练下EDLN模型的表示学习特性，揭示柏拉图表示与渐进锐化的共同原因，强调理解SGD不可逆性导致的“熵力”的重要性。

Method: 通过理论分析和证明，研究EDLN模型在SGD训练下的行为，特别是柏拉图表示的形成条件及其与渐进锐化的关联。

Result: 发现SGD训练会迫使EDLN模型学习到完美的柏拉图表示，且这一现象与渐进锐化有相同的成因。同时指出了六种可能破坏PRH的情况。

Conclusion: 研究揭示了SGD训练中“熵力”对表示学习的关键作用，并表明柏拉图表示与渐进锐化可能源于同一机制，为理解深度学习现象提供了新视角。

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [182] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: 提出了一种基于动态模式分解（DMD）和深度学习的神经算子，用于高效建模时空过程，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 科学计算与人工智能结合是研究热点，但需平衡轻量化和准确性。传统PDE求解方法计算资源消耗大。

Method: 结合DMD和深度学习，自动提取关键模式和系统动态，用于预测。

Result: 在热方程、拉普拉斯方程和Burgers方程中，与DeepONet和FNO相比，表现出高重构精度和低计算成本。

Conclusion: 该方法为高效求解PDE提供了一种新思路，兼具轻量化和准确性。

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [183] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种近似等变的网络（TDNs），通过低秩张量分解（如CP分解）替代计算昂贵的Clebsch-Gordan张量积，显著加速计算，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的SO(3)-等变网络在机器学习原子间势（MLIPs）中占主导，但其核心操作Clebsch-Gordan张量积计算成本高，亟需优化。

Method: 采用CP分解替代Clebsch-Gordan张量积，提出路径权重共享以减少参数数量，同时保持等变性。计算复杂度从O(L^6)降至O(L^4)。

Result: 在PubChemQCR、OC20和OC22数据集上，TDNs表现出与现有方法竞争的性能，同时显著加速计算。

Conclusion: TDNs是一种高效的近似等变网络，可作为现有网络中张量积的即插即用替代方案，显著提升计算效率。

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [184] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: 提出了一种名为Spectral Manifold Harmonization（SMH）的新方法，用于解决图结构数据中的不平衡回归问题，通过生成合成样本以关注目标分布中代表性不足的区域。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在科学领域中普遍存在，但现有研究在目标值范围不平衡的情况下表现不足，尤其是对科学价值较高的区域关注不够。

Method: SMH通过生成合成图样本，保持拓扑特性，同时聚焦于目标分布中代表性不足的区域。

Result: 实验结果表明，SMH在化学和药物发现基准数据集上显著提升了目标域范围的预测性能。

Conclusion: SMH为解决图结构数据中的不平衡回归问题提供了一种有效方法，特别适用于科学价值高的目标范围。

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [185] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: Diffusion Explorer是一个交互式工具，用于解释扩散模型的几何特性，用户可在浏览器中训练2D扩散模型并观察采样过程的动态。


<details>
  <summary>Details</summary>
Motivation: 现有解释扩散模型的资源要么需要高深的理论基础，要么过于关注神经网络架构，而忽略了其丰富的几何特性。

Method: 开发了Diffusion Explorer工具，利用交互式动画展示扩散模型的动态过程。

Result: Diffusion Explorer成功展示了扩散模型的几何特性和时间动态，且开源并提供在线演示。

Conclusion: Diffusion Explorer为理解和教学扩散模型的几何特性提供了直观且易用的工具。

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [186] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本文评估了大型脑波基础模型（LBMs）在脑机接口（BCI）任务中的表现，发现其性能提升有限且参数需求高，提出了通过LoRA技术优化参数效率的方法。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在脑波建模中的潜力，评估其在BCI任务中的实际效果和效率。

Method: 通过系统微调实验和LoRA技术，对比LBMs与传统深度架构的性能和参数需求。

Result: LBMs性能仅小幅提升（0.9%-1.2%），但参数需求显著增加；LoRA能有效减少参数且不降低性能。

Conclusion: 当前LBMs架构需优化，域特定开发策略是关键，LoRA技术为BCI应用提供了高效训练方案。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [187] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 论文提出了一种联合自编码调制器（JAM）框架，用于优化视觉和语言模型之间的表示对齐，通过多目标优化实现模态间的共享结构。


<details>
  <summary>Details</summary>
Motivation: 探索视觉和语言模型是否能够通过优化实现表示对齐，超越统计检测，直接优化对齐。

Method: 提出JAM框架，联合训练模态特定的自编码器，结合重构和跨模态目标，实现对齐。

Result: JAM框架在多种设计轴上表现优异，能够可靠地诱导对齐，即使模型是独立训练的。

Conclusion: JAM框架为将通用单模态模型转化为专用多模态模型提供了理论和实践路径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [188] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习在建模皮肤电导反应（SCR）事件中的应用，比较了量子支持向量机（QSVM）和量子神经网络（QNN）的性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算为复杂机器学习任务提供了新方法，特别是在智能交通系统中高维数据表示的需求下，研究如何利用量子机器学习建模SCR事件以反映行人压力。

Method: 开发了基于Pennylane的QSVM（使用八量子位ZZ特征映射）和QNN（使用树张量网络结构和八量子位ZZ特征映射），数据集包含SCR测量值和特征（如响应幅度和经过时间）。

Result: QSVM训练精度高但存在过拟合问题，测试精度仅为45%；QNN测试精度更高（55%），优于QSVM和经典模型。

Conclusion: QNN在分类任务中表现更优，为量子机器学习在复杂数据建模中的应用提供了潜力。

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [189] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型（LLM）训练的随机共轭次梯度方法，结合自适应采样，相比传统SGD方法，实现了更快的收敛速度和更好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度下降（SGD）在大规模应用中存在性能限制，因此需要一种更高效的方法来训练LLM。

Method: 提出随机共轭次梯度方法，结合自适应采样和AdamW-like算法调整步长，以解决LLM训练中的非凸性和非光滑性问题。

Result: 实验表明，该方法在速度和准确性上均优于传统SGD，且扩展性更强。

Conclusion: 该方法在保持一阶方法优势的同时，显著提升了LLM训练的效率和性能。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [190] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: 本文提出了PULSE协议，用于评估大型多模态模型（LMMs）在实际场景中的遗忘能力，重点关注预训练知识遗忘和长期可持续性评估。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs的遗忘基准仅关注单次遗忘操作，缺乏对预训练知识遗忘和连续遗忘请求的评估，因此需要更全面的框架。

Method: 引入PULSE协议，从预训练知识遗忘和长期可持续性两个维度评估现有遗忘方法。

Result: 研究发现，现有方法能有效遗忘微调知识，但对预训练知识效果不佳；单次批量遗忘方法在连续遗忘时性能显著下降。

Conclusion: PULSE协议为LMMs的遗忘提供了更现实的评估框架，揭示了现有方法的局限性。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [191] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: 提出了一种基于距离的聚合方法Dist-FedAvg，用于提升图联邦推荐系统中的个性化和聚合效率。


<details>
  <summary>Details</summary>
Motivation: 传统聚合方法忽视了用户嵌入的独特性和用户相似性在推荐效果中的关键作用，且无法适应动态用户交互。

Method: Dist-FedAvg通过为相似嵌入用户分配更高聚合权重，并保留锚用户的影响力，优化聚合过程。

Result: 在多个数据集上的实验表明，Dist-FedAvg在推荐准确性上优于基线方法。

Conclusion: Dist-FedAvg有效解决了传统聚合方法的局限性，同时与现有联邦学习框架无缝集成。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [192] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架（NHO）来解决高维随机控制问题，通过深度学习训练网络以满足PMP条件。


<details>
  <summary>Details</summary>
Motivation: 高维随机控制问题因维度灾难难以解决，传统动态规划方法效率低。

Method: 定义神经哈密顿算子（NHO），用神经网络参数化FBSDE系统，并通过训练网络满足PMP一致性条件。

Result: 证明了NHO在一般鞅驱动下的通用逼近能力，并分析了优化挑战。

Conclusion: NHO框架为高维随机控制问题提供了新的深度学习解决方案，并具有理论保证。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [193] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出了一种基于自适应困惑度感知强化学习（APARL）的框架，用于客户服务对话中的异常事件检测，显著提升了模型的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决商业数据复杂性和客户交互动态性带来的异常事件检测挑战，同时提升模型的跨领域泛化能力。

Method: 采用双环动态课程学习架构，利用大语言模型的高级推理能力，逐步聚焦更具挑战性的样本。

Result: 在食品配送对话任务中，F1分数平均提升17.19%，跨领域迁移测试平均提升9.59%。

Conclusion: APARL为工业部署提供了优越的异常检测解决方案，提升了运营效率和商业价值。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [194] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: 提出了一种基于小波域的生成模型WDM，用于将降水数据从10 km降尺度到1 km，显著提升了分辨率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有全球降水数据（如IMERG）分辨率较低（10 km），无法满足水文建模和极端天气分析的需求。

Method: WDM是一种条件扩散模型，直接在小波域中学习降水数据的复杂结构，通过关注高频小波系数生成高分辨率降水场。

Result: WDM实现了10倍空间超分辨率和9倍推理加速，生成结果更真实、细节更丰富，且伪影更少。

Conclusion: WDM为地球科学超分辨率问题提供了准确且高效的解决方案，有助于提升水文预报的可靠性。

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [195] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为DSAC-D的分布强化学习算法，通过引入扩散模型解决价值函数估计偏差和多模态策略表示问题，在控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法使用单模态分布（如高斯分布）建模价值分布，容易导致估计偏差和算法性能下降。

Method: 提出DSAC-D算法，结合策略熵和价值分布函数建立多模态分布策略迭代框架，并构建扩散价值网络以准确表征多峰分布。

Result: 在MuJoCo测试任务中，DSAC-D不仅学习到多模态策略，还在9个控制任务中达到SOTA性能，估计偏差显著抑制，总平均回报提升超10%。

Conclusion: DSAC-D能准确表征不同驾驶风格的多模态分布，扩散策略网络可表征多模态轨迹，验证了算法的有效性。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [196] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO是一种自引导过程奖励优化框架，通过理论推导和创新的奖励设计，显著提升了训练效率和模型性能，且无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决过程强化学习中计算开销大和缺乏统一理论框架的问题。

Method: 提出SPRO框架，包括从策略模型内推导过程奖励和定义累积过程奖励与掩码步骤优势（MSA）。

Result: SPRO在训练效率上比GRPO高3.4倍，测试准确率提升17.5%，同时减少响应长度1/3。

Conclusion: SPRO是一种高效且实用的过程强化学习方法，适用于工业实现。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [197] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: 提出了一种改进的替代模型，通过引入松弛变量将两步过程统一为一步，并在药物组合效应预测任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 受现有量子退火优化替代模型的启发，希望通过引入松弛变量提升模型性能。

Method: 在因子分解机及其Ising表示中引入松弛变量，统一优化过程，迭代更新松弛变量以捕捉高阶特征交互。

Result: 实验表明，松弛变量的引入显著提升了性能。

Conclusion: 该方法为利用量子优势构建高效替代模型提供了有前景的途径。

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [198] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 论文提出了一种名为Prefix-RFT的混合方法，结合了监督微调（SFT）和强化微调（RFT）的优势，在数学推理任务中表现优于单独使用SFT或RFT。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练技术（SFT和RFT）各有优缺点，SFT容易过度模仿演示数据，而RFT对初始策略敏感且可能学习到意外行为。

Method: 提出Prefix-RFT，结合演示和探索学习，通过数学推理任务验证其有效性。

Result: Prefix-RFT性能优于单独SFT和RFT，且对演示数据的质量和数量变化具有鲁棒性。

Conclusion: Prefix-RFT为LLM后训练提供了新视角，未来研究可探索结合演示和探索的统一范式。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [199] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: 论文介绍了一种结合线性回归式连续上下文学习（ICL）和离散关联召回的新玩具问题家族，研究了Transformer模型在此任务中的表现，发现其涉及两种机制，并验证了多机制现象在其他任务中的普遍性。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在结合连续和离散特征的玩具问题中的学习能力，特别是其在上下文标签下召回状态和预测后续状态的能力。

Method: 预训练Transformer模型于符号标记的随机线性确定性动态系统样本轨迹上，分析其召回和预测能力，并通过边缘剪枝和分布外实验验证机制。

Result: 模型需通过两种机制完成任务：基于离散标签的关联召回和基于上下文的贝叶斯预测，且两种机制的学习动态不同。

Conclusion: 多机制现象（表现为分离的相变）不仅限于玩具问题，在ICL翻译任务中也有类似表现。

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [200] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 本文提出了一种基于TVM编译器的工作流，用于高效地将AI工作负载映射到RISC-V向量单元，相比现有方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: RISC-V向量扩展（RVV）在AI工作负载加速中具有潜力，但缺乏高效的自动调优框架支持，限制了其实际部署。

Method: 通过将RVV扩展集成到TVM的MetaSchedule框架中，实现了对AI工作负载的自动调优，并在FPGA上实现了多种RISC-V SoC进行验证。

Result: 相比GCC的自动向量化功能和muRISCV-NN库，分别实现了46%和29%的延迟提升，且代码内存占用更小。在商用RISC-V SoC上，比LLVM快35%。

Conclusion: 该方法显著提升了RISC-V向量单元的效率，适合嵌入式设备，并已开源供社区扩展。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [201] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: 论文提出了一种名为BAR的新型RSS数据集，用于解决文化遗址中室内定位系统（IPS）开发的数据不足问题，并提供了基于邻近性和k-NN算法的分类基线。


<details>
  <summary>Details</summary>
Motivation: 文化遗址中IPS的实施面临环境限制和技术挑战，尤其是缺乏反映博物馆环境的公开RSS数据集。

Method: 在13个博物馆房间的90件艺术品前收集RSS数据，使用Android和iOS平台，并采用邻近性方法和k-NN算法进行分类。

Result: 提供了BAR数据集和分类基线，为博物馆环境中的IPS开发提供了数据支持和方法参考。

Conclusion: BAR数据集填补了博物馆环境RSS数据的空白，为未来研究提供了方向和基础。

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [202] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 提出了一种基于CPU的低秩适配器（LoRA）微调方法，适用于计算资源有限的用户，通过预训练适配器组合实现高效微调。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA依赖GPU训练的问题，为计算资源有限的用户提供可行的替代方案。

Method: 利用预训练适配器库，通过轻量级组合生成新的LoRA权重，无需梯度更新。

Result: 生成的适配器性能虽不及GPU训练版本，但优于基础模型，提供了一种实用的替代方案。

Conclusion: 该方法为资源受限用户提供了可行的LoRA微调方案，扩展了LoRA的应用范围。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [203] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: 论文重新审视了强化学习中奖励频率作为任务难度衡量标准的假设，揭示了当前方法在关键子目标无直接奖励时的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是发现并形式化当前策略学习方法在关键子目标无直接奖励时的结构性问题。

Method: 方法包括识别零激励动态，分析子目标完成与最终奖励的时间接近性对学习性能的影响。

Result: 结果表明，现有深度子目标算法无法有效利用零激励动态，学习性能高度依赖子目标与奖励的时间接近性。

Conclusion: 结论指出当前方法存在根本性局限，需要开发能够推断潜在任务结构而不依赖即时激励的机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [204] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1是一种反射生成模型，通过自监督过程奖励模型（SPRM）实现高效推理，性能媲美OpenAI o3，参数规模仅32B。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一接口整合策略模型和过程奖励模型，减少参数需求并提升推理效率。

Method: 采用共享主干网络和任务特定头，结合SPRM实现无额外标注的高效推理，支持可控思考长度的三种推理模式。

Result: 实验表明MetaStone-S1性能与OpenAI-o3-mini系列相当，参数规模显著减少。

Conclusion: MetaStone-S1为研究社区提供了高效且可扩展的模型，并开源了代码。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [205] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Main category: cs.LG

TL;DR: 本文系统分析了扩散模型中的损失函数，统一了多种目标目标，并通过理论和实证研究揭示了其性能差异及影响因素。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为强大的生成模型，其损失函数的选择是关键问题，但现有研究存在多种不同目标，缺乏统一理解。

Method: 通过理论分析将多种损失函数统一到变分下界目标框架下，并通过实证研究比较其性能差异。

Result: 研究发现不同目标在生成高质量样本和准确估计似然方面存在性能差异，并分析了影响因素。

Conclusion: 本研究为扩散模型的损失函数提供了统一理解，有助于未来更高效和针对性的模型设计。

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [206] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Chargax是一个基于JAX的电动车充电站仿真环境，显著提升RL代理训练效率，性能提升100x-1000x，支持多样化真实配置。


<details>
  <summary>Details</summary>
Motivation: 电网系统拥堵问题急需提升运营效率，传统RL方法因高样本复杂度和昂贵仿真成本效率低下。

Method: 开发了基于JAX的Chargax环境，用于电动车充电站的真实仿真，支持加速RL代理训练。

Result: 在多种真实数据场景下验证，性能提升100x-1000x，支持多样化配置。

Conclusion: Chargax为可持续能源挑战提供了高效的RL解决方案，具有实际应用潜力。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [207] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS是一种无需训练的方法，通过将潜在嵌入空间转换为视觉表示，利用视觉语言模型的空间和细粒度推理能力，实现多模态数据的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 解决专用模型缺乏灵活性和基础模型在非传统模态及长尾领域表现不佳的问题。

Method: 将潜在嵌入空间转换为视觉表示，利用视觉语言模型的空间和细粒度推理能力进行预测。

Result: 在视觉、音频、生物和表格领域表现优异，平均性能超过Gemini 16%，接近专用方法。

Conclusion: MARVIS提供了一种无需训练、保护隐私的多模态预测方法，具有广泛的应用潜力。

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [208] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 论文研究了神经网络最后一层权重重采样（“zapping”）在持续学习和少样本迁移学习中的作用，发现其能加速模型在新领域的恢复，并探讨了优化器选择对学习动态的影响。


<details>
  <summary>Details</summary>
Motivation: 探索zapping在持续学习和少样本迁移学习中的机制，以及优化器选择对学习动态的影响。

Method: 在卷积神经网络中，通过手写字符和自然图像的实验，分析zapping和优化器选择对学习与遗忘模式的影响。

Result: zapping能加速模型在新领域的恢复，优化器选择会影响任务间的协同/干扰模式。

Conclusion: zapping和优化器选择对持续学习的动态有显著影响，需综合考虑以提高模型性能。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [209] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文对Muon优化器进行了理论分析，证明了其四种变体的收敛性，并探讨了权重衰减对参数和梯度范数的影响，同时验证了临界批量大小的理论。


<details>
  <summary>Details</summary>
Motivation: 研究Muon优化器的理论性质，特别是其收敛性和权重衰减的作用，以提升优化器的性能。

Method: 通过理论分析推导Muon优化器的收敛性，并实验验证其临界批量大小和权重衰减的影响。

Result: 权重衰减能严格收紧参数和梯度范数的界限，并明确了权重衰减系数与学习率的关系。

Conclusion: Muon优化器在理论和实验中均表现出良好的性能，临界批量大小的推导为其实际应用提供了指导。

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [210] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: 提出一种高效的在线字典学习算法，用于基于核的稀疏表示，通过递归最小二乘法更新字典，实验表明其优于现有在线方法且接近批量训练模型的分类精度。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习框架下核稀疏表示的高效字典学习问题，提升计算效率和分类性能。

Method: 使用递归最小二乘法（RLS）递归更新字典，支持单样本或小批量处理，保持低计算复杂度。

Result: 在四个数据集上实验，性能优于现有在线方法，分类精度接近批量训练模型，且更高效。

Conclusion: 该方法在效率和性能上取得了平衡，适用于在线学习场景。

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [211] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: DDCL是一种基于ConvLSTM的新方法，用于自动生成DDR游戏谱面，相比之前的DDC方法，显著提高了生成准确性。


<details>
  <summary>Details</summary>
Motivation: 改进现有的DDR谱面自动生成方法（DDC），以提高生成准确性。

Method: 采用ConvLSTM架构，结合CNN和LSTM的优势，优化谱面生成。

Result: DDCL方法显著提高了谱面生成的准确性。

Conclusion: ConvLSTM架构在DDR谱面生成中表现优于之前的CNN-LSTM方法。

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [212] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: 论文提出了一种名为GradMetaNet的新型架构，专门设计用于处理梯度信息，基于三个原则：等变性设计、多数据点梯度集处理和高效梯度表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理梯度时未针对梯度处理设计专用架构，限制了其适用性。

Method: 基于等变性设计、多数据点梯度集处理和高效梯度表示（秩-1分解）的原则，构建GradMetaNet架构。

Result: GradMetaNet能够逼近自然梯度函数，并在MLP和Transformer上的多种梯度任务中表现优异。

Conclusion: GradMetaNet为梯度处理提供了一种高效且通用的解决方案，适用于多种任务。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [213] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow是一个异步流式强化学习框架，用于高效后训练，解决了传统RL框架的可扩展性和资源利用率问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL框架在可扩展性、数据流复杂性和资源利用率方面存在瓶颈，且与LLM训练或推理引擎紧密耦合，难以支持自定义引擎。

Method: 提出AsyncFlow，包括分布式数据存储与传输模块、生产者-消费者异步工作流，并与底层引擎解耦。

Result: 实验显示吞吐量平均提升1.59倍。

Conclusion: AsyncFlow为下一代RL训练系统设计提供了模块化、可定制化的解决方案。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [214] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: 论文提出了一种名为SODA的梯度算法，用于从大型语言模型（LLM）的输出中精确重构输入，解决了现有审计技术的补充问题。实验表明，SODA在恢复短输入方面表现优异，但对长输入的保护效果较好。


<details>
  <summary>Details</summary>
Motivation: 现有审计技术主要关注识别LLM的不当行为，而本文旨在解决如何从LLM输出中重构原始输入的补充问题，以支持事后分析和检测虚假报告。

Method: 将精确输入重构问题形式化为离散优化问题，并引入SODA算法，该算法基于梯度方法，在连续松弛的输入搜索空间中运行，结合周期性重启和参数衰减。

Result: 在33M到3B参数的LLM上，SODA显著优于现有方法，成功恢复了79.5%的短输入，但对15+ token的长输入保护效果较好。

Conclusion: SODA在短输入重构上表现优异，但长输入的保护措施可能已足够抵御恶意使用。标准部署实践可能提供了足够的保护。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [215] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: PERTINENCE是一种动态选择预训练模型的方法，通过输入特征复杂度分析，平衡准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型DNN模型虽然准确但资源消耗高，需要在不显著降低准确性的情况下减少对它们的依赖。

Method: 使用遗传算法训练输入调度器，动态选择最适合的模型处理输入。

Result: 在CIFAR-10、CIFAR-100和TinyImageNet上，PERTINENCE在减少36%操作的同时保持或提高准确性。

Conclusion: PERTINENCE提供了一种高效的方法，通过动态模型选择优化准确性与计算成本的权衡。

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [216] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出变分神经网络版本的图卷积网络，通过估计模型输出和注意力层的不确定性，提升模型解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 模型不确定性的估计可以提升图卷积网络的解释性和准确性，同时为关键应用提供结果验证支持。

Method: 提出空间和时空图卷积网络的变分神经网络版本，估计模型输出和层间注意力的不确定性。

Result: 在社交交易分析和基于骨架的人体动作识别任务中，模型准确性和不确定性估计均得到提升。

Conclusion: 变分图卷积网络通过不确定性估计，显著提升了模型的解释性和性能。

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [217] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: 提出了一种名为RelFCI的因果发现算法，用于处理具有潜在混杂因素的关系数据，填补了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法在处理关系数据和潜在混杂因素时存在局限性，需要一种更有效的方法。

Method: 基于FCI和RCD算法，提出RelFCI算法，定义了新的图模型以支持关系领域的因果发现。

Result: 实验证明RelFCI能有效识别具有潜在混杂因素的关系因果模型中的正确因果结构。

Conclusion: RelFCI是一种可靠且完整的因果发现算法，适用于关系数据和潜在混杂因素的场景。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [218] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 论文提出用贝叶斯PINN替代集成方法，通过评估后验方差提升信息传播效果，实验表明其优于集成方法。


<details>
  <summary>Details</summary>
Motivation: 解决PINN在前向问题中信息传播不足的收敛问题。

Method: 用贝叶斯PINN替代集成方法，利用后验方差评估信息传播。

Result: 在基准问题上表现优于集成方法，与Adam和LBFGS结合的PINN集成相当。

Conclusion: 贝叶斯PINN是一种数学上更严谨的改进方法。

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [219] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文比较了学习率控制的不同方法，发现现有方法在特定任务中表现良好但缺乏普适性，强调需要算法选择方法，并指出随着任务复杂性增加，超参数优化方法效果减弱。


<details>
  <summary>Details</summary>
Motivation: 评估当前学习率控制方法的现状，揭示其局限性，并提出改进方向。

Method: 比较多保真度超参数优化、固定超参数调度和无超参数学习等方法在不同深度学习任务中的表现。

Result: 现有方法在特定任务中表现良好，但缺乏跨设置的可靠性；超参数优化方法在复杂任务中效果减弱。

Conclusion: 需要开发算法选择方法，并关注更相关的测试任务和可调优方法（如元学习），以提升学习率控制的效果。

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [220] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: 提出了一种基于神经后验估计的模拟推理方法，用于高效估计1型糖尿病生理模型参数，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病的葡萄糖-胰岛素相互作用复杂，传统方法在高维参数空间中效率低下。

Method: 采用基于神经后验估计的模拟推理方法，捕捉饮食、胰岛素和血糖水平的复杂关系。

Result: 实验显示，该方法在参数估计上优于传统方法，泛化能力更强，支持实时后验推断。

Conclusion: 该方法为1型糖尿病数字孪生提供了高效、可靠的参数估计方案。

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [221] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 论文提出了两种新指标Clipped Density和Clipped Coverage，用于更可靠地评估生成模型样本的质量，解决了现有指标在鲁棒性和可解释性上的不足。


<details>
  <summary>Details</summary>
Motivation: 生成模型在关键应用中的使用受到样本质量评估不可靠的限制，现有指标缺乏校准或对异常值的鲁棒性。

Method: 通过剪裁单个样本贡献和最近邻球的半径，提出Clipped Density和Clipped Coverage指标，防止异常样本影响整体评估。

Result: 新指标在合成和真实数据集上表现出更高的鲁棒性、敏感性和可解释性，优于现有方法。

Conclusion: Clipped Density和Clipped Coverage为生成模型样本质量评估提供了更可靠和直观的解决方案。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [222] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet将决策树集成转换为稀疏、部分连接的神经网络，保留符号结构并支持梯度优化，性能优于XGBoost。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的梯度优化能力与决策树的符号可解释性，构建紧凑且无需手动调优的模型。

Method: 将决策树的每个分支映射为隐藏神经元，形成稀疏神经网络，保留符号结构。

Result: 在多类分类任务中，BranchNet在准确性上显著优于XGBoost。

Conclusion: BranchNet在符号可解释性和性能上表现优异，但在二元任务中可能需要进一步优化。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [223] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 探讨如何通过边缘AI设备的集体计算能力实现去中心化和可持续的基础模型训练。


<details>
  <summary>Details</summary>
Motivation: 基础模型的计算需求大，导致环境问题和中心化控制风险，需要更可持续和去中心化的解决方案。

Method: 利用连接边缘AI设备的闲置计算资源进行分布式训练。

Result: 提出了实现这一愿景的挑战和可持续性优势。

Conclusion: 去中心化和可持续的基础模型训练是可行的，但需解决技术和社会挑战。

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [224] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: 提出了一种基于模型强化学习的新知识迁移方法，将高容量多任务代理压缩为紧凑模型，显著提升性能，并优化模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境中部署大型世界模型的关键挑战。

Method: 通过蒸馏技术将高容量多任务代理（317M参数）压缩为紧凑模型（1M参数），并进行FP16后训练量化。

Result: 蒸馏模型在MT30基准上达到28.45的标准化分数，优于原始1M参数模型的18.93，模型大小减少约50%。

Conclusion: 该方法解决了实际部署限制，为资源受限应用中的多任务强化学习系统提供了更高效的解决方案。

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [225] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: 提出一种新方法，利用图神经网络（GNN）解决SAT问题，通过将k-CNF公式映射为MILP问题并编码为加权二分图进行训练和测试。理论证明方法的稳定性和局限性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在解决SAT问题中的应用，通过结合MILP技术提升GNN的性能和适用范围。

Method: 将k-CNF公式映射为MILP问题，编码为加权二分图后输入GNN进行训练和测试。

Result: 理论证明方法的稳定性和局限性，实验显示方法在简单神经网络架构下取得有希望的结果。

Conclusion: 该方法为GNN解决SAT问题提供了新思路，尽管存在理论局限性，但在实际应用中表现良好。

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [226] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: mGRADE是一种混合内存系统，结合了1D卷积和最小门控循环单元，适用于内存受限的边缘设备多尺度时间处理。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备在内存限制下同时捕捉短时和长时动态的需求，现有方法（如Transformer、RNN、TCN）各有不足。

Method: 提出mGRADE，结合可学习间隔的1D卷积和minGRU，卷积层捕捉快速变化，循环模块维护全局上下文。

Result: 在合成任务和像素级图像分类基准测试中，mGRADE表现优于纯卷积和纯循环模型，内存占用减少20%。

Conclusion: mGRADE是内存受限边缘设备多尺度时间处理的高效解决方案。

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [227] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文重新审视了流行的OOD检测方法，指出这些方法在根本上是回答错误的问题，无法有效识别OOD数据，且现有改进措施也无法解决这一根本问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于预测不确定性或特征的OOD检测方法存在根本性错误，无法有效识别OOD数据，需要重新审视其局限性。

Method: 分析了基于不确定性和特征的OOD检测方法，探讨了其局限性，并考察了现有改进措施（如混合方法、模型扩展等）的效果。

Result: 发现这些方法在OOD检测中存在不可减少的错误，且现有改进措施无法解决根本问题。

Conclusion: OOD检测需要新的方法，现有方法在根本目标上存在偏差，无法有效解决问题。

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [228] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: SubLoRA是一种基于子模函数最大化的低秩适应（LoRA）秩确定方法，利用二阶信息（Hessian矩阵）改进传统线性化方法的不足，并通过贪心算法实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如AdaLoRA）依赖损失函数的一阶近似，在LoRA参数优化良好时可能不准确且病态，因此需要更可靠的二阶方法。

Method: 将秩确定问题转化为组合优化问题，提出子模函数最大化框架和贪心算法，并构造满足子模条件的Hessian矩阵投影。

Result: 实验表明，SubLoRA在秩确定和联合训练性能上优于现有方法。

Conclusion: SubLoRA结合了理论基础、二阶准确性和计算效率，适用于物理信息神经网络（PINN）的微调。

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [229] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Main category: cs.LG

TL;DR: FAE是一种基于变分自编码器和扩张卷积神经网络的时间序列异常检测基础模型，能够通过大规模预训练学习复杂时间模式，实现零样本异常检测。


<details>
  <summary>Details</summary>
Motivation: 受大型预训练基础模型成功的启发，研究如何将其应用于时间序列建模，尤其是异常检测任务。

Method: 结合变分自编码器（VAEs）和扩张卷积神经网络（DCNNs），构建通用时间序列模型FAE，支持零样本异常检测。

Result: 在多维时间序列数据集（包括移动ISP数据和KDD 2021数据集）上展示了初步结果。

Conclusion: FAE作为一种基础生成模型，能够有效学习时间序列模式，适用于跨领域的异常检测任务。

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [230] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: 本文提出了一种结合LSTM和Transformer的混合深度学习模型，利用iForest和AE进行伪标记，用于心理健康账单异常检测，解决了类别不平衡和标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康账单的复杂性导致异常（如欺诈）频发，而传统机器学习方法在类别不平衡、标签稀缺和复杂序列模式上表现不佳。

Method: 采用混合深度学习方法，结合LSTM和Transformer，通过iForest和AE生成伪标签，并在两个真实账单数据集上评估。

Result: 在声明级数据上，iForest LSTM基线模型召回率最高（0.963）；在操作级数据上，基于iForest的混合模型召回率最高（0.744），但精度较低。

Conclusion: 研究表明，伪标记与混合深度学习结合在复杂、不平衡的异常检测场景中具有潜力。

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>
