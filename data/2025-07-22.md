<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.CV](#cs.CV) [Total: 35]
- [cs.CR](#cs.CR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一个针对专业领域（如金融、医学、法律）的多模态写作助手，通过离线知识库和任务分解生成高质量文档，解决了现有方法在检索一致性和内容可靠性上的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在专业领域写作中存在知识不足和幻觉问题，现有解决方案（如RAG和在线搜索）在检索一致性和内容质量上表现不佳。

Method: DeepWriter采用任务分解、大纲生成、多模态检索和分步写作的流程，结合结构化知识库和层次化知识表示。

Result: 实验表明，DeepWriter在金融报告生成中优于现有基线，生成内容的事实准确性和质量更高。

Conclusion: DeepWriter通过离线知识库和多模态检索，显著提升了专业领域写作的准确性和质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究发现，微调目标与模型编辑技术的交互会导致编辑知识比预训练知识更容易被遗忘，冻结相关层可改善知识保留。


<details>
  <summary>Details</summary>
Motivation: 理解微调对已编辑知识的影响，以提升模型编辑方法的实用性。

Method: 系统研究不同微调目标与模型编辑技术的交互作用，并测试冻结相关层的效果。

Result: 编辑知识在微调中更容易遗忘，冻结相关层能显著提高知识保留。

Conclusion: 当前编辑方法在微调下存在局限性，未来需评估编辑鲁棒性并改进方法。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS框架通过多智能体协作整合开源LLMs，性能超越闭源LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索开源LLMs集体协作是否能超越闭源LLMs。

Method: 提出SMACS框架，包括检索式先验选择（RPS）和探索-利用驱动的后验增强（EPE）。

Result: 在八个主流基准测试中，SMACS超越2025年领先闭源LLMs（如Claude-3.7-Sonnet、GPT-4.1等）。

Conclusion: SMACS证明了开源LLMs协作的潜力，推动了智能上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer是一个神经符号系统，通过NLP和逻辑推理分析隐私政策，帮助用户个性化理解数据使用条款，显著减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 现代用户很少阅读隐私政策，但需要工具来个性化分析政策内容，以保护数据隐私。

Method: 结合NLP提取政策文本的正式表示，通过逻辑推理比较用户偏好与政策内容，生成合规报告。

Result: 在评估中，PoliAnalyzer准确识别数据使用实践（F1-score 90-100%），并发现95.2%的政策内容与用户偏好无冲突。

Conclusion: PoliAnalyzer支持大规模自动化隐私政策分析，帮助用户掌握数据控制权，促进社会对平台数据实践的讨论。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 论文探讨了利用NLP模型识别社交媒体文本中的双相情感障碍症状，发现RoBERTa表现最佳，F1分数达98%，而基于静态嵌入的LSTM模型效果较差。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍常因早期症状不明显和社会污名被漏诊，研究旨在通过NLP技术提升早期筛查能力。

Method: 评估了多种基于Transformer的模型（如BERT、RoBERTa）和LSTM模型，使用Reddit帖子数据集进行实验。

Result: RoBERTa表现最佳，F1分数约98%；基于BERT嵌入的LSTM效果接近，而静态嵌入的LSTM表现极差。

Conclusion: 上下文语言建模对双相情感障碍检测至关重要，DistilBERT在效率和准确性间取得平衡，为心理健康NLP应用提供参考。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在用户交互应用中会根据文本中的身份标记（如种族、性别、年龄）产生偏见，影响医疗、法律、政治等领域的决策，可能导致有害结果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM如何利用用户文本中的身份信息进行决策，并分析其在高风险应用中的偏见影响。

Method: 对五个高风险领域的LLM应用（医疗、法律、政治、政府福利、薪资）进行综合分析，评估身份标记对模型响应的影响。

Result: LLM对身份标记极为敏感，种族、性别和年龄等因素显著影响其决策，例如医疗建议中的差异化标准、薪资推荐中的性别和种族偏见等。

Conclusion: 现成的LLM应用可能加剧社会不平等，建议在部署前进行全面评估，并提供新工具以检测身份标记对模型决策的影响。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 论文提出CCL-XCoT框架，通过两阶段微调减少多语言大模型在低资源语言中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中易产生幻觉（如不准确或虚构输出），影响领域特定生成任务。

Method: 采用课程对比学习增强跨语言语义对齐，结合跨语言思维链（XCoT）提示策略引导模型推理。

Result: 实验显示CCL-XCoT将幻觉率降低62%，显著提升跨语言事实知识迁移。

Conclusion: CCL-XCoT有效减少幻觉，无需依赖外部检索或多模型集成。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）供应链中模型与数据集的关系，构建了一个异构图进行分析，揭示了供应链的结构特征和动态性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的开发依赖预训练模型和外部数据集，可能继承漏洞或偏见，因此需要理解其供应链以检测风险、提高公平性。

Method: 设计方法系统收集LLM供应链数据，构建有向异构图（397,376节点和453,469边），并进行多种分析。

Result: 发现供应链图规模大、稀疏且符合幂律分布；数据集在训练中起关键作用；模型与数据集相互依赖；图具有动态性。

Conclusion: 研究为理解LLM供应链提供了基础，有助于风险检测和模型优化。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix是一个自动提示优化框架，将自然语言任务描述转化为高质量提示，无需手动调整或领域专业知识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）依赖精心设计的提示，但提示工程仍依赖人工且对非专家不友好。Promptomatix旨在解决这一问题。

Method: Promptomatix结合元提示优化器和DSPy编译器，分析用户意图、生成合成数据、选择提示策略并优化提示。

Result: 在5个任务类别中，Promptomatix表现优于现有库，同时减少提示长度和计算开销。

Conclusion: Promptomatix实现了高效、可扩展的提示优化，为非专家提供了便利。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一种针对多样化图表类型优化的LVLM，通过高效数据生成和双路径训练策略提升图表理解能力，并建立了新的评估基准ChartDQA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图表理解任务中存在泛化能力不足和数据对齐预训练缺乏的问题。

Method: 提出高效数据生成管道和双路径训练策略，结合底层数据推理。

Result: 实验表明ChartScope显著提升了对多样化图表的理解能力。

Conclusion: ChartScope通过创新方法解决了现有局限性，并在图表理解任务中表现出色。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 论文探讨了通过选择性翻译提升多语言大语言模型在低资源语言（如印地语）上的对齐效果，相比传统翻译方法更高效。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在低资源语言上表现较差，而高质量对齐数据稀缺，翻译现有英语数据时难以保留关键内容（如代码、数学表达式）。

Method: 采用基于大语言模型的选择性翻译技术，仅翻译可翻译部分，保留非翻译内容和句子结构，并与传统翻译方法（如GCP）对比。

Result: 实验表明选择性翻译在印地语上效果优于传统翻译，且混合翻译样本与原始英语数据能进一步提升对齐效果。

Conclusion: 选择性翻译是提升多语言大语言模型对齐的实用且有效方法。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）处理语言时间意义的方式，发现其与人类认知存在差异，并开发了标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否以人类方式处理语言时间意义，揭示其认知局限性。

Method: 采用专家参与循环的探测流程，进行针对性实验，评估LLMs的语义和语用推理能力。

Result: LLMs过度依赖典型性，产生不一致的时间判断，且因果推理能力不足，表明其叙事理解有限。

Conclusion: LLMs处理语言时间意义的方式与人类不同，缺乏稳健的叙事理解能力，需进一步研究其认知机制。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）在人格评估中的应用，发现尽管模型具有高重测信度，但在构念效度和准确性方面表现有限。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在人格特质推断中的潜力，解决以往研究中合成数据或缺乏心理测量效度的问题。

Method: 使用555个半结构化访谈和BFI-10自评分数作为基准，测试了三种LLM（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）的零样本提示和链式思维提示方法。

Result: 模型重测信度高，但构念效度低（最大Pearson's r=0.27），预测偏向中等或高特质水平，链式思维提示仅略微改善分布对齐。

Conclusion: 当前LLM在人格推断中存在局限性，需基于证据的开发以提升心理学应用效果。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 论文介绍了LinkedIn开发的内部聊天机器人，通过知识图谱和Text-to-SQL代理实现数据自助查询，支持多种用户意图，并展示了实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL基准测试中取得进展，但构建企业级解决方案仍具挑战。论文旨在解决这一问题。

Method: 方法包括构建动态知识图谱、开发Text-to-SQL代理（支持检索、排名、查询生成和错误修正）以及设计交互式聊天机器人。

Result: 聊天机器人每周有300多名用户，53%的响应在内部测试中正确或接近正确。消融研究揭示了关键组件。

Conclusion: 论文为企业级Text-to-SQL解决方案的开发提供了实用路径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出了一种基于错误感知的师生框架，通过GPT-4o的指导改进生物医学文本中的关系分类（RC），并结合知识图谱（KG）增强，实现了在多个数据集上的最优性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类对构建知识图谱和药物再利用等应用至关重要，但现有方法存在错误率高的问题。

Method: 使用师生框架，教师模型分析学生模型的预测错误，生成针对性修复（如句子改写和KG增强），并通过课程学习训练学生模型。

Result: 在5个PPI数据集中的4个和DDI数据集上达到最优性能，在ChemProt上保持竞争力。

Conclusion: 该方法通过错误分析和知识图谱增强显著提升了关系分类性能，为生物医学应用提供了有力支持。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是一个专为半导体显示行业设计的高性能推理模型，通过监督微调和强化学习提升能力，并在基准测试中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在半导体显示行业效果有限，缺乏领域专业知识，需要针对性解决方案。

Method: 结合行业知识库进行监督微调和强化学习，引入自动评估框架和领域特定的检索增强生成机制。

Result: X-Intelligence 3.0在多个评估中表现优于SOTA模型DeepSeek-R1-671B，展示了高效性。

Conclusion: X-Intelligence 3.0为半导体显示行业的复杂推理问题提供了强大解决方案。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel是一种优化的多语言Sentence Transformer模型，用于序数Word-in-Context分类，通过基于角度距离的排名目标在复杂空间中优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过优化序数任务来提升二元任务的性能，并为WiC建模提供统一处理方法。

Method: 测试多种回归和排名任务的损失函数，采用基于复杂空间角度距离的排名目标。

Result: 在序数和二元数据上表现优于先前模型，且优化序数任务提升了二元任务性能。

Conclusion: XL-DURel为不同任务形式的WiC建模提供了统一框架。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 论文探讨了使用AudiBERT模型在协作问题解决（CPS）诊断中的改进，特别是在社会认知维度上显著优于BERT模型，但情感维度未观察到类似提升。数据量和人类评分一致性对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 解决协作问题解决（CPS）诊断中多模态数据利用不足和人类与AI互补性缺乏指导的问题。

Method: 采用多模态BERT变体AudiBERT，整合语音和声学-韵律特征，并与BERT模型对比。

Result: AudiBERT在社会认知维度分类上显著优于BERT，但情感维度无显著差异；数据量和人类评分一致性影响模型性能。

Conclusion: 提出结构化方法以实现人类与AI互补性，强调模型可解释性以支持人类在反思性编码中的参与。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 研究使用SHAP分析BERT模型在CPS分类中词标记的贡献，发现高分类性能不一定有合理解释，并识别了无意义的词标记影响分类。


<details>
  <summary>Details</summary>
Motivation: 增强BERT模型在CPS诊断中的可解释性，以提升教师等终端用户的信任和采用率。

Method: 使用SHAP分析BERT模型对CPS转录数据中词标记的分类贡献。

Result: 发现分类性能高但解释性不足，部分词标记频繁影响分类，甚至包括无意义的词。

Conclusion: 需进一步研究集成模型架构和人机互补，以提升CPS诊断的细粒度区分能力。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型（如GPT）进行NLP数据增强的方法，比较了传统方法（如复述和回译）与纯生成方法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定机器学习任务中的数据稀缺和类别不平衡问题。

Method: 选择了基于ChatGPT的数据增强方法，比较了四种不同方法在多个实验设置中的表现。

Result: 回译和复述方法在生成数据质量和分类性能上表现优于或少样本生成方法。

Conclusion: 传统数据增强方法结合新模型可以取得与纯生成方法相当甚至更好的效果。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [22] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 论文提出了一种基于检索增强生成（RAG）的方法，创建了一个针对肯尼亚初级医疗的基准数据集和评估框架，以评估大型语言模型（LLMs）在非洲医疗场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在非洲低资源医疗环境中的有效性，填补其在本地化应用中的研究空白。

Method: 使用RAG技术将临床问题与肯尼亚国家指南对齐，生成多语言（英语和斯瓦希里语）的临床场景、选择题及答案，并通过专家审核确保准确性和文化适应性。

Result: 发现LLMs在本地化医疗场景中的表现显著低于美国基准，突出了本地化评估的重要性。

Conclusion: 该研究为非洲医疗系统提供了一个可复制的指南驱动动态评估模型，支持AI的安全部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [23] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 论文发现，通过两部分的仿射近似可以很好地近似某些主客体关系的Transformer计算，线性变换Ws能准确重现最终客体状态，尤其在形态学关系上达到90%的忠实度。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型中主客体关系的可解释性，验证线性变换是否能有效捕捉语言模型中的概念关系。

Method: 使用Bigger Analogy Test Set，通过线性变换Ws（s为主词的中层表示，W为模型导数）近似最终客体状态。

Result: 线性技术在形态学关系上达到90%的忠实度，多语言和多模型实验验证了类似结果。

Conclusion: 语言模型中的某些概念关系（如形态学）可通过潜在空间的线性变换稀疏编码，具有较高的可解释性。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [24] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为Cleanse的聚类语义一致性方法，用于估计大型语言模型（LLM）生成中的不确定性，以检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM在生成内容时可能出现幻觉（不准确回答），影响其安全性和可靠性，因此需要有效的不确定性估计方法。

Method: Cleanse通过聚类LLM隐藏嵌入中的语义信息，计算簇内一致性占总一致性的比例来量化不确定性。

Result: 在LLaMA-7B、LLaMA-13B、LLaMA2-7B和Mistral-7B模型及SQuAD和CoQA基准测试中验证了Cleanse的有效性。

Conclusion: Cleanse是一种有效的幻觉检测方法，有助于提升LLM的可靠性和安全性。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [25] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen是一个47B泰语语料库，通过定制化Dolma流程构建，显著提升了泰语语言模型的质量和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语料库处理泰语时缺乏文化敏感性和脚本适应性，且缺乏透明度和可复现性。

Method: 采用泰语适应的Dolma流程，包括自定义语言识别、质量过滤器和内容过滤器，并结合非网络来源数据。

Result: Mangosteen显著提升了模型性能，在泰语基准测试中超越现有模型。

Conclusion: Mangosteen为泰语及区域语言模型研究提供了透明、高质量的语料库基础。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [26] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在医疗编码（ICPC-2）中的潜力，使用特定领域搜索引擎的输出，结果显示多数模型表现良好，但需进一步优化和验证。


<details>
  <summary>Details</summary>
Motivation: 医疗编码对医疗数据的研究、质量监控和政策制定至关重要，研究旨在探索LLMs在自动化编码中的潜力。

Method: 使用437个巴西葡萄牙语临床表达的数据集，通过语义搜索引擎检索候选编码，33个LLMs模型进行匹配选择，评估性能指标包括F1分数、成本等。

Result: 28个模型F1分数>0.8，10个>0.85；优化检索器可提升性能4分；小模型在格式和输入长度上表现较差。

Conclusion: LLMs在自动化ICPC-2编码中潜力显著，但需更广泛的多语言和端到端临床验证。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [27] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: 论文介绍了MiroMind-M1系列，一个完全开源的推理语言模型，旨在提高透明度和可复现性，并在数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 封闭源代码的推理语言模型（如GPT-3）缺乏透明性和可复现性，而现有开源项目也常因缺少关键资源（如数据集和训练配置）而受限。

Method: 模型分两阶段训练：首先在719K数学推理问题的精选语料上进行SFT，随后在62K可验证问题上进行RLVR，并引入Context-Aware Multi-Stage Policy Optimization算法。

Result: 模型在AIME24、AIME25和MATH基准测试中表现优异，达到或超过现有开源模型的性能。

Conclusion: MiroMind-M1系列为社区提供了完整的资源（模型、数据集和配置），支持进一步研究和社区发展。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [28] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文综述了Hugging Face Hub上公开的阿拉伯语后训练数据集，从四个维度（LLM能力、可操控性、对齐性和鲁棒性）评估其质量，发现任务多样性不足、文档缺失等问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 后训练是提升预训练大语言模型性能的关键技术，而数据集的质量和多样性对后训练效果至关重要。本文旨在评估现有阿拉伯语后训练数据集的现状，以推动该领域发展。

Method: 通过四个维度（LLM能力、可操控性、对齐性和鲁棒性）对Hugging Face Hub上的阿拉伯语后训练数据集进行系统评估，考察其流行度、实用性、文档质量等指标。

Result: 发现阿拉伯语后训练数据集存在任务多样性不足、文档和标注不一致、社区采用率低等问题。

Conclusion: 本文指出了阿拉伯语后训练数据集的不足，并提出了具体改进建议，以促进阿拉伯语大语言模型的发展和应用。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [29] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 论文提出了一种新的土耳其语自杀意念检测语料库，并通过多标注者和大型语言模型（LLMs）的框架解决了标注可靠性问题。同时，通过迁移学习评估了模型在不同语言数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 自杀意念检测面临语言覆盖不足和标注不可靠的挑战，现有研究多依赖低质量或英语数据集，限制了AI在全球自杀预防中的应用。

Method: 构建土耳其语社交媒体语料库，引入多标注者和LLMs的标注框架，并通过迁移学习评估模型在多种语言数据集上的表现。

Result: 研究发现现有模型在零样本迁移学习中表现不佳，强调了标注和模型可靠性的重要性。

Conclusion: 呼吁在心理健康NLP中采用更严格、语言包容的标注和评估方法，并提升数据和模型的透明度与可靠性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [30] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 研究发现同行评审中存在语言偏见，匿名性对公平性的影响与传统假设不同。


<details>
  <summary>Details</summary>
Motivation: 探讨同行评审中语言如何强化不平等，揭示隐藏的偏见。

Method: 对8万多篇评审进行自然语言处理和统计建模，分析作者性别、种族和机构背景对评审语言的影响。

Result: 发现评审语气、情感和支持性语言因作者背景而异，匿名性披露影响评审语言。

Conclusion: 研究挑战了匿名性促进公平的假设，为学术出版改革提供重要启示。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [31] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究探讨机器学习如何通过模拟儿童语言输入环境来理解语言习得，验证了多模态神经网络在有限数据下的稳健性。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习模型是否能从类似儿童的语言输入中学习，以缩小与人类语言习得的差距。

Method: 使用自动语音转录方法处理SAYCam数据集，生成多模态数据集，并训练多种神经网络配置。

Result: 网络能从每个儿童的数据中学习并泛化词汇-指代映射，验证了模型的稳健性，同时揭示了学习中的个体差异。

Conclusion: 多模态神经网络在有限数据下表现稳健，但学习模式受个体输入差异影响。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [32] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE是一种生成式推荐框架，通过引入Chain-of-Thought（CoT）标记化和Journey-Aware稀疏注意力机制，解决了现有生成模型在多行为推荐中的效率和信息不足问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型在多行为推荐系统中潜力巨大，但面临标记推理信息不足、计算成本高和多尺度建模有限的问题。

Method: GRACE采用混合Chain-of-Thought标记化方法，结合产品知识图谱属性，并设计Journey-Aware稀疏注意力机制优化计算效率。

Result: 在两个真实数据集上，GRACE显著优于现有基线，HR@10和NDCG@10提升最高达106.9%和106.7%，同时减少48%的注意力计算。

Conclusion: GRACE通过创新的标记化和注意力机制，在多行为推荐中实现了高效且性能优越的生成式推荐。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [33] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech框架通过迭代融合和动态压缩训练，解决了LSLMs处理长语音的挑战，无需专用长语音训练数据，并在LongSpeech-Eval基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LSLMs在长语音处理方面存在不足，主要由于缺乏长语音训练数据和高计算成本。

Method: 引入FastLongSpeech框架，采用迭代融合策略压缩长语音序列，并通过动态压缩训练适应LSLMs。

Result: 实验表明，FastLongSpeech在长语音和短语音任务中均表现优异，同时显著提升推理效率。

Conclusion: FastLongSpeech为LSLMs高效处理长语音提供了可行方案，填补了研究空白。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [34] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 论文提出了一种基于意图从长文档生成图表的无监督两阶段框架，显著提升了数据准确性和图表类型匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以直接从长文档根据用户意图生成图表，需要用户预先手动选择相关内容。

Method: 采用两阶段框架：LLM分解意图并提取数据，启发式模块选择图表类型后生成代码。

Result: 在金融和科学领域的数据集上，方法在数据准确性和图表类型匹配度上分别比基线高9和17个百分点。

Conclusion: 提出的框架有效解决了从长文档基于意图生成图表的挑战，并通过新指标验证了数据准确性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [35] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 推理蒸馏能提升小语言模型的推理能力，但对长上下文检索和理解的影响尚未研究。本文通过实验证明，蒸馏显著改善长上下文理解，解决“中间迷失”问题。


<details>
  <summary>Details</summary>
Motivation: 研究大规模推理蒸馏对长上下文检索和理解的影响，尤其是在检索增强生成（RAG）系统中的重要性。

Method: 使用从Deepseek-R1蒸馏的开源模型，通过多文档问答任务评估长上下文信息提取和整合能力。

Result: 蒸馏显著提升长上下文理解，促进更详细的推理过程，缓解“中间迷失”问题。

Conclusion: 推理蒸馏不仅增强推理能力，还能有效改善长上下文理解，为RAG系统提供更可靠的生成能力。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [36] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 研究表明，即使是小规模的语言模型（TLMs），预训练也能显著提升分类任务性能，且性能差距随预训练数据量和任务相关性增加。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型（LLMs）预训练需要巨大计算资源，限制了广泛研究参与，因此探索小规模语言模型（TLMs）是否具备类似能力成为关键需求。

Method: 通过预训练BERT-6及其变体（如BERT-1）在Wikipedia子集上，并在FewRel、AGNews和DBPedia分类任务中评估性能。

Result: 预训练的TLMs在分类任务中表现优于未预训练模型，且性能差距随预训练数据量和任务相关性增加。此外，通过软委员会机制，多个浅层预训练模型可复现深层模型的分类精度。

Conclusion: TLMs预训练在小规模下仍有效，未来研究可进一步探索其机制，尤其是在生物启发模型中的应用。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [37] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT方法通过整合多源异构知识（内部情感知识和外部因果知识），显著提升大语言模型在情感-原因对提取任务（ECPE）中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在情感-原因对提取任务（ECPE）中表现不佳，主要原因是缺乏辅助知识，限制了其情感感知和因果推理能力。

Method: 提出MEKiT方法，结合指令模板和数据混合的指令调优，分别优化情感识别和因果推理。

Result: 实验证明MEKiT在ECPE任务中表现优于基线方法，显著提升LLMs的性能。

Conclusion: MEKiT为ECPE任务提供了更有效和适应性强的解决方案。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [38] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 论文提出了一种名为SASFT的方法，通过稀疏自编码器分析并减少大语言模型中的意外代码切换问题，实验表明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言能力上表现优异，但存在意外代码切换问题，导致模型响应可读性和可用性下降。现有研究缺乏机制分析且效果有限。

Method: 使用稀疏自编码器分析代码切换问题，提出SASFT方法，通过监督微调控制语言特征的预激活值。

Result: 在五种模型和三种语言上的实验显示，SASFT将意外代码切换减少50%以上，并在四种情况下完全消除，同时保持或提升多语言性能。

Conclusion: SASFT有效解决了代码切换问题，同时保留了大语言模型的多语言能力。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [39] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 比较分析用于拟合3D图像数据的镶嵌模型算法策略，评估不同优化方法在生成近似体素结构时的性能。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，准确拟合3D图像数据（如多晶体和泡沫）的镶嵌模型是一个不断发展的领域，需要评估不同算法的适用性。

Method: 使用线性/非线性规划、随机优化（交叉熵法）和梯度下降等方法生成Voronoi、Laguerre和GBPD模型。

Result: 通过实际数据集评估拟合质量，发现模型复杂度、优化方法复杂度和近似质量之间存在权衡。

Conclusion: 研究结果为根据数据特征和应用需求选择合适方法提供了指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [40] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 论文探讨了深度学习在语义分割中的应用，提出了几种高效模型，并研究了不同主干网络对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，机器在复杂任务中表现出色，减少了人类专家的需求。深度学习在自动驾驶等领域尤为重要，本文旨在通过语义分割提升场景理解能力。

Method: 使用BDD100k数据集，提出多种高效模型，并尝试不同主干网络作为编码器，研究其对语义分割性能的影响。

Result: 实验表明，选择合适的主干网络显著提升了语义分割的性能，从而更好地理解场景和环境。

Conclusion: 通过分析准确率、平均IoU和损失函数，验证了所提模型的有效性，各项指标均有所提升。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [41] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种基于梯度的测试时适应方法，针对视觉语言模型（VLMs）设计，通过软对比损失提升分布偏移下的泛化能力，并在开放集设置中表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本任务中表现良好，但在分布偏移下泛化能力不足。现有的测试时适应方法（如熵最小化）与VLMs的对比训练目标不匹配，导致性能受限和失败模式。

Method: 提出CLIPTTA，采用与CLIP预训练目标一致的软对比损失，并通过理论分析证明其批次感知设计能避免崩溃风险。扩展至开放集设置，使用异常对比暴露（OCE）损失提升OOD检测。

Result: 在75个数据集上的实验表明，CLIPTTA优于基于熵的目标，并在多样分布偏移下表现稳定，优于现有TTA方法。

Conclusion: CLIPTTA通过对齐预训练目标的适应方法，显著提升了VLMs在分布偏移和开放集任务中的性能。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [42] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 论文提出了一种名为注意力聚焦（AF）的机制，通过剪除非信息性标记来解决广义类别发现（GCD）中的注意力分散问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时，模型注意力容易被任务无关的背景区域分散，导致特征提取不理想。

Method: AF由两个组件组成：标记重要性度量（TIME）和多尺度标记自适应剪枝（TAP），通过量化标记重要性并剪除非信息性标记来优化注意力。

Result: AF在SimGCD方法中实现了高达15.4%的性能提升，且计算开销极小。

Conclusion: AF是一种轻量级、即插即用的模块，能有效提升GCD方法的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [43] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 生成超分辨率（GSR）在感知图像质量上领先，但存在与低分辨率图像（LRI）或真实图像（GTI）不匹配的伪影（幻觉）。本文提出一种基于多模态大语言模型（MLLM）的“幻觉评分”（HS）来衡量和缓解这一问题，并通过深度特征距离优化GSR模型。


<details>
  <summary>Details</summary>
Motivation: GSR模型在感知质量上表现优异，但生成的细节可能与LRI或GTI不匹配，导致幻觉问题，限制了实际应用。本文旨在解决这一未充分研究的问题。

Method: 利用MLLM构建提示，评估幻觉视觉元素并生成HS；发现某些深度特征距离与HS强相关，提出用这些特征作为可微奖励函数优化GSR模型。

Result: HS与人类评估高度一致，并为超分辨率模型提供了补充性见解；深度特征距离与HS的相关性为模型优化提供了新方向。

Conclusion: 通过HS和深度特征距离优化GSR模型，可有效缓解幻觉问题，提升生成图像的质量与一致性。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [44] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个结合深度学习和光流的半自动化工具包，用于B超视频中的点跟踪，解决了噪声和运动模糊等问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: B超视频中的组织运动跟踪因噪声和低对比度等问题而困难，需要一种通用且高精度的解决方案。

Method: 结合深度学习和光流技术，提供图形界面支持训练数据生成和模型迭代优化，并引入新型光流滤波技术。

Result: DUSTrack在精度上优于现有零样本跟踪器，与专用方法相当，适用于多种临床和生物力学场景。

Conclusion: DUSTrack是一个开源、通用的高精度组织运动跟踪工具，具有广泛的应用潜力。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [45] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，用于可解释的可用性基础，通过结合常识先验和视觉证据，迭代优化预测。


<details>
  <summary>Details</summary>
Motivation: 提升场景理解的准确性和可解释性，实现更稳健和可信赖的决策。

Method: 整合ConceptNet和语言模型的结构化常识先验与CLIP的视觉证据，通过基于能量的推理循环迭代优化。

Result: 在多对象、无标签设置中，CRAFT提高了准确性和可解释性。

Conclusion: CRAFT为场景理解提供了一种透明且目标驱动的方法，推动了稳健和可信赖的系统发展。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [46] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯变形场的3DGS视频流框架，通过混合显著性分块和差异化质量建模，实现高效压缩和带宽适应，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频因数据量大和传输复杂度高，对流媒体提出挑战，需创新解决方案。

Method: 设计基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模。

Result: 实验显示方法在视频质量、压缩效率和传输速率上优于现有方法。

Conclusion: 提出的框架有效解决了3DGS视频流传输的挑战，具有实际应用潜力。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [47] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个针对真实红外图像的多模态大语言模型，基于26万对真实红外图像-文本数据集（IR-TD），通过双跨模态课程迁移学习策略，在9项任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖合成红外图像、无法捕捉红外模态独特特性的问题。

Method: 构建大规模IR-TD数据集，结合LLM生成和规则标注的文本，提出双跨模态课程迁移学习策略。

Result: 在9项任务中表现优于现有方法，包括识别和定位等。

Conclusion: IRGPT通过真实数据集和迁移学习策略，显著提升了红外图像的多模态处理能力。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [48] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），通过正交几何一致性优化局部与全局特征的融合，提升点云配准中高质量对应关系的识别。


<details>
  <summary>Details</summary>
Motivation: 解决点云配准中局部与全局特征融合的挑战，如特征冗余和复杂空间关系。

Method: 利用Gestalt原则设计GPI-Net，包括正交集成策略、Gestalt特征注意力块（GFA）和双路径多粒度并行交互聚合块（DMG）。

Result: 在多个挑战性任务中表现优于现有方法。

Conclusion: GPI-Net通过Gestalt原则有效优化了局部与全局特征的融合，显著提升了点云配准的性能。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [49] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于显著性分析的自适应3D高斯溅射视频（3DGS）分块技术，结合了空间和时间特征，并开发了质量评估框架和基于元学习的自适应比特率算法。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频流在提供沉浸式3D视频体验方面表现出色，但仍面临分块、质量评估和比特率适应等挑战。

Method: 提出自适应3DGS分块技术、新型质量评估框架和基于元学习的自适应比特率算法。

Result: 实验表明，所提方法显著优于现有技术。

Conclusion: 本文为3DGS视频流的挑战提供了全面解决方案，具有显著性能提升。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [50] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS提出了一种混合专家框架，通过全局专家和场景自适应专家组结合双感知路由器，实现自适应和鲁棒的端到端自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 现有单模式规划方法难以处理多样化场景，需要一种能适应复杂交通环境的框架。

Method: GEMINUS包含全局专家、场景自适应专家组和双感知路由器，动态激活专家模块。

Result: 在Bench2Drive基准测试中表现优异，驾驶分数和成功率均领先，且仅需单目视觉输入。

Conclusion: GEMINUS通过混合专家框架显著提升了自动驾驶的适应性和鲁棒性。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [51] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一个抗篡改的可视化图像数据检索框架，通过嵌入元数据链接解决图像传播中信息丢失问题，具有高鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 可视化图像传播中常丢失关键信息（如源代码、交互功能），现有方法对常见图像篡改（如裁剪、编辑）脆弱，缺乏实用性。

Method: 提出VisGuard框架，采用重复数据平铺、可逆信息广播和基于锚点的裁剪定位技术，增强嵌入数据的鲁棒性。

Result: VisGuard在数据检索准确性、嵌入容量和抗篡改/隐写分析安全性方面表现优异。

Conclusion: VisGuard能有效保护和促进可视化传播与信息传递，支持交互式图表重建、篡改检测和版权保护等应用。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [52] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet提出了一种新颖的序列建模框架，结合空间特征提取和时间差分，通过轻量级1D卷积编码器和可学习的差分时间算子（DSD）提升动态环境中的视觉地点识别性能。


<details>
  <summary>Details</summary>
Motivation: 动态和感知混淆环境中的视觉地点识别（VPR）是长期定位的核心挑战，现有方法多基于单帧嵌入，忽略了图像序列的时间连贯性。

Method: OptiCorNet结合1D卷积编码器和DSD模块，通过固定权重差分核和LSTM细化生成紧凑且鲁棒的描述符，并引入四元组损失增强类间分离性。

Result: 在多个公开基准测试中，OptiCorNet在季节和视角变化下优于现有方法。

Conclusion: OptiCorNet通过端到端学习序列级嵌入，显著提升了动态环境中的地点识别效果。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [53] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT提出了一种无需数据的ViT量化方法，通过合成高质量样本和激活校正矩阵，显著提升了量化模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成样本时未能充分平衡全局和局部特征，且量化模型与全精度模型的中间层激活分布差异大，导致性能下降。

Method: DFQ-ViT通过按难度递增顺序合成样本，并引入激活校正矩阵来对齐中间层激活分布。

Result: 实验表明，DFQ-ViT性能优于现有DFQ方法，接近真实数据量化模型，如3位量化DeiT-T性能提升4.29%。

Conclusion: DFQ-ViT无需微调，降低了计算开销和部署门槛，符合绿色学习原则。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [54] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强的点云补全框架，通过跨模态检索学习结构先验信息，结合双通道控制门和分层特征融合机制，生成精细点云并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决不完整点云补全任务中缺乏典型结构特征的挑战，现有方法局限于特定输入类别，限制了生成能力。

Method: 设计了结构共享特征编码器（SSFE）提取跨模态特征并重构参考特征作为先验，结合双通道控制门增强相关特征；提出渐进式检索增强生成器（PRAG）分层融合参考先验与输入特征。

Result: 在多个数据集和真实场景中验证了方法的有效性，能够生成精细点云，并处理稀疏数据和未见类别。

Conclusion: 该方法通过检索增强和分层特征融合，显著提升了点云补全的质量和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [55] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种新型多模态大语言模型，通过令牌压缩技术解决病理学全切片图像（WSI）视觉问答（VQA）中的高计算资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 病理学全切片图像（WSI）的高分辨率（如10,000 x 10,000像素）导致多模态大语言模型（MLLM）处理时面临长上下文和高计算需求的问题。现有方法缺乏生成能力或资源消耗过大。

Method: 提出TCP-LLaVA，通过可训练的压缩令牌和模态压缩模块聚合视觉与文本信息，仅将压缩后的令牌输入语言模型生成答案。

Result: 在10种TCGA肿瘤亚型的实验中，TCP-LLaVA在VQA准确率上优于现有MLLM基线，并显著降低训练资源消耗。

Conclusion: TCP-LLaVA通过令牌压缩有效解决了WSI VQA中的计算资源问题，同时提升了性能。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [56] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 提出了一种基于事件流数据的运动分割与自运动估计框架，适用于神经形态视觉传感器，无需完整光流计算。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而事件数据的高时间分辨率与稀疏性未被充分利用，需结合几何约束提升性能。

Method: 通过优化流程进行事件过分割，利用残差分析分离运动物体，并结合运动相似性与时间一致性进行层次聚类。

Result: 在EVIMO2v2数据集上验证了准确的分割与平移运动估计，尤其在物体边界表现优异。

Conclusion: 该方法在实时机器人及导航应用中具有显著潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [57] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 综述了基于前馈方法的3D重建与视图合成技术，分类讨论了不同表示架构，并探讨了其应用、数据集及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算复杂且难以实时应用，而深度学习驱动的前馈方法提供了快速且通用的解决方案，推动了3D视觉领域的发展。

Method: 通过分类讨论点云、3D高斯泼溅（3DGS）、神经辐射场（NeRF）等表示架构，分析姿态无关重建、动态3D重建等关键任务。

Result: 总结了前馈方法的优势及其在数字人、SLAM等领域的应用，并提供了数据集和评估协议的详细统计。

Conclusion: 前馈方法在3D视觉领域具有巨大潜力，但仍需解决开放性研究挑战，未来方向值得关注。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [58] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为DCHM的框架，通过深度一致的人体建模和多视图融合，显著减少了噪声，提高了行人检测的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视图行人检测中常引入噪声且精度低，且依赖昂贵的人工标注，难以泛化到多样场景。

Method: 采用深度一致的人体建模（DCHM）框架，结合超像素级高斯泼溅技术，实现多视图深度一致性和精确点云生成。

Result: DCHM显著减少了建模过程中的噪声，性能优于现有方法，并在稀疏视图、大规模和拥挤场景中表现优异。

Conclusion: DCHM无需依赖人工标注，在多视图行人检测中实现了高精度建模和分割，是首个在此类挑战性场景中重建行人的方法。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [59] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 提出ArtiMuse模型和ArtiMuse-10K数据集，解决多模态大语言模型在图像美学评估中的模态偏差和细粒度属性分解不足问题。


<details>
  <summary>Details</summary>
Motivation: 教育、艺术创作和AI生成内容的发展对图像美学评估提出更高要求，现有方法存在模态偏差和缺乏细粒度分析。

Method: 开发ArtiMuse模型，结合评分和专家级理解能力，并构建ArtiMuse-10K数据集，包含10,000张专家标注的图像。

Result: ArtiMuse模型和ArtiMuse-10K数据集将公开，推动图像美学评估领域发展。

Conclusion: ArtiMuse模型和数据集填补了现有方法的不足，为图像美学评估提供了更全面的解决方案。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [60] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 提出一种浏览器扩展，将手语自动翻译为视频会议中的字幕，以帮助听力障碍者与普通人沟通。


<details>
  <summary>Details</summary>
Motivation: 听力障碍者与普通人沟通困难，疫情期间视频会议成为主要沟通方式，听力障碍者更倾向于使用手语而非打字。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展实现手语到字幕的自动翻译。

Result: 通过浏览器扩展实现手语实时翻译，提升听力障碍者在视频会议中的沟通效率。

Conclusion: 该技术有望消除听力障碍者与普通人之间的沟通障碍，尤其在远程交流场景中具有重要价值。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [61] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍了针对ImageCLEFmed MEDVQA 2025挑战赛子任务1的视觉问答（VQA）方法，采用Florence模型作为基础，结合领域增强技术提升泛化能力，实验结果表明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决胃肠道内窥镜视觉问答任务，探索多模态大模型在医学VQA中的应用潜力。

Method: 使用Florence多模态基础模型，结合领域特定的数据增强技术，对KASVIR数据集进行微调。

Result: 在官方挑战指标上表现优异，验证了多模态大模型在医学VQA中的潜力。

Conclusion: 为未来在可解释性、鲁棒性和临床整合方面的研究提供了强基线，代码已开源。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [62] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 研究探讨了人工神经网络（ANN）与人类情感感知的关联，发现ANN分类模糊的刺激同样引发人类感知差异，并通过行为数据优化ANN模型，实现与人类感知模式的对齐。


<details>
  <summary>Details</summary>
Motivation: 情感认知科学的核心挑战是建立外部情感刺激与人类内部体验的准确模型。尽管ANN在面部表情识别中表现优异，但其对人类感知个体差异的建模能力尚未充分探索。

Method: 提出一种新颖的感知边界采样方法，生成位于ANN决策边界的面部表情刺激，构建varEmotion数据集，并通过大规模人类行为实验验证。

Result: 分析显示，ANN分类模糊的刺激同样引发人类感知不确定性，揭示了情感感知的共享计算原则。通过行为数据微调ANN，实现了与人类群体及个体感知模式的匹配。

Conclusion: 研究建立了ANN决策边界与人类感知变异性之间的系统性联系，为情感解释的个性化建模提供了新视角。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [63] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 论文提出了一种相机引导系统，帮助用户识别和去除照片中的杂乱元素，提升照片美学质量。


<details>
  <summary>Details</summary>
Motivation: 照片中的杂乱元素会分散注意力，影响情感或故事的传达，尤其是业余摄影师常因疏忽或经验不足而未能避免。

Method: 系统通过算法评估对象对照片美学和内容的贡献，提供交互式杂乱识别工具，并结合生成对抗网络实现高分辨率图像修复。

Result: 用户研究表明，系统能帮助用户更高效地识别干扰物并拍摄更高质量的照片。

Conclusion: 该系统通过灵活的界面和精准的算法，有效提升了摄影作品的质量和效率。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [64] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D通过自然语言显式编码对象间关系，提升3D场景理解，在多个任务和数据集上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景-语言模型在关系理解上表现不足，尤其是视觉嵌入无法充分表达对象角色和交互。

Method: Descrip3D为每个对象添加文本描述，捕捉其属性和上下文关系，并通过嵌入融合和提示级注入实现双级集成。

Result: 在五个基准数据集上，Descrip3D表现优于基线模型。

Conclusion: 语言引导的关系表示对复杂室内场景理解有效。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [65] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 论文提出LEAD方法，通过建模优化过程和非线性演化，有效预测预训练模型在下游任务中的迁移性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型数量激增，如何高效选择适合下游任务的模型成为挑战，现有方法未能准确捕捉微调动态的非线性特性。

Method: LEAD基于网络输出的logits，提出理论框架建模优化过程，并用ODE描述非线性演化，设计类感知分解方法。

Result: 在24个预训练模型和10个下游数据集上的实验显示LEAD性能优异，适应性强，尤其在低数据场景下。

Conclusion: LEAD通过优化目标对齐和非线性建模，单步解决优化差距，显著提升预训练模型选择效率。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [66] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文比较了生成对抗网络（GANs）、扩散模型和流匹配（FM）技术在T1w到T2w MRI图像转换中的表现，发现GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的对比度图像。

Method: 使用GANs、扩散模型和FM技术进行T1w到T2w的2D MRI图像转换，并在三个公开数据集上评估。

Result: GAN-based Pix2Pix模型表现最佳，扩散和FM方法在小数据集上容易过拟合。

Conclusion: GANs更适合当前MRI图像合成任务，未来研究可探索更多数据以提升FM性能。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [67] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了TensorFlow、PyTorch和JAX三种深度学习框架在血液细胞图像分类中的性能，重点关注推理时间和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在血液图像分析中表现出潜力，但缺乏对不同框架性能的详细分析。

Method: 使用BloodMNIST数据集，比较三种框架在分类任务中的表现，分析推理时间和图像大小的影响。

Result: JAX和PyTorch的分类准确性接近当前基准，性能受图像分辨率和框架优化影响。

Conclusion: JAX和PyTorch在医学图像分类中表现高效，适合实际应用。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [68] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是一种结合无监督分割和开放词汇引导的3D开放词汇子概念发现方法，在场景和用户查询适应方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务目标或场景内容，无法同时满足场景和用户查询的需求。DiSCO-3D旨在填补这一空白。

Method: 基于神经场表示，结合无监督分割和弱开放词汇引导。

Result: 在开放词汇子概念发现中表现优异，同时在开放词汇和无监督分割的边缘案例中达到先进水平。

Conclusion: DiSCO-3D为3D语义分割提供了更灵活的解决方案，适应性强且性能优越。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [69] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph是一个基于图建模的新框架，用于面部表情识别，结合了视觉Transformer和图卷积网络，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互等领域至关重要，但面部属性的结构变化需要被有效捕捉以提高识别效果。

Method: 使用面部关键点作为图的顶点，基于邻近性和局部外观相似性确定边，结合视觉Transformer和图卷积网络捕捉结构依赖关系。

Result: 在Oulu-CASIA、eNTERFACE05和AFEW数据集上分别达到98.09%、79.01%和56.39%的准确率。

Conclusion: Exp-Graph在实验室和真实环境中均表现出强大的泛化能力，适用于实际应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [70] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2是一个高效适配SAM2的框架，通过Depthwise-Dilated Adapter增强多尺度特征提取，适用于医学视频分割与跟踪，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态特定设计，适应性差，且SAM2等模型在医学视频场景中需大规模数据集重新训练，计算成本高。

Method: 提出DD-SAM2框架，结合Depthwise-Dilated Adapter，以最小参数量增强多尺度特征提取，支持小数据微调。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上分别达到Dice分数0.93和0.97。

Conclusion: DD-SAM2首次系统探索了基于适配器的SAM2微调方法，为医学视频分割与跟踪提供了高效解决方案。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [71] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++是一个新型跨模态检测框架，用于识别和解释合成媒体，通过强化学习后训练策略和多阶段训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有单模态检测方法无法应对多模态合成内容。

Method: 采用强化学习后训练策略，结合多阶段训练、思维奖励和混合推理。

Result: BusterX++在性能上有显著提升，并通过GenBuster++基准验证了其有效性。

Conclusion: BusterX++为解决跨模态合成媒体检测提供了有效方案，具有现实应用潜力。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [72] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: 提出了一种基于状态空间模型（SSM）的多光谱特征融合框架MS2Fusion，通过双路径参数交互机制解决现有方法在局部互补特征偏好和计算复杂度上的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多光谱特征融合方法过度关注局部互补特征而忽视跨模态共享语义，且存在感受野大小与计算复杂度的权衡问题。

Method: MS2Fusion采用双路径参数交互机制：一条路径通过跨模态隐藏状态解码挖掘互补信息，另一条路径通过参数共享探索跨模态对齐。两者在SSM中联合优化。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他多光谱目标检测方法，并在RGB-T语义分割和RGBT显著目标检测中取得最优结果。

Conclusion: MS2Fusion通过统一框架实现了功能互补和共享语义空间，具有高效性和通用性。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [73] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个基于AI的框架，用于提升体育裁判的实时决策能力，特别是在跆拳道中实时头部踢击检测和评分。


<details>
  <summary>Details</summary>
Motivation: 传统裁判系统存在延迟、主观性和不一致性，影响公平性和运动员信任。

Method: 结合计算机视觉、深度学习和边缘推理，自动化动作识别与分类。

Result: 显著减少决策时间，提高一致性和透明度。

Conclusion: FST.ai框架具有鲁棒性、可扩展性，适用于多种运动项目。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [74] [DM-RSA: An Extension of RSA with Dual Modulus](https://arxiv.org/abs/2507.14197)
*Andriamifidisoa Ramamonjy,Rufine Marius Lalasoa*

Main category: cs.CR

TL;DR: DM-RSA是一种RSA变体，使用双模数增强安全性，通过中国剩余定理（CRT）提高抗侧信道攻击能力，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 增强RSA的安全性，特别是抗侧信道攻击和模数部分泄露的能力。

Method: 采用双模数设计，利用CRT进行解密。

Result: 提高了安全性，同时保持了与经典RSA相当的效率，易于集成到现有基础设施中。

Conclusion: DM-RSA是一种安全且高效的RSA改进方案。

Abstract: We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem
that employs two distinct moduli symmetrically to enhance security. By
leveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides
increased robustness against side-channel attacks while preserving the
efficiency of classical RSA. This approach improves resistance to partial
compromise of a modulus and integrates easily into existing infrastructures.

</details>


### [75] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench是首个用于评估LLM代理在网络安全威胁调查任务中的基准，基于专家构建的调查图生成问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的安全分析师需要处理大量异构警报和日志，构建自动化的LLM代理是一个有前景的方向。

Method: 通过Azure租户构建数据集，模拟真实攻击，生成调查图和问题，利用LLM自动生成任务和答案。

Result: 实验显示任务难度较高，最佳模型得分仅为0.368，表明未来研究空间大。

Conclusion: ExCyTIn-Bench为LLM代理的开发与评估提供了可扩展的基准，代码和数据即将发布。

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [76] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: 提出了一种无需PRM的安全对齐框架，通过自动化红队测试和对抗训练提升LLM的安全性，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于PRM的安全对齐方法计算开销大且扩展性受限，亟需高效且可扩展的替代方案。

Method: 采用自动化红队测试（如遗传算法优化、多智能体模拟和高级提示变异技术）和对抗训练（课程学习和自适应正则化）来增强模型鲁棒性。

Result: 在五种先进LLM上验证，性能优于PRM方法，计算成本降低61%。

Conclusion: 该框架为资源受限组织提供了高效安全对齐方案，并为应对持续演变的对抗威胁奠定了基础。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [77] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: 研究探讨了K-12教育中大型语言模型（LLMs）的安全漏洞，学生可能通过特洛伊化提示绕过内容审核系统。实验揭示了GPT-3.5和GPT-4的脆弱性，并提出了检测工具TrojanPromptGuard（TPG）。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在教育中的潜在风险，特别是学生如何利用提示绕过安全机制。

Method: 通过模拟K-12查询和多轮对话的系统实验，分析LLMs的漏洞。

Result: 揭示了GPT-3.5和GPT-4的关键脆弱性，并开发了原型工具TPG以检测和缓解特洛伊化提示。

Conclusion: 研究为AI安全研究人员和教育技术专家提供了关于LLMs安全部署的见解。

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [78] [Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks](https://arxiv.org/abs/2507.14212)
*Federico Mason,Federico Chiariotti,Pietro Talli,Andrea Zanella*

Main category: cs.CR

TL;DR: 目标导向通信（GoC）通过减少传输频率优化通信，但存在基于时间的侧信道攻击风险。本文研究针对GoC的窃听攻击，提出理论框架和防御措施，平衡性能与信息泄露。


<details>
  <summary>Details</summary>
Motivation: GoC虽能优化通信效率，但其调度机制可能被窃听者利用，通过时间信息推断系统状态，绕过传统安全措施。

Method: 研究针对基于拉取的目标导向调度的窃听攻击，提出理论框架评估攻击效果，并设计两种启发式防御策略。

Result: 实验显示，原始GoC调度导致窃听者60%准确率，而防御策略可减少50%信息泄露，同时保留GoC性能优势。

Conclusion: 通过启发式防御策略，可在保持GoC性能的同时显著降低信息泄露风险。

Abstract: Goal-oriented Communication (GoC) is a new paradigm that plans data
transmission to occur only when it is instrumental for the receiver to achieve
a certain goal. This leads to the advantage of reducing the frequency of
transmissions significantly while maintaining adherence to the receiver's
objectives. However, GoC scheduling also opens a timing-based side channel that
an eavesdropper can exploit to obtain information about the state of the
system. This type of attack sidesteps even information-theoretic security, as
it exploits the timing of updates rather than their content. In this work, we
study such an eavesdropping attack against pull-based goal-oriented scheduling
for remote monitoring and control of Markov processes. We provide a theoretical
framework for defining the effectiveness of the attack and propose possible
countermeasures, including two practical heuristics that provide a balance
between the performance gains offered by GoC and the amount of leaked
information. Our results show that, while a naive goal-oriented scheduler
allows the eavesdropper to correctly guess the system state about 60% of the
time, our heuristic defenses can halve the leakage with a marginal reduction of
the benefits of goal-oriented approaches.

</details>


### [79] [Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level](https://arxiv.org/abs/2507.14213)
*Irena Spasojevic,Federica Celegato,Alessandro Magni,Paola Tiberto,Jordi Sort*

Main category: cs.CR

TL;DR: 提出了一种基于磁离子策略的硬件级安全技术，通过电压控制氮离子迁移生成可调谐的磁性指纹，用于抗黑客和防伪。


<details>
  <summary>Details</summary>
Motivation: 大数据时代对高效、安全的硬件需求增加，传统加密方案资源密集且易受攻击，需要新材料和设计来增强安全性。

Method: 利用电压控制氮离子在FeCoN点中的迁移，生成可调谐的磁性亚层，实现确定性或概率性状态，用于磁性指纹识别。

Result: 该方法实现了自保护原语（如真随机数生成器、物理不可克隆函数等），具有抗篡改、低能耗和可扩展性。

Conclusion: 这种可重构架构为基于磁性现象的下一代硬件安全提供了重要突破。

Abstract: The Big Data revolution has heightened the demand for robust,
energy-efficient security hardware capable of withstanding increasingly
sophisticated cyber threats. Conventional encryption schemes, reliant on
complex algorithms, are resource-intensive and remain vulnerable. To fortify
sensitive information, society needs innovative anti-hacking and
anti-counterfeiting technologies that exploit new materials and designs. Here,
we present a magneto-ionic strategy for hardware-level security based on fully
selective voltage-controlled N3- ion migration within pre-defined, initially
paramagnetic FeCoN dots. This process generates ferromagnetic sublayers of
tuneable thickness, resulting in either deterministic (single-domain or vortex)
or probabilistic states (with coexisting magnetic configurations and
voltage-adjustable probabilities), each exhibiting stochastic orientation and
chirality, thereby providing a rich platform for magnetic fingerprinting. This
approach enables self-protected primitives, including true random number
generators, physical unclonable functions, and in-memory probabilistic
inference. The resulting reconfigurable architecture combines tamper
resistance, low energy consumption, and scalability, marking a significant leap
toward next-generation hardware security rooted in emergent magnetic phenomena.

</details>


### [80] [GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics](https://arxiv.org/abs/2507.14222)
*Shu-Ting Huang,Wen-Cheng Chung,Hao-Ting Pai*

Main category: cs.CR

TL;DR: IG-GPU是一种基于GPU的PyTorch重构方法，显著提升了IG机制在入侵检测中的性能，实现了116倍的速度提升，并在大规模数据集上保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 解决IG机制在CPU上运行时因立方时间复杂度和大中间位集导致的全规模数据集处理不切实际的问题。

Method: 通过PyTorch重构IG机制，将所有成对交集和子集评估任务卸载到GPU上。

Result: 在NSL-KDD数据集上，IG-GPU实现了116倍的速度提升，并在全规模数据集上保持了高召回率、精确率和AUC值。

Conclusion: IG-GPU填补了严格可解释性与实时网络防御之间的鸿沟，为未来硬件感知调度和多GPU分片提供了便携基础。

Abstract: The Interpretable Generalization (IG) mechanism recently published in IEEE
Transactions on Information Forensics and Security delivers state-of-the-art,
evidence-based intrusion detection by discovering coherent normal and attack
patterns through exhaustive intersect-and-subset operations-yet its cubic-time
complexity and large intermediate bitsets render full-scale datasets
impractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads
all pairwise intersections and subset evaluations to commodity GPUs.
Implemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,
IG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.
In the full size of NSL-KDD (148k-record), given small training data (e.g.,
10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,
Precision 0.973, and AUC 0.961, whereas IG required down-sampling to
15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision
0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales
and could provide millisecond-level per-flow inference once patterns are
learned. IG-GPU thus bridges the gap between rigorous interpretability and
real-time cyber-defense, offering a portable foundation for future work on
hardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity
optimizations.

</details>


### [81] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: 论文提出了一种可解释的入侵检测系统（IDS）方法，通过Interpretable Generalization（IG）机制学习清晰的特征组合规则，并引入Multi-Granular Discretization（IG-MD）进一步提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI（XAI）方法通常仅在分类器上附加解释器，导致分析结果不完整或误导。

Method: IG机制学习独特的特征组合规则，IG-MD通过多粒度离散化处理连续特征。

Result: 在多个数据集上表现优异，IG-MD在UKM-IDS20上提升精度4个百分点，召回率保持1.0。

Conclusion: IG-MD证明了单一可解释模型可跨领域扩展，无需定制调整。

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [82] [Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently](https://arxiv.org/abs/2507.14229)
*Vanja Stojanović,Žiga Lesar,CIril Bohak*

Main category: cs.CR

TL;DR: 本文提出了一种混合神经网络架构，结合模算术感知和统计特征学习，用于分析仿射密码。实验表明，该模型在短到中等长度密文上表现优异，但对长密文效果下降。


<details>
  <summary>Details</summary>
Motivation: 受可解释神经网络在模算术和经典密码神经密码分析中的进展启发，旨在提升仿射密码的密钥恢复准确性。

Method: 采用混合架构，包含处理原始密文的模算术分支和利用字母频率特征的统计分支。

Result: 在自然英语文本数据集上，模型对短到中等长度密文实现高密钥恢复准确率，优于纯统计方法。

Conclusion: 混合模型在短到中等长度密文上表现优异，但对长密文的泛化能力有限，需进一步优化。

Abstract: We investigate the cryptanalysis of affine ciphers using a hybrid neural
network architecture that combines modular arithmetic-aware and statistical
feature-based learning. Inspired by recent advances in interpretable neural
networks for modular arithmetic and neural cryptanalysis of classical ciphers,
our approach integrates a modular branch that processes raw ciphertext
sequences and a statistical branch that leverages letter frequency features.
Experiments on datasets derived from natural English text demonstrate that the
hybrid model attains high key recovery accuracy for short and moderate
ciphertexts, outperforming purely statistical approaches for the affine cipher.
However, performance degrades for very long ciphertexts, highlighting
challenges in model generalization.

</details>


### [83] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: 本文提出了一种名为AdViT的攻击方法，能够同时欺骗视觉Transformer模型及其解释模型，攻击成功率达100%，且在黑白盒场景下均生成准确解释。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉Transformer模型结合解释模型被视为安全，但现有攻击研究未考虑对解释模型的影响，本文旨在填补这一空白。

Method: 提出AdViT攻击方法，生成能同时误导Transformer模型及其解释模型的对抗样本。

Result: 实验显示AdViT在黑白盒场景下攻击成功率达100%，且生成的对抗样本解释准确，难以检测。

Conclusion: AdViT揭示了Transformer模型即使结合解释模型仍存在漏洞，需进一步研究防御方法。

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [84] [Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems](https://arxiv.org/abs/2507.14324)
*Yao Ma,Wen Yu Kon,Jefferson Chu,Kevin Han Yong Loh,Kaushik Chakraborty,Charles Lim*

Main category: cs.CR

TL;DR: 该论文改进了基于图着色的相对零知识证明（RZKP）协议，降低了相对约束条件（从60米到30米），并增强了协议的稳定性和可扩展性。同时，针对长期安全性，提出了改进协议并扩展了多验证者配置。


<details>
  <summary>Details</summary>
Motivation: 当前基于密码/PIN的身份验证方法易受钓鱼攻击，需要更安全的解决方案。

Method: 采用基于图着色的相对零知识证明（RZKP）协议，优化工程实现并扩展多验证者配置。

Result: 成功降低了相对约束条件，提升了协议稳定性和可扩展性，并证明了多验证者配置的安全性。

Conclusion: 该研究为身份验证提供了更安全、高效的解决方案，适用于近长期应用场景。

Abstract: Identity verification is the process of confirming an individual's claimed
identity, which is essential in sectors like finance, healthcare, and online
services to ensure security and prevent fraud. However, current
password/PIN-based identity solutions are susceptible to phishing or skimming
attacks, where malicious intermediaries attempt to steal credentials using fake
identification portals. Alikhani et al. [Nature, 2021] began exploring identity
verification through graph coloring-based relativistic zero-knowledge proofs
(RZKPs), a key cryptographic primitive that enables a prover to demonstrate
knowledge of secret credentials to a verifier without disclosing any
information about the secret. Our work advances this field and addresses
unresolved issues: From an engineering perspective, we relax further the
relativistic constraints from 60m to 30m, and significantly enhance the
stability and scalability of the experimental demonstration of the 2-prover
graph coloring-based RZKP protocol for near-term use cases. At the same time,
for long-term security against entangled malicious provers, we propose a
modified protocol with comparable computation and communication costs, we
establish an upper bound on the soundness parameter for this modified protocol.
On the other hand, we extend the two-prover, two-verifier setup to a
three-prover configuration, demonstrating the security of such relativistic
protocols against entangled malicious provers.

</details>


### [85] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: 本文综述了隐私保护机器学习（PPML）的研究进展，重点关注跨层级优化，包括协议、模型和系统层级，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: PPML虽然能保护用户数据隐私，但效率低且扩展性差，因此需要优化以缩小与明文机器学习的差距。

Method: 通过分类和比较现有研究，从协议、模型和系统三个层级进行综述，并提供技术见解。

Result: 总结了PPML的研究进展，提出了跨层级优化的必要性，并提供了公开的GitHub仓库以跟踪最新发展。

Conclusion: PPML领域需要进一步整合跨层级优化，本文为未来研究提供了方向，并希望激发突破性进展。

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [86] [FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum](https://arxiv.org/abs/2507.14588)
*Usayd Shahul,J. Harshan*

Main category: cs.CR

TL;DR: FORTA是一个在实数域操作的拜占庭弹性安全聚合框架，利用DFT编码保护隐私，并通过改进的Krum方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于有限域算术的安全聚合方法在处理实数模型更新时可能引入数值错误和溢出，因此需要直接在实数域操作的安全聚合方法。

Method: FORTA结合DFT编码和反馈改进的Krum异常检测方法，直接在实数域实现安全聚合。

Result: 理论分析和实验表明，改进的Krum方法比标准Krum具有更高的鲁棒性和聚合准确性。

Conclusion: FORTA在实数域实现了高效且鲁棒的安全聚合，解决了数值误差问题。

Abstract: Secure federated learning enables collaborative model training across
decentralized users while preserving data privacy. A key component is secure
aggregation, which keeps individual updates hidden from both the server and
users, while also defending against Byzantine users who corrupt the
aggregation. To this end, Jinhyun So et al. recently developed a
Byzantine-resilient secure aggregation scheme using a secret-sharing strategy
over finite-field arithmetic. However, such an approach can suffer from
numerical errors and overflows when applied to real-valued model updates,
motivating the need for secure aggregation methods that operate directly over
the real domain. We propose FORTA, a Byzantine-resilient secure aggregation
framework that operates entirely in the real domain. FORTA leverages Discrete
Fourier Transform (DFT) codes for privacy and employs Krum-based outlier
detection for robustness. While DFT decoder is error-free under infinite
precision, finite precision introduces numerical perturbations that can distort
distance estimates and allow malicious updates to evade detection. To address
this, FORTA refines Krum using feedback from DFT decoder, improving the
selection of trustworthy updates. Theoretical analysis and experiments show
that our modification of Krum offers improved robustness and more accurate
aggregation than standard Krum.

</details>


### [87] [Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords](https://arxiv.org/abs/2507.14600)
*MA. Khajeian*

Main category: cs.CR

TL;DR: 提出了一种结合彩虹表和量子搜索的混合攻击框架，用于高效破解长且不规则的人类生成密码。


<details>
  <summary>Details</summary>
Motivation: 长且不规则的人类生成密码对经典和量子攻击都构成挑战，需要更高效的破解方法。

Method: 使用基于字典的密码生成和转换规则构建彩虹表，并通过分桶优化查找效率；采用分布式精确Grover算法进行量子搜索。

Result: 提出的混合框架降低了量子电路的深度，提高了抗噪声能力，适用于近期量子设备。

Conclusion: 结合彩虹表和高效量子搜索的混合框架显著提升了密码恢复效率。

Abstract: Passwords that are long and human-generated pose a challenge for both
classical and quantum attacks due to their irregular structure and large search
space. In this work, we present an enhanced classical-quantum hybrid attack
tailored to this scenario. We build rainbow tables using dictionary-based
password generation with transformation rules to better model real user
behavior. These tables are then organized into buckets, enabling faster lookup
and reduced space complexity. To perform quantum search within each bucket, we
use a distributed exact variant of Grover's algorithm, which offers lower
circuit depth and deterministic success. As a result, the overall quantum
circuit is shallower and more robust against noise, particularly from
depolarizing channels commonly found in near-term quantum devices. Through this
work, Overall, we propose a hybrid framework that combines structured rainbow
tables with efficient quantum search to enhance password recovery.

</details>


### [88] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: VTarbel是一种针对垂直联邦学习（VFL）的两阶段攻击框架，能够绕过检测器并实现目标标签攻击。


<details>
  <summary>Details</summary>
Motivation: 现有VFL安全研究多关注隐私漏洞，而针对标签攻击的研究不足，且现有方法假设不现实。

Method: VTarbel通过两阶段攻击：准备阶段选择高表现样本训练本地模型，攻击阶段生成对抗样本以绕过检测。

Result: VTarbel在多种模型、数据集和检测器下均优于现有方法，并能抵御隐私保护防御。

Conclusion: VFL当前部署存在安全盲点，亟需攻击感知的防御机制。

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [89] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: VMask是一种新型标签隐私保护框架，通过层掩码技术防御模型完成攻击，实现了隐私与效用的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习（VFL）易受标签推断攻击，现有防御方法要么牺牲模型准确性，要么计算开销过大。

Method: 采用秘密共享（SS）技术掩码攻击者模型的关键层参数，破坏输入数据与中间输出的强相关性。

Result: VMask成功将标签推断准确率降至随机猜测水平，模型性能损失极小（如Transformer模型准确率仅下降0.09%），运行速度远超加密方法。

Conclusion: VMask是首个提供可调隐私预算的框架，在高效防御攻击的同时保持了模型性能，适用于多模态数据集。

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [90] [CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus](https://arxiv.org/abs/2507.14739)
*Franco Oberti,Stefano Di Carlo,Alessandro Savino*

Main category: cs.CR

TL;DR: 本文提出了一种基于硬件性能计数器（HPCs）的新型入侵检测系统（IDS），用于提升CAN协议的安全性，通过模拟RISC-V CAN接收器并优化HPC特征，显著提高了检测效率。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏内置安全功能，随着自动驾驶车辆的普及，其面临的安全威胁日益严重，传统安全措施效果有限。

Method: 使用gem5模拟器模拟RISC-V CAN接收器，通过AES-128加密处理CAN帧载荷，并利用HPCs检测异常行为。通过数据提取和相关性分析优化HPC特征。

Result: 结果表明，该方法显著提升了CAN协议的安全性，有效应对了汽车网络安全的新挑战。

Conclusion: 基于HPCs的IDS为CAN环境提供了一种高效的安全解决方案，具有实际应用潜力。

Abstract: The Controller Area Network (CAN) protocol, essential for automotive embedded
systems, lacks inherent security features, making it vulnerable to cyber
threats, especially with the rise of autonomous vehicles. Traditional security
measures offer limited protection, such as payload encryption and message
authentication. This paper presents a novel Intrusion Detection System (IDS)
designed for the CAN environment, utilizing Hardware Performance Counters
(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN
receiver is simulated using the gem5 simulator, processing CAN frame payloads
with AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC
responses. Key HPC features are optimized through data extraction and
correlation analysis to enhance classification efficiency. Results indicate
that this approach could significantly improve CAN security and address
emerging challenges in automotive cybersecurity.

</details>


### [91] [Careful Whisper: Attestation for peer-to-peer Confidential Computing networks](https://arxiv.org/abs/2507.14796)
*Ceren Kocaoğullar,Gustavo Petri,Dominic P. Mulligan,Derek Miller,Hugo J. M. Vincent,Shale Xiong,Alastair R. Beresford*

Main category: cs.CR

TL;DR: Careful Whisper协议通过基于gossip的方法高效传播信任，减少验证开销至线性复杂度，适用于动态网络环境。


<details>
  <summary>Details</summary>
Motivation: 解决TEE在点对点网络中直接验证导致的二次通信开销问题，适应节点频繁变化的动态环境。

Method: 提出Careful Whisper协议，利用gossip机制传播信任，支持异构网络中的传递信任和离线节点验证。

Result: 协议在200节点网络中每轮仅发送21.5 KiB数据，耗时0.158秒，且对验证失败具有强鲁棒性。

Conclusion: Careful Whisper协议高效、资源节约且适应性强，适用于动态网络中的信任传播。

Abstract: Trusted Execution Environments (TEEs) are designed to protect the privacy and
integrity of data in use. They enable secure data processing and sharing in
peer-to-peer networks, such as vehicular ad hoc networks of autonomous
vehicles, without compromising confidentiality. In these networks, nodes must
establish mutual trust to collaborate securely. TEEs can achieve this through
remote attestation, where a prover presents evidence of its trustworthiness to
a verifier, which then decides whether or not to trust the prover. However, a
naive peer-to-peer attestation approach, where every TEE directly attests every
other TEE, results in quadratic communication overhead. This is inefficient in
dynamic environments, where nodes frequently join and leave the network.
  To address this, we present Careful Whisper, a gossip-based protocol that
disseminates trust efficiently, reducing attestation overhead to linear
complexity under ideal conditions. It enables interoperability by enabling
transitive trust across heterogeneous networks, and supports trust
establishment with offline nodes via relayed attestations. Using a custom
discrete-event simulator, we show that Careful Whisper propagates trust both
faster and more widely than naive approaches across various network topologies.
Our results demonstrate that our protocol is resource efficient, sending ~21.5
KiB and requiring 0.158 seconds per round in a 200-node network, and that our
protocol is resilient to attestation failures across various network
topologies.

</details>


### [92] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: LLM-based web navigation agents are vulnerable to Indirect Prompt Injection (IPI) attacks, where adversaries can hijack agent behavior via HTML triggers, leading to security risks like credential theft.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in LLM-based web navigation agents, particularly their susceptibility to IPI attacks, and highlight the need for stronger defenses.

Method: Utilized the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1 to demonstrate IPI attacks on real websites.

Result: High success rates in targeted and general attacks, including credential exfiltration and forced ad clicks.

Conclusion: LLM-driven web agents pose significant security risks, necessitating improved defenses as their adoption grows.

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [93] [Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies](https://arxiv.org/abs/2507.14822)
*Zeeshan Kaleem,Misha Urooj Khan,Ahmad Suleman,Waqas Khalid,Kai-Kit Wong,Chau Yuen*

Main category: cs.CR

TL;DR: 论文提出了一种名为Quantum Skyshield的量子安全架构，用于解决低空无线网络（LAWN）中的安全挑战，结合了BB84量子密钥分发和后量子认证机制。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和高空平台的密集部署，低空无线网络的安全问题日益突出，尤其是自由空间光通信（FSO）易受拦截、湍流和天气影响。

Method: 采用BB84量子密钥分发（QKD）和后量子认证机制（如Lamport一次性签名和HMAC），并引入Grover启发的威胁检测机制。

Result: 仿真结果显示，在量子比特错误率低于11%时能可靠生成128位对称密钥，威胁检测机制单次迭代识别异常概率达89%。

Conclusion: Quantum Skyshield为LAWN提供了可靠的安全解决方案，未来研究需进一步优化和扩展该架构。

Abstract: Recently, low-altitude wireless networks (LAWNs) have emerged as a critical
backbone for supporting the low-altitude economy, particularly with the
densification of unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs). To meet growing data demands, some LAWN deployments incorporate
free-space optical (FSO) links, which offer exceptional bandwidth and beam
directivity. However, without strong security measures in place, both
conventional radio frequency channels and FSO beams remain vulnerable to
interception and spoofing and FSO in particular can suffer from turbulence,
misalignment, and weather-related attenuation. To address these challenges in
the quantum era, a quantum-secure architecture called Quantum Skyshield is
proposed to enable reliable communication between the base transceiver station
(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution
(QKD) with post-quantum authentication mechanisms. Simulation results confirm
the reliable generation of a 128-bit symmetric key when the quantum bit error
rate (QBER) remains below the threshold of 11%. Authentication is enforced
using Lamport one-time signatures and hash-based message authentication codes
(HMAC) to ensure message integrity. A Grover-inspired threat detection
mechanism identifies anomalies with up to 89% probability in a single
iteration, enabling real-time trust evaluation. Lastly, future research
challenges have also been identified and discussed to guide further development
in this area.

</details>


### [94] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: 本文探讨了如何将混合同态加密（HHE）与联邦学习（FL）结合，以解决通信和隐私问题，实现可扩展且安全的去中心化学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感领域具有潜力，但面临通信开销和数据隐私的挑战。现有的隐私保护技术（如全同态加密）虽有效但计算和通信成本高，限制了实际应用。

Method: 提出将混合同态加密（HHE）与联邦学习结合，HHE结合了对称加密和全同态加密的优势。

Result: 通过HHE与FL的结合，能够有效降低通信和计算成本，同时保障数据隐私。

Conclusion: HHE与FL的整合为解决联邦学习中的通信和隐私问题提供了可行方案，推动了可扩展且安全的去中心化学习系统的发展。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [95] [A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies](https://arxiv.org/abs/2507.14893)
*Farzin Renan*

Main category: cs.CR

TL;DR: 论文提出了一种基于同源的新型强指定验证者签名方案（CSI-SDVS），具有紧凑的密钥和签名大小，且能抵抗量子攻击。


<details>
  <summary>Details</summary>
Motivation: 隐私敏感应用（如电子投票和数字现金）需要更严格的验证模型以确保机密性和控制，而现有SDVS方案易受量子攻击且密钥和签名较大。

Method: 基于CSIDH的理想类群动作框架和CSI-FiSh的签名技术，依赖于多目标群动作逆问题（MT-GAIP）的困难性。

Result: CSI-SDVS实现了强安全性（SUF-CMA、NT、PSI），密钥和签名大小为O(λ)，优于现有后量子SDVS方案的O(λ²)。

Conclusion: CSI-SDVS是目前最紧凑的后量子SDVS方案之一，也是唯一基于同源的后量子安全构造。

Abstract: Digital signatures are essential cryptographic tools that provide
authentication and integrity in digital communications. However,
privacy-sensitive applications, such as e-voting and digital cash, require more
restrictive verification models to ensure confidentiality and control. Strong
Designated Verifier Signature (SDVS) schemes address this need by enabling the
signer to designate a specific verifier, ensuring that only this party can
validate the signature. Existing SDVS constructions are primarily based on
number-theoretic assumptions and are therefore vulnerable to quantum attacks.
Although post-quantum alternatives, particularly those based on lattices, have
been proposed, they often entail large key and signature sizes. In this work,
we introduce $\mathsf{CSI\text{-}SDVS}$, a novel isogeny-based SDVS scheme that
offers a compact, quantum-resistant alternative. Our construction builds on the
ideal class group action framework of CSIDH and the signature techniques of
CSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse
Problem (MT-GAIP). $\mathsf{CSI\text{-}SDVS}$ achieves strong security
guarantees; namely, Strong Unforgeability under Chosen-Message Attacks
(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in
the random oracle model. Remarkably, both the keys and signatures in
$\mathsf{CSI\text{-}SDVS}$ are of size $\mathcal{O}(\lambda)$, representing a
significant improvement over the typical $\mathcal{O}(\lambda^2)$ bounds in
existing post-quantum SDVS schemes, thereby making it among the most compact
PQC-based SDVS schemes and the only post-quantum secure construction based on
isogenies.

</details>


### [96] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 论文系统回顾了2013-2024年间关于元宇宙安全与隐私的研究，总结了方法、重点领域及研究趋势，并指出仍需解决的挑战。


<details>
  <summary>Details</summary>
Motivation: 元宇宙技术的快速发展带来了新的安全与隐私挑战，需要系统分析现有研究以指导未来方向。

Method: 通过文献综述，按方法、安全隐私属性、沉浸式组件和评估策略对研究进行分类。

Result: 过去五年研究激增，重点关注用户中心方法，但政策合规、可访问性等领域仍存空白。

Conclusion: 元宇宙安全需结合技术与人文因素，呼吁跨学科合作以构建可信的沉浸式环境。

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [97] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz是一个框架，通过结合大型语言模型（LLM）和轻量级工具链，自动化分析闭源库的二进制文件，减少模糊测试的成本。


<details>
  <summary>Details</summary>
Motivation: 解决闭源和专有软件中模糊测试的高成本和复杂性。

Method: 使用LLM和工具链（反汇编器/编译器/模糊测试器）自主分析二进制文件，生成驱动程序并自我修复错误。

Result: 在四个Linux库上测试，覆盖100%的API函数，75.52%的驱动程序首次执行正确。

Conclusion: LLM增强的中间件有望降低黑盒组件的模糊测试成本，为未来研究奠定基础。

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [98] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: PromptArmor是一种简单有效的防御方法，用于检测和移除LLM代理中的恶意提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLM代理易受提示注入攻击，导致执行恶意任务而非用户意图。

Method: PromptArmor利用现成LLM检测并移除输入中的潜在注入提示。

Result: 在AgentDojo基准测试中，PromptArmor的误报率和漏报率均低于1%，攻击成功率降至1%以下。

Conclusion: PromptArmor可作为评估新防御方法的标准基线。

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [99] [The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head](https://arxiv.org/abs/2507.15377)
*Magali Bardet,Charles Brion,Philippe Gaborit,Mercedes Haiech,Romaric Neveu*

Main category: cs.CR

TL;DR: 论文提出了两个新的矩阵码等价问题，并基于MPCitH范式构建了一个签名方案，其签名和公钥尺寸优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 在矩阵码领域，仅研究了码等价问题，而子码等价问题在汉明度量中已有定义。本文旨在填补这一空白，并构建更高效的签名方案。

Method: 引入矩阵子码等价问题和矩阵码置换核问题，应用MPCitH范式构建签名方案，并分析算法的复杂性。

Result: 新方案的签名尺寸约为4800字节，公钥尺寸约为275字节，优于SPHINCS+、MEDS和CROSS等现有方案。

Conclusion: 通过依赖新的难题，该签名方案在性能和多样性上为后量子签名领域提供了新的选择。

Abstract: Nowadays, equivalence problems are widely used in cryptography, most notably
to establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as
the most recent ones. However, in the context of matrix codes, only the code
equivalence problem has been studied, while the subcode equivalence is
well-defined in the Hamming metric. In this work, we introduce two new
problems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted
Kernel Problem, to which we apply the MPCitH paradigm to build a signature
scheme. These new problems, closely related to the Matrix Code Equivalence
problem, ask to find an isometry given a code $C$ and a subcode $D$.
Furthermore, we prove that the Matrix Subcode Equivalence problem reduces to
the Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus
introducing the matrix code version of the Permuted Kernel Problem. We also
adapt the combinatorial and algebraic algorithms for the Matrix Code
Equivalence problem to the subcode case, and we analyze their complexities. We
find with this analysis that the algorithms perform much worse than in the code
equivalence case, which is the same as what happens in the Hamming metric.
Finally, our analysis of the attacks allows us to take parameters much smaller
than in the Matrix Code Equivalence case. Coupled with the effectiveness of
\textit{Threshold-Computation-in-the-Head} or \textit{VOLE-in-the-Head}, we
obtain a signature size of $\approx$ 4 800 Bytes, with a public key of
$\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings
diversity in the landscape of post-quantum signature schemes, by relying on a
new hard problem. In particular, this new signature scheme performs better than
SPHINCS+, with a smaller size of public key + signature. Our signature compares
also well with other signature schemes: compared to MEDS, the signature is
smaller, and we reduced the size of the sum of signature and public key by a
factor close to 5. We also obtain a signature size that is almost half the size
of the CROSS signature scheme.

</details>


### [100] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: 论文提出PiMRef，一种基于知识不变量的钓鱼邮件检测方法，通过验证发件人身份的真实性和检测可疑行为，有效对抗由大型语言模型生成的钓鱼邮件。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则和特征工程的钓鱼邮件检测方法难以应对攻击者利用大型语言模型生成的高说服力钓鱼邮件，亟需新的防御手段。

Method: PiMRef将钓鱼检测重构为身份事实核查任务，通过提取发件人身份、验证域名合法性及检测用户互动提示来识别钓鱼邮件。

Result: PiMRef在标准基准测试中提升精度8.8%，在真实世界评估中达到92.1%的精度和87.9%的召回率，运行效率高。

Conclusion: PiMRef通过知识基不变量的方法，显著提升了钓鱼邮件检测的精度和效率，为防御高级钓鱼攻击提供了有效工具。

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [101] [PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15419)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: 提出PhishIntentionLLM框架，利用多代理检索增强生成（RAG）技术从网站截图识别钓鱼意图，显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注钓鱼网站检测，而对恶意意图的识别研究不足。

Method: 基于大型语言模型（LLMs）的视觉-语言能力，设计多代理RAG框架，识别四种钓鱼目标。

Result: 在GPT-4o上达到0.7895的微精度，比单代理基线提升约95%。

Conclusion: 提供了一种可扩展且可解释的钓鱼意图分析解决方案。

Abstract: Phishing websites remain a major cybersecurity threat, yet existing methods
primarily focus on detection, while the recognition of underlying malicious
intentions remains largely unexplored. To address this gap, we propose
PhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework
that uncovers phishing intentions from website screenshots. Leveraging the
visual-language capabilities of large language models (LLMs), our framework
identifies four key phishing objectives: Credential Theft, Financial Fraud,
Malware Distribution, and Personal Information Harvesting. We construct and
release the first phishing intention ground truth dataset (~2K samples) and
evaluate the framework using four commercial LLMs. Experimental results show
that PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and
significantly outperforms the single-agent baseline with a ~95% improvement in
micro-precision. Compared to the previous work, it achieves 0.8545 precision
for credential theft, marking a ~4% improvement. Additionally, we generate a
larger dataset of ~9K samples for large-scale phishing intention profiling
across sectors. This work provides a scalable and interpretable solution for
intention-aware phishing analysis.

</details>


### [102] [Cryptanalysis of a multivariate CCZ scheme](https://arxiv.org/abs/2507.15449)
*Alessio Caminata,Elisa Gorla,Madison Mabe,Martina Vigorito,Irene Villa*

Main category: cs.CR

TL;DR: 论文分析了Pesto方案，发现其CCZ变换后的4次多项式系统可高效降为2次，质疑其安全性提升。


<details>
  <summary>Details</summary>
Motivation: 探讨Pesto方案中CCZ变换是否显著提升安全性。

Method: 将CCZ变换后的4次多项式系统降为2次系统。

Result: 发现降阶效率高，CCZ变换可能未显著增强安全性。

Conclusion: CCZ变换在Pesto方案中的安全性提升效果存疑。

Abstract: We consider the multivariate scheme Pesto, which was introduced by Calderini,
Caminata, and Villa. In this scheme, the public polynomials are obtained by
applying a CCZ transformation to a set of quadratic secret polynomials. As a
consequence, the public key consists of polynomials of degree 4. In this work,
we show that the public degree 4 polynomial system can be efficiently reduced
to a system of quadratic polynomials. This seems to suggest that the CCZ
transformation may not offer a significant increase in security, contrary to
what was initially believed.

</details>


### [103] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: 论文研究了企业环境中大型语言模型（LLM）面临的多阶段提示推理攻击，提出了威胁模型和防御措施。


<details>
  <summary>Details</summary>
Motivation: 企业部署的LLM面临新型安全威胁，如通过多阶段提示推理攻击逐步提取机密数据。

Method: 通过模拟攻击场景、概率理论、优化框架和信息泄漏边界分析多阶段攻击，并提出防御措施。

Result: 攻击能可靠地提取敏感信息，提出的防御措施（如异常检测、访问控制等）有效降低攻击成功率。

Conclusion: 企业LLM安全需从单轮提示过滤转向多阶段攻击与防御的整体视角。

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


### [104] [Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls](https://arxiv.org/abs/2507.15660)
*Rohit Negi,Amit Negi,Manish Sharma,S. Venkatesan,Prem Kumar,Sandeep K. Shukla*

Main category: cs.CR

TL;DR: 论文探讨了大型活动（如奥运会、世界杯等）数字化基础设施面临的网络安全威胁，并以MahaKumbh 2025为例，介绍了其网络安全评估和风险管理方法。


<details>
  <summary>Details</summary>
Motivation: 大型活动的数字化基础设施通常是临时构建的，容易成为黑客攻击目标，因此需要独特的网络安全方法。

Method: 作为网络安全评估和风险管理团队，作者详细描述了MahaKumbh 2025的安全评估范围、流程和方法。

Result: 在45天的活动中，所有网络攻击均未成功。

Conclusion: 论文总结了方法论，并讨论了未来类似大型活动的改进方向。

Abstract: Mega events such as the Olympics, World Cup tournaments, G-20 Summit,
religious events such as MahaKumbh are increasingly digitalized. From event
ticketing, vendor booth or lodging reservations, sanitation, event scheduling,
customer service, crime reporting, media streaming and messaging on digital
display boards, surveillance, crowd control, traffic control and many other
services are based on mobile and web applications, wired and wireless
networking, network of Closed-Circuit Television (CCTV) cameras, specialized
control room with network and video-feed monitoring. Consequently, cyber
threats directed at such digital infrastructure are common. Starting from hobby
hackers, hacktivists, cyber crime gangs, to the nation state actors, all target
such infrastructure to unleash chaos on an otherwise smooth operation, and
often the cyber threat actors attempt to embarrass the organizing country or
the organizers. Unlike long-standing organizations such as a corporate or a
government department, the infrastructure of mega-events is temporary,
constructed over a short time span in expediency, and often shortcuts are taken
to make the deadline for the event. As a result, securing such an elaborate yet
temporary infrastructure requires a different approach than securing a standard
organizational digital infrastructure. In this paper, we describe our approach
to securing MahaKumbh 2025, a 600 million footfall event for 45 days in
Prayagraj, India, as a cyber security assessment and risk management oversight
team. We chronicle the scope, process, methodology, and outcome of our team's
effort to secure this mega event. It should be noted that none of the cyber
attacks during the 45-day event was successful. Our goal is to put on record
the methodology and discuss what we would do differently in case we work on
similar future mega event.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [105] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的结构化剪枝方法Catalyst，通过引入辅助变量和代数条件，解决了传统剪枝方法中存在的偏差和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如L1或Group Lasso）存在基于幅度的偏差和决策边界不鲁棒的问题，影响剪枝效果。

Method: 通过代数条件定义剪枝操作，引入辅助催化剂变量构建新正则化器，确保公平剪枝和宽裕决策边界。

Result: 实验验证了Catalyst剪枝算法的有效性，其剪枝结果优于现有方法，并表现出鲁棒和公平的特性。

Conclusion: Catalyst剪枝方法在理论和实践中均表现出色，解决了传统剪枝方法的局限性。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [106] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种基于投影空间的新型剪枝策略IPPRO，通过PROscore评分公平评估滤波器剪枝可能性，挑战传统基于幅度的剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于幅度的剪枝方法限制了剪枝决策的能力，即使冗余滤波器幅度较大也可能不被剪枝。

Method: 将滤波器置于投影空间，观察梯度下降运动方向（是否向原点移动）以评估剪枝可能性，构建PROscore评分。

Result: 实验表明，该方法在剪枝后性能下降极小，微调后表现优异，挑战了剪枝中“大小决定一切”的固有观念。

Conclusion: IPPRO通过理论创新和实证验证，扩展了基于重要性的剪枝方法的前沿。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [107] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种结合语言模型和自改进进化循环的程序合成方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在单次尝试中难以解决复杂的程序合成任务，而基于搜索的进化方法受限于生成模型的固定能力。

Method: SOAR通过交替进行进化搜索和后见学习，利用语言模型采样和优化候选解，并通过微调提升模型能力。

Result: 在ARC-AGI基准测试中，SOAR显著提升了性能，解决了52%的公共测试集问题。

Conclusion: SOAR通过自改进循环和微调，有效提升了程序合成的能力，具有广泛的应用潜力。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [108] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 论文研究了使用中间（潜在空间）融合技术预测抑郁症状，发现其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型无法充分捕捉精神数据的多模态复杂性，需要更先进的整合技术。

Method: 比较了随机森林（RF）的早期融合和基于自动编码器与神经网络的组合模型（CM）的中间融合。

Result: CM在所有设置中表现最佳，MSE更低（0.4985 vs. 0.5305），R2更高（0.4695 vs. 0.4356）。

Conclusion: 潜在空间融合是多模态心理健康数据预测的强有力方法，未来需探索模型可解释性和个体化预测。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [109] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文提出Predictive Representativity（PR）框架，用于公平性审计，强调从数据集组成转向结果层面的公平性。通过皮肤病学案例研究，发现AI模型在深色皮肤人群中表现较差，提出动态公平性评估方法。


<details>
  <summary>Details</summary>
Motivation: 解决AI医疗决策中的算法偏见和不公平结果问题，特别是针对历史上边缘化人群。

Method: 引入PR框架，通过皮肤病学案例（HAM10000和BOSQUE数据集）评估AI皮肤癌分类器的公平性。

Result: 发现AI模型在深色皮肤人群中表现较差，提出External Transportability Criterion以量化公平性。

Conclusion: 强调事后公平性审计、数据集透明度和包容性验证的重要性，为AI系统的结构不平等提供诊断工具。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [110] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 论文研究了使用反向传播算法训练的两层神经网络的解，重点分析了平滑激活函数（如sigmoid）的隐藏层机制，揭示了其解空间的奥秘。


<details>
  <summary>Details</summary>
Motivation: 理解反向传播算法在两层神经网络中的训练解，特别是针对平滑激活函数的隐藏层，以揭示其解空间的神秘性。

Method: 通过构建泰勒级数展开、严格的节点偏序、平滑样条实现和平滑连续性限制四种机制进行分析。

Result: 证明了任意输入维度的通用逼近能力，并通过实验验证，丰富了逼近理论。

Conclusion: 研究揭示了神经网络解空间的奥秘，同时为逼近理论提供了新的证明方法。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [111] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为Feature Bank Enhancement (FBE)的方法，用于改进基于距离的OOD检测方法，通过约束极端特征提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 基于距离的OOD检测方法因数据特征分布偏差和极端特征的存在，导致对ID样本评分过低，限制了检测能力。

Method: FBE利用数据集的统计特性识别并约束极端特征，扩大分布内外样本的距离。

Result: 在ImageNet-1k和CIFAR-10上的实验表明，FBE达到了最先进的性能。

Conclusion: FBE是一种简单有效的方法，显著提升了OOD检测的性能，并通过理论分析和补充实验验证了其有效性。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [112] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性聚类，高效预测和利用LLMs中的激活稀疏性，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的激活稀疏性为降低计算成本提供了机会，但直接预测神经元级激活模式计算成本高，需要一种高效的方法。

Method: 采用聚类方法，将相似的激活模式分组为少量代表性聚类，通过预测聚类分配而非单个神经元状态来降低计算复杂度。

Result: 聚类精度高达79.34%，优于标准二元聚类方法，同时困惑度（PPL）得分最低为12.49，表明在保持模型质量的同时显著减少计算开销。

Conclusion: 该方法为激活模式预测提供了高效基础，有望推动大规模语言模型的高效推理。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [113] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的鲁棒且可解释的毫米波MIMO系统波束对准引擎（BAE），通过数字孪生和迁移学习减少数据需求，利用SHAP和DkNN提升透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在6G愿景下，毫米波系统的可解释性和鲁棒性对建立信任和确保可靠性能至关重要。现有DL解决方案面临数据收集开销大、硬件限制、缺乏可解释性及易受对抗攻击等问题。

Method: BAE利用宽波束的RSSI测量预测最佳窄波束，减少波束扫描开销；通过数字孪生生成合成数据，结合迁移学习优化模型；采用SHAP和DkNN提升透明度和鲁棒性。

Result: 实验表明，该框架减少70%真实数据需求、62%波束训练开销，异常检测鲁棒性提升8.5倍，接近最优频谱效率。

Conclusion: 该框架在减少开销、提升透明度和鲁棒性方面表现优异，为毫米波MIMO系统提供了高效可靠的波束对准解决方案。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [114] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 本文提出了一种半监督联邦学习框架SSFL-DCSL，通过双重对比损失和软标签解决数据分布差异和标签稀缺问题，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法需要大量标注数据，且数据分布差异和标签获取成本高，影响模型性能。

Method: 设计了基于拉普拉斯分布的样本加权函数、双重对比损失（局部和全局），并通过加权平均和动量更新原型实现知识共享。

Result: 在仅10%数据标注的情况下，SSFL-DCSL比现有方法准确率提升1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据分布差异和标签稀缺问题，提升了模型性能。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [115] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 论文提出B4模型，通过联合嵌入价格序列和外部信号，捕捉投资者偏见与行为动态，提升市场趋势预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场行为复杂，受历史价格和外部叙事（如新闻、社交媒体情绪）影响，投资者偏见使建模困难。本文探索牛市和熊市机制在投资者驱动市场中的作用。

Method: 提出B4模型，将价格序列和外部信号嵌入共享潜在空间，通过惯性配对和双竞争机制捕捉偏见驱动的非对称性和行为惯性。

Result: 实验表明B4在预测市场趋势上表现优异，并提供偏见、行为和市场动态关系的可解释性。

Conclusion: B4模型有效建模市场动态，为投资者行为和偏见提供了新视角。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [116] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种无需训练的KV缓存优化方法，通过梯形状KV缓存模式和迭代压缩机制，提升LLMs的长距离建模能力和连续生成效率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，LLMs中的KV对数量激增，导致效率瓶颈，需要一种方法同时解决长距离建模和连续生成的内存问题。

Method: LaCache结合梯形状KV缓存模式（跨层存储KV对）和迭代压缩机制（动态压缩旧缓存），在固定缓存预算下提升性能。

Result: 实验验证LaCache能有效增强LLMs的长距离能力，并在多种任务和模型中表现一致。

Conclusion: LaCache为LLMs的长距离建模和连续生成提供了一种高效且无需训练的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [117] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 该研究开发了一种用于聋人或听力障碍者的深度学习系统，能够实时定位和识别声源，填补了当前研究的空白。


<details>
  <summary>Details</summary>
Motivation: 针对弱势群体，利用机器学习技术开发辅助设备，解决聋人或听力障碍者的声源定位和识别问题。

Method: 系统包括三个主要组件：JerryNet（用于声源方向定位的CNN架构）、基于CLAP模型的音频分类、以及结合音频、视觉和文本数据的多模态集成模型。硬件包括四麦克风阵列和摄像头眼镜。

Result: JerryNet在声源方向上达到91.1%的精度，CLAP模型在自定义和AudioSet数据集上分别达到98.5%和95%的准确率，音频-视觉定位模型的cIoU为0.892，AUC为0.658。

Conclusion: 该研究为新一代辅助设备的开发奠定了基础，具有广阔的应用前景。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [118] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，结合非线性效用聚合和几何感知查询选择，解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，通过更高效的用户交互提升模式挖掘的准确性。

Method: 使用Choquet积分建模用户偏好，结合几何感知查询选择和分支定界策略，高效识别决策边界附近的查询。

Result: 在UCI数据集上的实验表明，该方法优于现有方法（如ChoquetRank），以更少的用户交互实现更高的排名准确性。

Conclusion: 该方法通过几何感知和高效查询选择，显著提升了模式挖掘的效率和准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [119] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本研究提出了一种基于AI的框架，用于计算绿色氢产量和选址适宜性指数，结合了无监督聚类、监督学习和SHAP算法，为数据稀缺地区提供了可复制的决策工具。


<details>
  <summary>Details</summary>
Motivation: 随着各国寻求化石燃料的可持续替代品，绿色氢成为脱碳的重要途径，但选址需综合考虑复杂因素，且缺乏直接产量数据。

Method: 采用多阶段AI框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，整合气象、地形和时间数据。

Result: 模型预测准确率达98%，显示水源距离、海拔和季节变化是阿曼绿色氢选址的最关键因素。

Conclusion: 该研究为数据稀缺地区提供了客观、可复制的选址工具，替代主观专家权重，助力绿色氢基础设施规划。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [120] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种新的POGM方法，通过最大化梯度内积并限制梯度偏离，解决了梯度波动和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在梯度匹配中存在梯度波动和计算开销大的问题，需要一种更高效的方法。

Method: 利用梯度轨迹作为数据，在元学习器中进行独立训练，最大化GIP并限制梯度偏离。

Result: 在DomainBed数据集上表现出竞争力，同时实现了计算效率。

Conclusion: POGM方法有效解决了梯度波动和计算开销问题，具有实际应用潜力。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [121] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M是最大的纳米材料-蛋白质相互作用数据集，结合NanoProFormer模型，通过多模态学习显著提升了预测性能，并展示了广泛的下游任务适用性。


<details>
  <summary>Details</summary>
Motivation: 纳米材料在医学和环境科学中的应用潜力受限于对蛋白质相互作用的理解，而现有模型因数据集有限和泛化能力不足而进展缓慢。

Method: 提出NanoPro-3M数据集（320万样本，3.7万独特蛋白质），并开发NanoProFormer模型，通过多模态表示学习预测亲和力。

Result: 多模态建模显著优于单模态方法，能处理缺失特征和未知样本，并识别出冠状形成的关键因素。

Conclusion: 该研究为高性能、泛化的纳米材料-蛋白质相互作用预测奠定了基础，减少实验依赖并加速体外应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [122] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，通过线性近似扩散映射核，结合几何直觉与计算效率，适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 结合非线性扩散方法的几何直觉与线性方法（如PCA）的计算效率和可解释性。

Method: 通过线性近似扩散映射核构建LDM，并在合成和真实数据集上验证其性能。

Result: LDM在显式流形结构的数据中优于PCA，而PCA在噪声或方差主导的场景中更优。LDM的核矩阵完全正定，适用于NMF。

Conclusion: LDM是一种有价值的线性降维技术，具有理论和实际扩展潜力。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [123] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为UFO的多轮强化学习方法，通过最小化的用户反馈提升大模型的单轮和多轮推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在单轮任务上表现良好，但在多轮推理和反馈修正中效果不佳，导致重复回答。

Method: 引入Unary Feedback as Observation (UFO)，利用简单的用户反馈（如“再试一次”）进行多轮强化学习训练。

Result: 实验显示，UFO方法在保持单轮性能的同时，多轮推理准确率提升高达14%。

Conclusion: UFO方法有效提升模型在多轮任务中的反馈响应能力，同时设计了奖励机制以减少修正轮次并鼓励多样性推理。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [124] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一个基于元学习的框架，通过动态选择最优聚合规则来防御联邦学习中的模型中毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受模型中毒攻击，现有静态防御方法效果有限，尤其是在对抗性环境或异构数据中。

Method: 设计了一个轻量级上下文赌博机代理，实时选择最优聚合规则，并通过实验验证其有效性。

Result: 实验表明，FedStrategist能适应多样场景，包括对抗性攻击和异构数据环境，并在模型完整性和性能之间取得平衡。

Conclusion: FedStrategist提供了一种可分析且实用的方法，增强了去中心化AI系统的弹性和智能性。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [125] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 本文提出了一种提升深度伪造检测中个体公平性的通用框架，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用带来了严重风险，而现有检测方法在个体公平性方面存在不足，导致特定群体可能受到不公平对待。

Method: 作者首次指出个体公平性原则在深度伪造检测中的根本性失败，并提出了一种可集成到现有检测器中的通用框架。

Result: 在多个主流深度伪造数据集上的实验表明，该方法显著提升了个体公平性，同时保持了强大的检测性能。

Conclusion: 该研究为深度伪造检测中的个体公平性问题提供了首个通用解决方案，具有重要的理论和实践意义。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [126] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 该研究开发了四种机器学习模型，用于预测环形几何中的临界热通量（CHF），显著优于传统经验模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测CHF对反应堆安全分析至关重要，传统方法存在局限性，机器学习提供了改进机会。

Method: 使用CTF子通道代码，基于三种经验模型（Biasi、Bowring、Katto）开发了四种混合ML模型，利用577个实验数据点进行训练和验证。

Result: ML模型的平均相对误差低于3.5%，显著优于经验模型的26%误差。

Conclusion: 混合ML模型在环形几何中表现出色，为CHF预测提供了更准确的解决方案。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [127] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 论文通过近似影响函数过滤噪声数据，提升微调语言模型的性能，实验显示移除10%训练数据后准确率提升1.5%。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据通常噪声较大，影响语言模型微调效果，需有效方法检测和过滤有害数据。

Method: 使用共轭梯度近似影响函数过滤TL;DR数据集中的噪声数据，并比较梯度相似性与影响函数的效果。

Result: 移除10%训练数据后，准确率提升1.5%；梯度相似性在检测有益数据时优于影响函数。

Conclusion: 局部曲率对检测有害数据重要，但对有益数据影响较小，梯度相似性是更优方法。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [128] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection是一种新的参数高效微调方法，通过调整解码器块级别的表示而非单个权重矩阵，优于LoRA，并显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进现有PEFT方法，通过引入长跳跃连接和基于同伦理论的线性变换，实现更平滑稳定的模型适应。

Method: 提出Solo Connection方法，利用解码器块级别的长跳跃连接和线性变换，逐步插值任务特定表示。

Result: Solo Connection在E2E自然语言生成任务中优于LoRA，可训练参数减少59%（相比LoRA）和99%以上（相比全微调）。

Conclusion: Solo Connection为大型语言模型的高效微调提供了新思路，尤其适用于多层模型的任务适应。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [129] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET是一种针对实时网络攻击检测的增量因果图学习框架，通过动态更新因果图来适应系统行为变化，显著提高了检测准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 网络攻击对关键基础设施的威胁日益严重，传统检测方法因高数据方差和类别不平衡导致误报率高，且现有因果图方法无法适应实时动态变化。

Method: INCADET包含三个模块：早期症状检测、增量因果图学习和因果图分类，结合经验重放和图卷积网络动态更新因果图。

Result: 在真实关键基础设施数据集上的实验表明，INCADET在准确性和适应性上优于静态因果和深度时序基线方法。

Conclusion: INCADET为实时网络攻击检测提供了一种高效、适应性强的解决方案。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [130] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单测试时缩放方法，发现其主要效果来自限制最大长度，而通过追加“Wait”进行扩展会导致不一致性。o1类模型通过强化学习自然扩展计算能力，性能超越峰值。


<details>
  <summary>Details</summary>
Motivation: 研究简单测试时缩放方法的实际效果及其与o1类模型的区别，以明确测试时计算扩展的真正目标。

Method: 分析简单测试时缩放方法，包括限制最大长度和追加“Wait”扩展，并与o1类模型（如DeepSeek-R1@）的扩展行为对比。

Result: 限制最大长度是简单测试时缩放的主要效果来源，而追加“Wait”会导致模型解决方案的振荡。o1类模型通过自然扩展计算能力实现更高性能。

Conclusion: 测试时计算扩展的目标应是解锁更高性能，而非仅复制缩放行为的外观。o1类模型的自然扩展能力优于简单测试时缩放。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [131] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 该论文提出了一种结合强化学习和深度学习的方法，用于解决大规模随机优化问题，特别是在供应链中的多源多周期库存管理问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于如何高效地利用强化学习解决复杂的随机优化问题，尤其是在供应链管理中，通过预训练的深度学习模型模拟和组合随机过程，以更好地探索解空间。

Method: 方法包括使用深度强化学习模型学习和预测随机供应链过程，并引入约束协调机制来预测库存网络中的双重成本。通过将复杂的物理约束分解为可扩展和可组合的深度学习模块，避免了直接建模的复杂性。

Result: 结果表明，该方法在大型实际数据集上表现更优，能够有效解决复杂的供应链优化问题。

Conclusion: 结论指出，该方法为大规模随机优化问题提供了可行的解决方案，并提出了未来研究的开放性问题，以进一步验证此类模型的有效性。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [132] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 论文提出ReDiSC，一种基于重参数化掩码扩散模型的图节点分类方法，解决了现有方法忽略节点标签相关性的问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）方法假设节点标签条件独立，但实际中标签仍相关。ReDiSC旨在解决这一问题，实现结构化预测。

Method: ReDiSC通过变分期望最大化框架学习重参数化掩码扩散模型，估计节点标签的联合分布。

Result: ReDiSC在多种图上表现优于现有方法，且能高效扩展到大规模数据集。

Conclusion: ReDiSC在结构化节点分类任务中具有显著优势，尤其在计算效率和性能上优于现有方法。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [133] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 提出了一种联邦强化学习框架FRL-EH，解决局部环境统计异质性问题，通过聚合经验学习全局策略并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现实场景中局部环境存在统计异质性，需设计鲁棒的联邦强化学习框架以优化全局策略。

Method: 提出FedRQ算法，理论证明其渐近收敛性，并通过期望损失扩展至连续状态空间。

Result: 实验验证FedRQ在异质环境中的有效性和鲁棒性，性能优于现有方法。

Conclusion: FRL-EH框架及FedRQ算法在联邦强化学习中表现出色，适用于复杂异质环境。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [134] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种新的不可靠行为来源——glitches（故障点），并证明了其在梯度提升决策树（GBDT）模型中的广泛存在。通过形式化定义和算法搜索，展示了检测glitches的NP完全性及其对模型一致性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在关键决策任务中的广泛应用，确保其输出的可靠性和一致性变得至关重要。论文旨在识别并解决模型决策边界陡峭时可能出现的glitches问题。

Method: 论文首先形式化定义了glitches，并通过文献中的模型和数据集验证其普遍存在。接着，针对GBDT模型，提出了基于混合整数线性规划（MILP）的glitch搜索算法。

Result: 研究证明，对于深度为4的树集成模型，检测glitches是NP完全问题。实验结果表明，glitch搜索算法在广泛使用的GBDT基准数据集上具有有效性和计算可行性。

Conclusion: glitches是影响AI模型可靠性的重要因素，论文提出的方法为检测和解决此类问题提供了理论基础和实用工具。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [135] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件生成问题的知识蒸馏框架GenDD，通过Split Tokenization和Distribution Contraction技术解决了高维优化和标签监督不足的问题，实验表明其在无监督和监督设置下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）通常面临高维优化和标签监督不足的挑战，论文旨在通过生成式方法改进KD的效果。

Method: 提出GenDD框架，结合Split Tokenization实现稳定的无监督KD，并通过Distribution Contraction技术引入标签监督。

Result: 在无监督设置下，GenDD比KL基线提升16.29%；在监督设置下，ResNet-50在ImageNet上达到82.28%的top-1准确率。

Conclusion: GenDD框架在知识蒸馏中表现出色，尤其在无监督和监督任务中均取得显著效果。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [136] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种结构感知的度量函数SDSC，用于时间序列自监督表示学习，解决了传统距离目标（如MSE）的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法（如MSE）对幅度敏感、对波形极性不变且尺度无界，阻碍了语义对齐和可解释性。

Method: SDSC基于Dice相似系数，量化时间信号的结构一致性，并通过可微分近似实现梯度优化。还提出了一种结合SDSC与MSE的混合损失。

Result: 实验表明，SDSC在预测和分类任务中表现优于或与MSE相当，尤其在领域内和低资源场景中。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [137] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 论文提出了一种基于正未标记（PU）学习的方法，用于在观察性研究中识别控制组，从而估计平均处理效应（ATE）。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，缺乏明确标记的控制组是一个常见问题，导致难以估计无偏的ATE。

Method: 采用PU学习框架，仅基于已处理的（正）单元，从未标记数据中高置信度地识别控制单元。

Result: 在模拟和真实数据中，该方法成功识别控制组，并估计出接近真实值的ATE。

Conclusion: PU学习为观察性因果推断提供了新工具，尤其适用于难以进行随机实验的领域。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [138] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于最大因果熵的逆向强化学习方法，用于无限时域平稳均值场博弈，通过再生核希尔伯特空间建模奖励函数，解决了现有方法中奖励函数线性组合的限制。


<details>
  <summary>Details</summary>
Motivation: 现有逆向强化学习方法在均值场博弈中通常限制奖励函数为固定基函数的线性组合，且多基于有限时域。本文旨在解决这些限制，推断更丰富的非线性奖励结构。

Method: 采用拉格朗日松弛法将问题转化为无约束对数似然最大化，并通过梯度上升算法求解。证明了软贝尔曼算子在再生核希尔伯特空间参数下的Fr'echet可微性。

Result: 在均值场交通路由博弈中，该方法能准确恢复专家行为，验证了其有效性。

Conclusion: 本文方法在无限时域均值场博弈中成功推断非线性奖励函数，扩展了逆向强化学习的应用范围。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [139] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 论文提出了一种分布式的遗忘框架，旨在高效移除模型中的特定分布数据，同时保留其他数据的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘工具主要针对单个样本，难以满足隐私、法律或质量需求中对整个主题域的删除要求。

Method: 采用基于Kullback-Leibler散度的分布遗忘框架，通过精确的Pareto前沿和距离选择规则减少删除量。

Result: 实验表明，该方法在多个数据集上比随机删除减少15-72%的删除量，且对保留数据性能影响极小。

Conclusion: 分布遗忘框架为高效移除特定分布数据提供了模型无关的解决方案。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>
