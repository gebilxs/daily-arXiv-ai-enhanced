{"id": "2507.14189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter\u662f\u4e00\u4e2a\u9488\u5bf9\u4e13\u4e1a\u9886\u57df\uff08\u5982\u91d1\u878d\u3001\u533b\u5b66\u3001\u6cd5\u5f8b\uff09\u7684\u591a\u6a21\u6001\u5199\u4f5c\u52a9\u624b\uff0c\u901a\u8fc7\u79bb\u7ebf\u77e5\u8bc6\u5e93\u548c\u4efb\u52a1\u5206\u89e3\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u6863\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u7d22\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u53ef\u9760\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e13\u4e1a\u9886\u57df\u5199\u4f5c\u4e2d\u5b58\u5728\u77e5\u8bc6\u4e0d\u8db3\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982RAG\u548c\u5728\u7ebf\u641c\u7d22\uff09\u5728\u68c0\u7d22\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "DeepWriter\u91c7\u7528\u4efb\u52a1\u5206\u89e3\u3001\u5927\u7eb2\u751f\u6210\u3001\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5206\u6b65\u5199\u4f5c\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u548c\u5c42\u6b21\u5316\u77e5\u8bc6\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepWriter\u5728\u91d1\u878d\u62a5\u544a\u751f\u6210\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u751f\u6210\u5185\u5bb9\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "DeepWriter\u901a\u8fc7\u79bb\u7ebf\u77e5\u8bc6\u5e93\u548c\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e13\u4e1a\u9886\u57df\u5199\u4f5c\u7684\u51c6\u786e\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.14198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5fae\u8c03\u76ee\u6807\u4e0e\u6a21\u578b\u7f16\u8f91\u6280\u672f\u7684\u4ea4\u4e92\u4f1a\u5bfc\u81f4\u7f16\u8f91\u77e5\u8bc6\u6bd4\u9884\u8bad\u7ec3\u77e5\u8bc6\u66f4\u5bb9\u6613\u88ab\u9057\u5fd8\uff0c\u51bb\u7ed3\u76f8\u5173\u5c42\u53ef\u6539\u5584\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u7406\u89e3\u5fae\u8c03\u5bf9\u5df2\u7f16\u8f91\u77e5\u8bc6\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u5fae\u8c03\u76ee\u6807\u4e0e\u6a21\u578b\u7f16\u8f91\u6280\u672f\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u6d4b\u8bd5\u51bb\u7ed3\u76f8\u5173\u5c42\u7684\u6548\u679c\u3002", "result": "\u7f16\u8f91\u77e5\u8bc6\u5728\u5fae\u8c03\u4e2d\u66f4\u5bb9\u6613\u9057\u5fd8\uff0c\u51bb\u7ed3\u76f8\u5173\u5c42\u80fd\u663e\u8457\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u3002", "conclusion": "\u5f53\u524d\u7f16\u8f91\u65b9\u6cd5\u5728\u5fae\u8c03\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u6765\u9700\u8bc4\u4f30\u7f16\u8f91\u9c81\u68d2\u6027\u5e76\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.14200", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "SMACS\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6574\u5408\u5f00\u6e90LLMs\uff0c\u6027\u80fd\u8d85\u8d8a\u95ed\u6e90LLMs\u3002", "motivation": "\u63a2\u7d22\u5f00\u6e90LLMs\u96c6\u4f53\u534f\u4f5c\u662f\u5426\u80fd\u8d85\u8d8a\u95ed\u6e90LLMs\u3002", "method": "\u63d0\u51faSMACS\u6846\u67b6\uff0c\u5305\u62ec\u68c0\u7d22\u5f0f\u5148\u9a8c\u9009\u62e9\uff08RPS\uff09\u548c\u63a2\u7d22-\u5229\u7528\u9a71\u52a8\u7684\u540e\u9a8c\u589e\u5f3a\uff08EPE\uff09\u3002", "result": "\u5728\u516b\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSMACS\u8d85\u8d8a2025\u5e74\u9886\u5148\u95ed\u6e90LLMs\uff08\u5982Claude-3.7-Sonnet\u3001GPT-4.1\u7b49\uff09\u3002", "conclusion": "SMACS\u8bc1\u660e\u4e86\u5f00\u6e90LLMs\u534f\u4f5c\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u4e0a\u9650\u3002"}}
{"id": "2507.14214", "categories": ["cs.CL", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u901a\u8fc7NLP\u548c\u903b\u8f91\u63a8\u7406\u5206\u6790\u9690\u79c1\u653f\u7b56\uff0c\u5e2e\u52a9\u7528\u6237\u4e2a\u6027\u5316\u7406\u89e3\u6570\u636e\u4f7f\u7528\u6761\u6b3e\uff0c\u663e\u8457\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u73b0\u4ee3\u7528\u6237\u5f88\u5c11\u9605\u8bfb\u9690\u79c1\u653f\u7b56\uff0c\u4f46\u9700\u8981\u5de5\u5177\u6765\u4e2a\u6027\u5316\u5206\u6790\u653f\u7b56\u5185\u5bb9\uff0c\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "method": "\u7ed3\u5408NLP\u63d0\u53d6\u653f\u7b56\u6587\u672c\u7684\u6b63\u5f0f\u8868\u793a\uff0c\u901a\u8fc7\u903b\u8f91\u63a8\u7406\u6bd4\u8f83\u7528\u6237\u504f\u597d\u4e0e\u653f\u7b56\u5185\u5bb9\uff0c\u751f\u6210\u5408\u89c4\u62a5\u544a\u3002", "result": "\u5728\u8bc4\u4f30\u4e2d\uff0cPoliAnalyzer\u51c6\u786e\u8bc6\u522b\u6570\u636e\u4f7f\u7528\u5b9e\u8df5\uff08F1-score 90-100%\uff09\uff0c\u5e76\u53d1\u73b095.2%\u7684\u653f\u7b56\u5185\u5bb9\u4e0e\u7528\u6237\u504f\u597d\u65e0\u51b2\u7a81\u3002", "conclusion": "PoliAnalyzer\u652f\u6301\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u9690\u79c1\u653f\u7b56\u5206\u6790\uff0c\u5e2e\u52a9\u7528\u6237\u638c\u63e1\u6570\u636e\u63a7\u5236\u6743\uff0c\u4fc3\u8fdb\u793e\u4f1a\u5bf9\u5e73\u53f0\u6570\u636e\u5b9e\u8df5\u7684\u8ba8\u8bba\u3002"}}
{"id": "2507.14268", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "\u6bd4\u8f83\u5206\u6790\u7528\u4e8e\u62df\u54083D\u56fe\u50cf\u6570\u636e\u7684\u9576\u5d4c\u6a21\u578b\u7b97\u6cd5\u7b56\u7565\uff0c\u8bc4\u4f30\u4e0d\u540c\u4f18\u5316\u65b9\u6cd5\u5728\u751f\u6210\u8fd1\u4f3c\u4f53\u7d20\u7ed3\u6784\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u6750\u6599\u79d1\u5b66\u4e2d\uff0c\u51c6\u786e\u62df\u54083D\u56fe\u50cf\u6570\u636e\uff08\u5982\u591a\u6676\u4f53\u548c\u6ce1\u6cab\uff09\u7684\u9576\u5d4c\u6a21\u578b\u662f\u4e00\u4e2a\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u7b97\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u7ebf\u6027/\u975e\u7ebf\u6027\u89c4\u5212\u3001\u968f\u673a\u4f18\u5316\uff08\u4ea4\u53c9\u71b5\u6cd5\uff09\u548c\u68af\u5ea6\u4e0b\u964d\u7b49\u65b9\u6cd5\u751f\u6210Voronoi\u3001Laguerre\u548cGBPD\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u6570\u636e\u96c6\u8bc4\u4f30\u62df\u5408\u8d28\u91cf\uff0c\u53d1\u73b0\u6a21\u578b\u590d\u6742\u5ea6\u3001\u4f18\u5316\u65b9\u6cd5\u590d\u6742\u5ea6\u548c\u8fd1\u4f3c\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6839\u636e\u6570\u636e\u7279\u5f81\u548c\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.14170", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14170", "abs": "https://arxiv.org/abs/2507.14170", "authors": ["Jaeheun Jung", "Donghun Lee"], "title": "Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space", "comment": "ICML 2025 workshop HiLD 2025 (3rd workshop on High-dimensional\n  Learning Dynamics)", "summary": "Structured pruning aims to reduce the size and computational cost of deep\nneural networks by removing entire filters or channels. The traditional\nregularizers such as L1 or Group Lasso and its variants lead to\nmagnitude-biased pruning decisions, such that the filters with small magnitudes\nare likely to be pruned. Also, they often entail pruning results with almost\nzero margin around pruning decision boundary, such that tiny perturbation in a\nfilter magnitude can flip the pruning decision. In this paper, we identify the\nprecise algebraic condition under which pruning operations preserve model\nperformance, and use the condition to construct a novel regularizer defined in\nan extended parameter space via auxiliary catalyst variables. The proposed\nCatalyst regularization ensures fair pruning chance for each filters with\ntheoretically provable zero bias to their magnitude and robust pruning behavior\nachieved by wide-margin bifurcation of magnitudes between the preserved and the\npruned filters. The theoretical properties naturally lead to real-world\neffectiveness, as shown by empirical validations of Catalyst Pruning algorithm.\nPruning results on various datasets and models are superior to state-of-the-art\nfilter pruning methods, and at the same time confirm the predicted robust and\nfair pruning characteristics of Catalyst pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5Catalyst\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u548c\u4ee3\u6570\u6761\u4ef6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\uff08\u5982L1\u6216Group Lasso\uff09\u5b58\u5728\u57fa\u4e8e\u5e45\u5ea6\u7684\u504f\u5dee\u548c\u51b3\u7b56\u8fb9\u754c\u4e0d\u9c81\u68d2\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u526a\u679d\u6548\u679c\u3002", "method": "\u901a\u8fc7\u4ee3\u6570\u6761\u4ef6\u5b9a\u4e49\u526a\u679d\u64cd\u4f5c\uff0c\u5f15\u5165\u8f85\u52a9\u50ac\u5316\u5242\u53d8\u91cf\u6784\u5efa\u65b0\u6b63\u5219\u5316\u5668\uff0c\u786e\u4fdd\u516c\u5e73\u526a\u679d\u548c\u5bbd\u88d5\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Catalyst\u526a\u679d\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5176\u526a\u679d\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u9c81\u68d2\u548c\u516c\u5e73\u7684\u7279\u6027\u3002", "conclusion": "Catalyst\u526a\u679d\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14197", "categories": ["cs.CR", "cs.IT", "math.IT", "94A60"], "pdf": "https://arxiv.org/pdf/2507.14197", "abs": "https://arxiv.org/abs/2507.14197", "authors": ["Andriamifidisoa Ramamonjy", "Rufine Marius Lalasoa"], "title": "DM-RSA: An Extension of RSA with Dual Modulus", "comment": "5 pages", "summary": "We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem\nthat employs two distinct moduli symmetrically to enhance security. By\nleveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides\nincreased robustness against side-channel attacks while preserving the\nefficiency of classical RSA. This approach improves resistance to partial\ncompromise of a modulus and integrates easily into existing infrastructures.", "AI": {"tldr": "DM-RSA\u662f\u4e00\u79cdRSA\u53d8\u4f53\uff0c\u4f7f\u7528\u53cc\u6a21\u6570\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u4e2d\u56fd\u5269\u4f59\u5b9a\u7406\uff08CRT\uff09\u63d0\u9ad8\u6297\u4fa7\u4fe1\u9053\u653b\u51fb\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "motivation": "\u589e\u5f3aRSA\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u6297\u4fa7\u4fe1\u9053\u653b\u51fb\u548c\u6a21\u6570\u90e8\u5206\u6cc4\u9732\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u6570\u8bbe\u8ba1\uff0c\u5229\u7528CRT\u8fdb\u884c\u89e3\u5bc6\u3002", "result": "\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u7ecf\u5178RSA\u76f8\u5f53\u7684\u6548\u7387\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u4e2d\u3002", "conclusion": "DM-RSA\u662f\u4e00\u79cd\u5b89\u5168\u4e14\u9ad8\u6548\u7684RSA\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2507.14231", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528NLP\u6a21\u578b\u8bc6\u522b\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u7684\u53cc\u76f8\u60c5\u611f\u969c\u788d\u75c7\u72b6\uff0c\u53d1\u73b0RoBERTa\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe98%\uff0c\u800c\u57fa\u4e8e\u9759\u6001\u5d4c\u5165\u7684LSTM\u6a21\u578b\u6548\u679c\u8f83\u5dee\u3002", "motivation": "\u53cc\u76f8\u60c5\u611f\u969c\u788d\u5e38\u56e0\u65e9\u671f\u75c7\u72b6\u4e0d\u660e\u663e\u548c\u793e\u4f1a\u6c61\u540d\u88ab\u6f0f\u8bca\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7NLP\u6280\u672f\u63d0\u5347\u65e9\u671f\u7b5b\u67e5\u80fd\u529b\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982BERT\u3001RoBERTa\uff09\u548cLSTM\u6a21\u578b\uff0c\u4f7f\u7528Reddit\u5e16\u5b50\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "RoBERTa\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u7ea698%\uff1b\u57fa\u4e8eBERT\u5d4c\u5165\u7684LSTM\u6548\u679c\u63a5\u8fd1\uff0c\u800c\u9759\u6001\u5d4c\u5165\u7684LSTM\u8868\u73b0\u6781\u5dee\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u5bf9\u53cc\u76f8\u60c5\u611f\u969c\u788d\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0cDistilBERT\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7NLP\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.14303", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u9ad8\u6548\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u4e3b\u5e72\u7f51\u7edc\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u673a\u5668\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86\u4eba\u7c7b\u4e13\u5bb6\u7684\u9700\u6c42\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u5c24\u4e3a\u91cd\u8981\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u63d0\u5347\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f7f\u7528BDD100k\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u591a\u79cd\u9ad8\u6548\u6a21\u578b\uff0c\u5e76\u5c1d\u8bd5\u4e0d\u540c\u4e3b\u5e72\u7f51\u7edc\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u7814\u7a76\u5176\u5bf9\u8bed\u4e49\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u5408\u9002\u7684\u4e3b\u5e72\u7f51\u7edc\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u573a\u666f\u548c\u73af\u5883\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u51c6\u786e\u7387\u3001\u5e73\u5747IoU\u548c\u635f\u5931\u51fd\u6570\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5404\u9879\u6307\u6807\u5747\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2507.14171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14171", "abs": "https://arxiv.org/abs/2507.14171", "authors": ["Jaeheun Jung", "Jaehyuk Lee", "Yeajin Lee", "Donghun Lee"], "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "comment": null, "summary": "With the growth of demand on neural network compression methods, the\nstructured pruning methods including importance-based approach are actively\nstudied. The magnitude importance and many correlated modern importance\ncriteria often limit the capacity of pruning decision, since the filters with\nlarger magnitudes are not likely to be pruned if the smaller one didn't, even\nif it is redundant. In this paper, we propose a novel pruning strategy to\nchallenge this dominating effect of magnitude and provide fair chance to each\nfilter to be pruned, by placing it on projective space. After that, we observe\nthe gradient descent movement whether the filters move toward the origin or\nnot, to measure how the filter is likely to be pruned. This measurement is used\nto construct PROscore, a novel importance score for IPPRO, a novel\nimportance-based structured pruning with magnitude-indifference. Our evaluation\nresults shows that the proposed importance criteria using the projective space\nachieves near-lossless pruning by reducing the performance drop in pruning,\nwith promising performance after the finetuning. Our work debunks the\n``size-matters'' myth in pruning and expands the frontier of importance-based\npruning both theoretically and empirically.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u7a7a\u95f4\u7684\u65b0\u578b\u526a\u679d\u7b56\u7565IPPRO\uff0c\u901a\u8fc7PROscore\u8bc4\u5206\u516c\u5e73\u8bc4\u4f30\u6ee4\u6ce2\u5668\u526a\u679d\u53ef\u80fd\u6027\uff0c\u6311\u6218\u4f20\u7edf\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u9650\u5236\u4e86\u526a\u679d\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u5373\u4f7f\u5197\u4f59\u6ee4\u6ce2\u5668\u5e45\u5ea6\u8f83\u5927\u4e5f\u53ef\u80fd\u4e0d\u88ab\u526a\u679d\u3002", "method": "\u5c06\u6ee4\u6ce2\u5668\u7f6e\u4e8e\u6295\u5f71\u7a7a\u95f4\uff0c\u89c2\u5bdf\u68af\u5ea6\u4e0b\u964d\u8fd0\u52a8\u65b9\u5411\uff08\u662f\u5426\u5411\u539f\u70b9\u79fb\u52a8\uff09\u4ee5\u8bc4\u4f30\u526a\u679d\u53ef\u80fd\u6027\uff0c\u6784\u5efaPROscore\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u526a\u679d\u540e\u6027\u80fd\u4e0b\u964d\u6781\u5c0f\uff0c\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u5f02\uff0c\u6311\u6218\u4e86\u526a\u679d\u4e2d\u201c\u5927\u5c0f\u51b3\u5b9a\u4e00\u5207\u201d\u7684\u56fa\u6709\u89c2\u5ff5\u3002", "conclusion": "IPPRO\u901a\u8fc7\u7406\u8bba\u521b\u65b0\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u65b9\u6cd5\u7684\u524d\u6cbf\u3002"}}
{"id": "2507.14201", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14201", "abs": "https://arxiv.org/abs/2507.14201", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Ra\u00fal Mel\u00e9ndez Luj\u00e1n", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "comment": null, "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!", "AI": {"tldr": "ExCyTIn-Bench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u8c03\u67e5\u4efb\u52a1\u4e2d\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e\u4e13\u5bb6\u6784\u5efa\u7684\u8c03\u67e5\u56fe\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u5b89\u5168\u5206\u6790\u5e08\u9700\u8981\u5904\u7406\u5927\u91cf\u5f02\u6784\u8b66\u62a5\u548c\u65e5\u5fd7\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u7684LLM\u4ee3\u7406\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "method": "\u901a\u8fc7Azure\u79df\u6237\u6784\u5efa\u6570\u636e\u96c6\uff0c\u6a21\u62df\u771f\u5b9e\u653b\u51fb\uff0c\u751f\u6210\u8c03\u67e5\u56fe\u548c\u95ee\u9898\uff0c\u5229\u7528LLM\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u548c\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u96be\u5ea6\u8f83\u9ad8\uff0c\u6700\u4f73\u6a21\u578b\u5f97\u5206\u4ec5\u4e3a0.368\uff0c\u8868\u660e\u672a\u6765\u7814\u7a76\u7a7a\u95f4\u5927\u3002", "conclusion": "ExCyTIn-Bench\u4e3aLLM\u4ee3\u7406\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u51c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5373\u5c06\u53d1\u5e03\u3002"}}
{"id": "2507.14238", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7528\u6237\u4ea4\u4e92\u5e94\u7528\u4e2d\u4f1a\u6839\u636e\u6587\u672c\u4e2d\u7684\u8eab\u4efd\u6807\u8bb0\uff08\u5982\u79cd\u65cf\u3001\u6027\u522b\u3001\u5e74\u9f84\uff09\u4ea7\u751f\u504f\u89c1\uff0c\u5f71\u54cd\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u653f\u6cbb\u7b49\u9886\u57df\u7684\u51b3\u7b56\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u7ed3\u679c\u3002", "motivation": "\u63a2\u8ba8LLM\u5982\u4f55\u5229\u7528\u7528\u6237\u6587\u672c\u4e2d\u7684\u8eab\u4efd\u4fe1\u606f\u8fdb\u884c\u51b3\u7b56\uff0c\u5e76\u5206\u6790\u5176\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u504f\u89c1\u5f71\u54cd\u3002", "method": "\u5bf9\u4e94\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\u7684LLM\u5e94\u7528\uff08\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u653f\u6cbb\u3001\u653f\u5e9c\u798f\u5229\u3001\u85aa\u8d44\uff09\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u8bc4\u4f30\u8eab\u4efd\u6807\u8bb0\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "result": "LLM\u5bf9\u8eab\u4efd\u6807\u8bb0\u6781\u4e3a\u654f\u611f\uff0c\u79cd\u65cf\u3001\u6027\u522b\u548c\u5e74\u9f84\u7b49\u56e0\u7d20\u663e\u8457\u5f71\u54cd\u5176\u51b3\u7b56\uff0c\u4f8b\u5982\u533b\u7597\u5efa\u8bae\u4e2d\u7684\u5dee\u5f02\u5316\u6807\u51c6\u3001\u85aa\u8d44\u63a8\u8350\u4e2d\u7684\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u7b49\u3002", "conclusion": "\u73b0\u6210\u7684LLM\u5e94\u7528\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\uff0c\u5efa\u8bae\u5728\u90e8\u7f72\u524d\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u65b0\u5de5\u5177\u4ee5\u68c0\u6d4b\u8eab\u4efd\u6807\u8bb0\u5bf9\u6a21\u578b\u51b3\u7b56\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.14312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Cl\u00e9ment Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "CLIPTTA\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8f6f\u5bf9\u6bd4\u635f\u5931\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5f00\u653e\u96c6\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff08\u5982\u71b5\u6700\u5c0f\u5316\uff09\u4e0eVLMs\u7684\u5bf9\u6bd4\u8bad\u7ec3\u76ee\u6807\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u63d0\u51faCLIPTTA\uff0c\u91c7\u7528\u4e0eCLIP\u9884\u8bad\u7ec3\u76ee\u6807\u4e00\u81f4\u7684\u8f6f\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u6279\u6b21\u611f\u77e5\u8bbe\u8ba1\u80fd\u907f\u514d\u5d29\u6e83\u98ce\u9669\u3002\u6269\u5c55\u81f3\u5f00\u653e\u96c6\u8bbe\u7f6e\uff0c\u4f7f\u7528\u5f02\u5e38\u5bf9\u6bd4\u66b4\u9732\uff08OCE\uff09\u635f\u5931\u63d0\u5347OOD\u68c0\u6d4b\u3002", "result": "\u572875\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLIPTTA\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\uff0c\u5e76\u5728\u591a\u6837\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u7a33\u5b9a\uff0c\u4f18\u4e8e\u73b0\u6709TTA\u65b9\u6cd5\u3002", "conclusion": "CLIPTTA\u901a\u8fc7\u5bf9\u9f50\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86VLMs\u5728\u5206\u5e03\u504f\u79fb\u548c\u5f00\u653e\u96c6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14172", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14172", "abs": "https://arxiv.org/abs/2507.14172", "authors": ["Julien Pourcel", "C\u00e9dric Colas", "Pierre-Yves Oudeyer"], "title": "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI", "comment": null, "summary": "Many program synthesis tasks prove too challenging for even state-of-the-art\nlanguage models to solve in single attempts. Search-based evolutionary methods\noffer a promising alternative by exploring solution spaces iteratively, but\ntheir effectiveness remain limited by the fixed capabilities of the underlying\ngenerative model.\n  We propose SOAR, a method that learns program synthesis by integrating\nlanguage models into a self-improving evolutionary loop.\n  SOAR alternates between (1) an evolutionary search that uses an LLM to sample\nand refine candidate solutions, and (2) a hindsight learning phase that\nconverts search attempts into valid problem-solution pairs used to fine-tune\nthe LLM's sampling and refinement capabilities\\, -- \\,enabling increasingly\neffective search in subsequent iterations.\n  On the challenging ARC-AGI benchmark, SOAR achieves significant performance\ngains across model scales and iterations, leveraging positive transfer between\nthe sampling and refinement finetuning tasks. These improvements carry over to\ntest-time adaptation, enabling SOAR to solve 52\\% of the public test set. Our\ncode is open-sourced at: https://github.com/flowersteam/SOAR", "AI": {"tldr": "SOAR\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u6539\u8fdb\u8fdb\u5316\u5faa\u73af\u7684\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6b21\u5c1d\u8bd5\u4e2d\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff0c\u800c\u57fa\u4e8e\u641c\u7d22\u7684\u8fdb\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u751f\u6210\u6a21\u578b\u7684\u56fa\u5b9a\u80fd\u529b\u3002", "method": "SOAR\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u8fdb\u5316\u641c\u7d22\u548c\u540e\u89c1\u5b66\u4e60\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u548c\u4f18\u5316\u5019\u9009\u89e3\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5728ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSOAR\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u89e3\u51b3\u4e8652%\u7684\u516c\u5171\u6d4b\u8bd5\u96c6\u95ee\u9898\u3002", "conclusion": "SOAR\u901a\u8fc7\u81ea\u6539\u8fdb\u5faa\u73af\u548c\u5fae\u8c03\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7a0b\u5e8f\u5408\u6210\u7684\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14202", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14202", "abs": "https://arxiv.org/abs/2507.14202", "authors": ["Pengfei Du"], "title": "PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse applications, yet they pose significant security risks that threaten\ntheir safe deployment in critical domains. Current security alignment\nmethodologies predominantly rely on Process Reward Models (PRMs) to evaluate\nintermediate reasoning steps, introducing substantial computational overhead\nand scalability constraints. This paper presents a novel PRM-free security\nalignment framework that leverages automated red teaming and adversarial\ntraining to achieve robust security guarantees while maintaining computational\nefficiency. Our approach systematically identifies vulnerabilities through\nsophisticated attack strategies including genetic algorithm optimization,\nmulti-agent simulation, and advanced prompt mutation techniques. The framework\nenhances model robustness via targeted adversarial training with curriculum\nlearning and adaptive regularization mechanisms. Comprehensive experimental\nevaluation across five state-of-the-art LLMs demonstrates that our method\nachieves superior security alignment performance compared to PRM-based\napproaches while reducing computational costs by 61\\%. The framework\nincorporates transparent reporting and continuous audit mechanisms that enable\niterative security improvement and regulatory compliance. Our contributions\nadvance the field of efficient LLM security alignment by democratizing access\nto robust security measures for resource-constrained organizations and\nproviding a scalable foundation for addressing evolving adversarial threats.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700PRM\u7684\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\u548c\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347LLM\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8ePRM\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u6269\u5c55\u6027\u53d7\u9650\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\uff08\u5982\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u3001\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9ad8\u7ea7\u63d0\u793a\u53d8\u5f02\u6280\u672f\uff09\u548c\u5bf9\u6297\u8bad\u7ec3\uff08\u8bfe\u7a0b\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6b63\u5219\u5316\uff09\u6765\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e94\u79cd\u5148\u8fdbLLM\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8ePRM\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e61%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b89\u5168\u5bf9\u9f50\u65b9\u6848\uff0c\u5e76\u4e3a\u5e94\u5bf9\u6301\u7eed\u6f14\u53d8\u7684\u5bf9\u6297\u5a01\u80c1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCCL-XCoT\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u51cf\u5c11\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6613\u4ea7\u751f\u5e7b\u89c9\uff08\u5982\u4e0d\u51c6\u786e\u6216\u865a\u6784\u8f93\u51fa\uff09\uff0c\u5f71\u54cd\u9886\u57df\u7279\u5b9a\u751f\u6210\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8de8\u8bed\u8a00\u8bed\u4e49\u5bf9\u9f50\uff0c\u7ed3\u5408\u8de8\u8bed\u8a00\u601d\u7ef4\u94fe\uff08XCoT\uff09\u63d0\u793a\u7b56\u7565\u5f15\u5bfc\u6a21\u578b\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCCL-XCoT\u5c06\u5e7b\u89c9\u7387\u964d\u4f4e62%\uff0c\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u4e8b\u5b9e\u77e5\u8bc6\u8fc1\u79fb\u3002", "conclusion": "CCL-XCoT\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u6216\u591a\u6a21\u578b\u96c6\u6210\u3002"}}
{"id": "2507.14315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6ce8\u610f\u529b\u805a\u7126\uff08AF\uff09\u7684\u673a\u5236\uff0c\u901a\u8fc7\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\u6765\u89e3\u51b3\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GCD\u65b9\u6cd5\u5728\u5904\u7406\u672a\u6807\u8bb0\u6570\u636e\u65f6\uff0c\u6a21\u578b\u6ce8\u610f\u529b\u5bb9\u6613\u88ab\u4efb\u52a1\u65e0\u5173\u7684\u80cc\u666f\u533a\u57df\u5206\u6563\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u7406\u60f3\u3002", "method": "AF\u7531\u4e24\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff1a\u6807\u8bb0\u91cd\u8981\u6027\u5ea6\u91cf\uff08TIME\uff09\u548c\u591a\u5c3a\u5ea6\u6807\u8bb0\u81ea\u9002\u5e94\u526a\u679d\uff08TAP\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u6807\u8bb0\u91cd\u8981\u6027\u5e76\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\u6765\u4f18\u5316\u6ce8\u610f\u529b\u3002", "result": "AF\u5728SimGCD\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe15.4%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "AF\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u6709\u6548\u63d0\u5347GCD\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14175", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.14175", "abs": "https://arxiv.org/abs/2507.14175", "authors": ["Youcef Barkat", "Dylan Hamitouche", "Deven Parekh", "Ivy Guo", "David Benrimoh"], "title": "Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data", "comment": null, "summary": "Background: Mental illnesses such as depression and anxiety require improved\nmethods for early detection and personalized intervention. Traditional\npredictive models often rely on unimodal data or early fusion strategies that\nfail to capture the complex, multimodal nature of psychiatric data. Advanced\nintegration techniques, such as intermediate (latent space) fusion, may offer\nbetter accuracy and clinical utility. Methods: Using data from the BRIGHTEN\nclinical trial, we evaluated intermediate (latent space) fusion for predicting\ndaily depressive symptoms (PHQ-2 scores). We compared early fusion implemented\nwith a Random Forest (RF) model and intermediate fusion implemented via a\nCombined Model (CM) using autoencoders and a neural network. The dataset\nincluded behavioral (smartphone-based), demographic, and clinical features.\nExperiments were conducted across multiple temporal splits and data stream\ncombinations. Performance was evaluated using mean squared error (MSE) and\ncoefficient of determination (R2). Results: The CM outperformed both RF and\nLinear Regression (LR) baselines across all setups, achieving lower MSE (0.4985\nvs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed\nsigns of overfitting, with a large gap between training and test performance,\nwhile the CM maintained consistent generalization. Performance was best when\nintegrating all data modalities in the CM (in contradistinction to RF),\nunderscoring the value of latent space fusion for capturing non-linear\ninteractions in complex psychiatric datasets. Conclusion: Latent space fusion\noffers a robust alternative to traditional fusion methods for prediction with\nmultimodal mental health data. Future work should explore model\ninterpretability and individual-level prediction for clinical deployment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e2d\u95f4\uff08\u6f5c\u5728\u7a7a\u95f4\uff09\u878d\u5408\u6280\u672f\u9884\u6d4b\u6291\u90c1\u75c7\u72b6\uff0c\u53d1\u73b0\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7cbe\u795e\u6570\u636e\u7684\u591a\u6a21\u6001\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6574\u5408\u6280\u672f\u3002", "method": "\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\uff08RF\uff09\u7684\u65e9\u671f\u878d\u5408\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u5408\u6a21\u578b\uff08CM\uff09\u7684\u4e2d\u95f4\u878d\u5408\u3002", "result": "CM\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\uff0cMSE\u66f4\u4f4e\uff080.4985 vs. 0.5305\uff09\uff0cR2\u66f4\u9ad8\uff080.4695 vs. 0.4356\uff09\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u662f\u591a\u6a21\u6001\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u9884\u6d4b\u7684\u5f3a\u6709\u529b\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u63a2\u7d22\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e2a\u4f53\u5316\u9884\u6d4b\u3002"}}
{"id": "2507.14207", "categories": ["cs.CR", "cs.AI", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.14207", "abs": "https://arxiv.org/abs/2507.14207", "authors": ["Richard M. Charles", "James H. Curry", "Richard B. Charles"], "title": "Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design", "comment": "12 pages, 1 figure", "summary": "The integration of Large Language Models (LLMs) in K--12 education offers\nboth transformative opportunities and emerging risks. This study explores how\nstudents may Trojanize prompts to elicit unsafe or unintended outputs from\nLLMs, bypassing established content moderation systems with safety guardrils.\nThrough a systematic experiment involving simulated K--12 queries and\nmulti-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This\npaper presents our experimental design, detailed findings, and a prototype\ntool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized\neducational prompts. These insights aim to inform both AI safety researchers\nand educational technologists on the safe deployment of LLMs for educators.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86K-12\u6559\u80b2\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5b66\u751f\u53ef\u80fd\u901a\u8fc7\u7279\u6d1b\u4f0a\u5316\u63d0\u793a\u7ed5\u8fc7\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86GPT-3.5\u548cGPT-4\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u68c0\u6d4b\u5de5\u5177TrojanPromptGuard\uff08TPG\uff09\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u7279\u522b\u662f\u5b66\u751f\u5982\u4f55\u5229\u7528\u63d0\u793a\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u3002", "method": "\u901a\u8fc7\u6a21\u62dfK-12\u67e5\u8be2\u548c\u591a\u8f6e\u5bf9\u8bdd\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5206\u6790LLMs\u7684\u6f0f\u6d1e\u3002", "result": "\u63ed\u793a\u4e86GPT-3.5\u548cGPT-4\u7684\u5173\u952e\u8106\u5f31\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u539f\u578b\u5de5\u5177TPG\u4ee5\u68c0\u6d4b\u548c\u7f13\u89e3\u7279\u6d1b\u4f0a\u5316\u63d0\u793a\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u5b89\u5168\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u6280\u672f\u4e13\u5bb6\u63d0\u4f9b\u4e86\u5173\u4e8eLLMs\u5b89\u5168\u90e8\u7f72\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.14240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f9b\u5e94\u94fe\u4e2d\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u7684\u5173\u7cfb\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5f02\u6784\u56fe\u8fdb\u884c\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4f9b\u5e94\u94fe\u7684\u7ed3\u6784\u7279\u5f81\u548c\u52a8\u6001\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u7684\u5f00\u53d1\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5916\u90e8\u6570\u636e\u96c6\uff0c\u53ef\u80fd\u7ee7\u627f\u6f0f\u6d1e\u6216\u504f\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u5176\u4f9b\u5e94\u94fe\u4ee5\u68c0\u6d4b\u98ce\u9669\u3001\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "method": "\u8bbe\u8ba1\u65b9\u6cd5\u7cfb\u7edf\u6536\u96c6LLM\u4f9b\u5e94\u94fe\u6570\u636e\uff0c\u6784\u5efa\u6709\u5411\u5f02\u6784\u56fe\uff08397,376\u8282\u70b9\u548c453,469\u8fb9\uff09\uff0c\u5e76\u8fdb\u884c\u591a\u79cd\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4f9b\u5e94\u94fe\u56fe\u89c4\u6a21\u5927\u3001\u7a00\u758f\u4e14\u7b26\u5408\u5e42\u5f8b\u5206\u5e03\uff1b\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff1b\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u76f8\u4e92\u4f9d\u8d56\uff1b\u56fe\u5177\u6709\u52a8\u6001\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3LLM\u4f9b\u5e94\u94fe\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u98ce\u9669\u68c0\u6d4b\u548c\u6a21\u578b\u4f18\u5316\u3002"}}
{"id": "2507.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "\u751f\u6210\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u5728\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u4e0a\u9886\u5148\uff0c\u4f46\u5b58\u5728\u4e0e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08LRI\uff09\u6216\u771f\u5b9e\u56fe\u50cf\uff08GTI\uff09\u4e0d\u5339\u914d\u7684\u4f2a\u5f71\uff08\u5e7b\u89c9\uff09\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u201c\u5e7b\u89c9\u8bc4\u5206\u201d\uff08HS\uff09\u6765\u8861\u91cf\u548c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4f18\u5316GSR\u6a21\u578b\u3002", "motivation": "GSR\u6a21\u578b\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u7684\u7ec6\u8282\u53ef\u80fd\u4e0eLRI\u6216GTI\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528MLLM\u6784\u5efa\u63d0\u793a\uff0c\u8bc4\u4f30\u5e7b\u89c9\u89c6\u89c9\u5143\u7d20\u5e76\u751f\u6210HS\uff1b\u53d1\u73b0\u67d0\u4e9b\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u5f3a\u76f8\u5173\uff0c\u63d0\u51fa\u7528\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a\u53ef\u5fae\u5956\u52b1\u51fd\u6570\u4f18\u5316GSR\u6a21\u578b\u3002", "result": "HS\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u4e3a\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u8865\u5145\u6027\u89c1\u89e3\uff1b\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u7684\u76f8\u5173\u6027\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7HS\u548c\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4f18\u5316GSR\u6a21\u578b\uff0c\u53ef\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u4e0e\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.14176", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14176", "abs": "https://arxiv.org/abs/2507.14176", "authors": ["Andr\u00e9s Morales-Forero", "Lili J. Rueda", "Ronald Herrera", "Samuel Bassetto", "Eric Coatanea"], "title": "Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection", "comment": null, "summary": "Artificial intelligence (AI) systems increasingly inform medical\ndecision-making, yet concerns about algorithmic bias and inequitable outcomes\npersist, particularly for historically marginalized populations. This paper\nintroduces the concept of Predictive Representativity (PR), a framework of\nfairness auditing that shifts the focus from the composition of the data set to\noutcomes-level equity. Through a case study in dermatology, we evaluated\nAI-based skin cancer classifiers trained on the widely used HAM10000 dataset\nand on an independent clinical dataset (BOSQUE Test set) from Colombia. Our\nanalysis reveals substantial performance disparities by skin phototype, with\nclassifiers consistently underperforming for individuals with darker skin,\ndespite proportional sampling in the source data. We argue that\nrepresentativity must be understood not as a static feature of datasets but as\na dynamic, context-sensitive property of model predictions. PR operationalizes\nthis shift by quantifying how reliably models generalize fairness across\nsubpopulations and deployment contexts. We further propose an External\nTransportability Criterion that formalizes the thresholds for fairness\ngeneralization. Our findings highlight the ethical imperative for post-hoc\nfairness auditing, transparency in dataset documentation, and inclusive model\nvalidation pipelines. This work offers a scalable tool for diagnosing\nstructural inequities in AI systems, contributing to discussions on equity,\ninterpretability, and data justice and fostering a critical re-evaluation of\nfairness in data-driven healthcare.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPredictive Representativity\uff08PR\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u6027\u5ba1\u8ba1\uff0c\u5f3a\u8c03\u4ece\u6570\u636e\u96c6\u7ec4\u6210\u8f6c\u5411\u7ed3\u679c\u5c42\u9762\u7684\u516c\u5e73\u6027\u3002\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0AI\u6a21\u578b\u5728\u6df1\u8272\u76ae\u80a4\u4eba\u7fa4\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u63d0\u51fa\u52a8\u6001\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3AI\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u7b97\u6cd5\u504f\u89c1\u548c\u4e0d\u516c\u5e73\u7ed3\u679c\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5386\u53f2\u4e0a\u8fb9\u7f18\u5316\u4eba\u7fa4\u3002", "method": "\u5f15\u5165PR\u6846\u67b6\uff0c\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6848\u4f8b\uff08HAM10000\u548cBOSQUE\u6570\u636e\u96c6\uff09\u8bc4\u4f30AI\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u3002", "result": "\u53d1\u73b0AI\u6a21\u578b\u5728\u6df1\u8272\u76ae\u80a4\u4eba\u7fa4\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u63d0\u51faExternal Transportability Criterion\u4ee5\u91cf\u5316\u516c\u5e73\u6027\u3002", "conclusion": "\u5f3a\u8c03\u4e8b\u540e\u516c\u5e73\u6027\u5ba1\u8ba1\u3001\u6570\u636e\u96c6\u900f\u660e\u5ea6\u548c\u5305\u5bb9\u6027\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\uff0c\u4e3aAI\u7cfb\u7edf\u7684\u7ed3\u6784\u4e0d\u5e73\u7b49\u63d0\u4f9b\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2507.14212", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14212", "abs": "https://arxiv.org/abs/2507.14212", "authors": ["Federico Mason", "Federico Chiariotti", "Pietro Talli", "Andrea Zanella"], "title": "Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks", "comment": null, "summary": "Goal-oriented Communication (GoC) is a new paradigm that plans data\ntransmission to occur only when it is instrumental for the receiver to achieve\na certain goal. This leads to the advantage of reducing the frequency of\ntransmissions significantly while maintaining adherence to the receiver's\nobjectives. However, GoC scheduling also opens a timing-based side channel that\nan eavesdropper can exploit to obtain information about the state of the\nsystem. This type of attack sidesteps even information-theoretic security, as\nit exploits the timing of updates rather than their content. In this work, we\nstudy such an eavesdropping attack against pull-based goal-oriented scheduling\nfor remote monitoring and control of Markov processes. We provide a theoretical\nframework for defining the effectiveness of the attack and propose possible\ncountermeasures, including two practical heuristics that provide a balance\nbetween the performance gains offered by GoC and the amount of leaked\ninformation. Our results show that, while a naive goal-oriented scheduler\nallows the eavesdropper to correctly guess the system state about 60% of the\ntime, our heuristic defenses can halve the leakage with a marginal reduction of\nthe benefits of goal-oriented approaches.", "AI": {"tldr": "\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\uff08GoC\uff09\u901a\u8fc7\u51cf\u5c11\u4f20\u8f93\u9891\u7387\u4f18\u5316\u901a\u4fe1\uff0c\u4f46\u5b58\u5728\u57fa\u4e8e\u65f6\u95f4\u7684\u4fa7\u4fe1\u9053\u653b\u51fb\u98ce\u9669\u3002\u672c\u6587\u7814\u7a76\u9488\u5bf9GoC\u7684\u7a83\u542c\u653b\u51fb\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u548c\u9632\u5fa1\u63aa\u65bd\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u4fe1\u606f\u6cc4\u9732\u3002", "motivation": "GoC\u867d\u80fd\u4f18\u5316\u901a\u4fe1\u6548\u7387\uff0c\u4f46\u5176\u8c03\u5ea6\u673a\u5236\u53ef\u80fd\u88ab\u7a83\u542c\u8005\u5229\u7528\uff0c\u901a\u8fc7\u65f6\u95f4\u4fe1\u606f\u63a8\u65ad\u7cfb\u7edf\u72b6\u6001\uff0c\u7ed5\u8fc7\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u3002", "method": "\u7814\u7a76\u9488\u5bf9\u57fa\u4e8e\u62c9\u53d6\u7684\u76ee\u6807\u5bfc\u5411\u8c03\u5ea6\u7684\u7a83\u542c\u653b\u51fb\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u8bc4\u4f30\u653b\u51fb\u6548\u679c\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u542f\u53d1\u5f0f\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u539f\u59cbGoC\u8c03\u5ea6\u5bfc\u81f4\u7a83\u542c\u800560%\u51c6\u786e\u7387\uff0c\u800c\u9632\u5fa1\u7b56\u7565\u53ef\u51cf\u5c1150%\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u4fdd\u7559GoC\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u542f\u53d1\u5f0f\u9632\u5fa1\u7b56\u7565\uff0c\u53ef\u5728\u4fdd\u6301GoC\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\u3002"}}
{"id": "2507.14241", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Promptomatix\u662f\u4e00\u4e2a\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6216\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u4f46\u63d0\u793a\u5de5\u7a0b\u4ecd\u4f9d\u8d56\u4eba\u5de5\u4e14\u5bf9\u975e\u4e13\u5bb6\u4e0d\u53cb\u597d\u3002Promptomatix\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Promptomatix\u7ed3\u5408\u5143\u63d0\u793a\u4f18\u5316\u5668\u548cDSPy\u7f16\u8bd1\u5668\uff0c\u5206\u6790\u7528\u6237\u610f\u56fe\u3001\u751f\u6210\u5408\u6210\u6570\u636e\u3001\u9009\u62e9\u63d0\u793a\u7b56\u7565\u5e76\u4f18\u5316\u63d0\u793a\u3002", "result": "\u57285\u4e2a\u4efb\u52a1\u7c7b\u522b\u4e2d\uff0cPromptomatix\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5e93\uff0c\u540c\u65f6\u51cf\u5c11\u63d0\u793a\u957f\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "Promptomatix\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u63d0\u793a\u4f18\u5316\uff0c\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002"}}
{"id": "2507.14368", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallar\u00e8s-L\u00f3pez", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack\u662f\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u7684\u534a\u81ea\u52a8\u5316\u5de5\u5177\u5305\uff0c\u7528\u4e8eB\u8d85\u89c6\u9891\u4e2d\u7684\u70b9\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "B\u8d85\u89c6\u9891\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\u8ddf\u8e2a\u56e0\u566a\u58f0\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u7b49\u95ee\u9898\u800c\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u6280\u672f\uff0c\u63d0\u4f9b\u56fe\u5f62\u754c\u9762\u652f\u6301\u8bad\u7ec3\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u5149\u6d41\u6ee4\u6ce2\u6280\u672f\u3002", "result": "DUSTrack\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u8ddf\u8e2a\u5668\uff0c\u4e0e\u4e13\u7528\u65b9\u6cd5\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e34\u5e8a\u548c\u751f\u7269\u529b\u5b66\u573a\u666f\u3002", "conclusion": "DUSTrack\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u901a\u7528\u7684\u9ad8\u7cbe\u5ea6\u7ec4\u7ec7\u8fd0\u52a8\u8ddf\u8e2a\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14177", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "68T07(Primary), 41A15(Secondary)", "I.2.6; G.1.2"], "pdf": "https://arxiv.org/pdf/2507.14177", "abs": "https://arxiv.org/abs/2507.14177", "authors": ["Changcun Huang"], "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions", "comment": null, "summary": "This paper aims to understand the training solution, which is obtained by the\nback-propagation algorithm, of two-layer neural networks whose hidden layer is\ncomposed of the units with smooth activation functions, including the usual\nsigmoid type most commonly used before the advent of ReLUs. The mechanism\ncontains four main principles: construction of Taylor series expansions, strict\npartial order of knots, smooth-spline implementation and smooth-continuity\nrestriction. The universal approximation for arbitrary input dimensionality is\nproved and experimental verification is given, through which the mystery of\n``black box'' of the solution space is largely revealed. The new proofs\nemployed also enrich approximation theory.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u8bad\u7ec3\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u89e3\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\uff08\u5982sigmoid\uff09\u7684\u9690\u85cf\u5c42\u673a\u5236\uff0c\u63ed\u793a\u4e86\u5176\u89e3\u7a7a\u95f4\u7684\u5965\u79d8\u3002", "motivation": "\u7406\u89e3\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5728\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8bad\u7ec3\u89e3\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u7684\u9690\u85cf\u5c42\uff0c\u4ee5\u63ed\u793a\u5176\u89e3\u7a7a\u95f4\u7684\u795e\u79d8\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u3001\u4e25\u683c\u7684\u8282\u70b9\u504f\u5e8f\u3001\u5e73\u6ed1\u6837\u6761\u5b9e\u73b0\u548c\u5e73\u6ed1\u8fde\u7eed\u6027\u9650\u5236\u56db\u79cd\u673a\u5236\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u610f\u8f93\u5165\u7ef4\u5ea6\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e30\u5bcc\u4e86\u903c\u8fd1\u7406\u8bba\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u89e3\u7a7a\u95f4\u7684\u5965\u79d8\uff0c\u540c\u65f6\u4e3a\u903c\u8fd1\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc1\u660e\u65b9\u6cd5\u3002"}}
{"id": "2507.14213", "categories": ["cs.CR", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.14213", "abs": "https://arxiv.org/abs/2507.14213", "authors": ["Irena Spasojevic", "Federica Celegato", "Alessandro Magni", "Paola Tiberto", "Jordi Sort"], "title": "Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level", "comment": null, "summary": "The Big Data revolution has heightened the demand for robust,\nenergy-efficient security hardware capable of withstanding increasingly\nsophisticated cyber threats. Conventional encryption schemes, reliant on\ncomplex algorithms, are resource-intensive and remain vulnerable. To fortify\nsensitive information, society needs innovative anti-hacking and\nanti-counterfeiting technologies that exploit new materials and designs. Here,\nwe present a magneto-ionic strategy for hardware-level security based on fully\nselective voltage-controlled N3- ion migration within pre-defined, initially\nparamagnetic FeCoN dots. This process generates ferromagnetic sublayers of\ntuneable thickness, resulting in either deterministic (single-domain or vortex)\nor probabilistic states (with coexisting magnetic configurations and\nvoltage-adjustable probabilities), each exhibiting stochastic orientation and\nchirality, thereby providing a rich platform for magnetic fingerprinting. This\napproach enables self-protected primitives, including true random number\ngenerators, physical unclonable functions, and in-memory probabilistic\ninference. The resulting reconfigurable architecture combines tamper\nresistance, low energy consumption, and scalability, marking a significant leap\ntoward next-generation hardware security rooted in emergent magnetic phenomena.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u78c1\u79bb\u5b50\u7b56\u7565\u7684\u786c\u4ef6\u7ea7\u5b89\u5168\u6280\u672f\uff0c\u901a\u8fc7\u7535\u538b\u63a7\u5236\u6c2e\u79bb\u5b50\u8fc1\u79fb\u751f\u6210\u53ef\u8c03\u8c10\u7684\u78c1\u6027\u6307\u7eb9\uff0c\u7528\u4e8e\u6297\u9ed1\u5ba2\u548c\u9632\u4f2a\u3002", "motivation": "\u5927\u6570\u636e\u65f6\u4ee3\u5bf9\u9ad8\u6548\u3001\u5b89\u5168\u7684\u786c\u4ef6\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u52a0\u5bc6\u65b9\u6848\u8d44\u6e90\u5bc6\u96c6\u4e14\u6613\u53d7\u653b\u51fb\uff0c\u9700\u8981\u65b0\u6750\u6599\u548c\u8bbe\u8ba1\u6765\u589e\u5f3a\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u7535\u538b\u63a7\u5236\u6c2e\u79bb\u5b50\u5728FeCoN\u70b9\u4e2d\u7684\u8fc1\u79fb\uff0c\u751f\u6210\u53ef\u8c03\u8c10\u7684\u78c1\u6027\u4e9a\u5c42\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u6216\u6982\u7387\u6027\u72b6\u6001\uff0c\u7528\u4e8e\u78c1\u6027\u6307\u7eb9\u8bc6\u522b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81ea\u4fdd\u62a4\u539f\u8bed\uff08\u5982\u771f\u968f\u673a\u6570\u751f\u6210\u5668\u3001\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570\u7b49\uff09\uff0c\u5177\u6709\u6297\u7be1\u6539\u3001\u4f4e\u80fd\u8017\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8fd9\u79cd\u53ef\u91cd\u6784\u67b6\u6784\u4e3a\u57fa\u4e8e\u78c1\u6027\u73b0\u8c61\u7684\u4e0b\u4e00\u4ee3\u786c\u4ef6\u5b89\u5168\u63d0\u4f9b\u4e86\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2507.14298", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "ChartScope\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6837\u5316\u56fe\u8868\u7c7b\u578b\u4f18\u5316\u7684LVLM\uff0c\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u751f\u6210\u548c\u53cc\u8def\u5f84\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u56fe\u8868\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6ChartDQA\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u6570\u636e\u5bf9\u9f50\u9884\u8bad\u7ec3\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9ad8\u6548\u6570\u636e\u751f\u6210\u7ba1\u9053\u548c\u53cc\u8def\u5f84\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u5e95\u5c42\u6570\u636e\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eChartScope\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u591a\u6837\u5316\u56fe\u8868\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "ChartScope\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u5c40\u9650\u6027\uff0c\u5e76\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.14426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u53ef\u7528\u6027\u57fa\u7840\uff0c\u901a\u8fc7\u7ed3\u5408\u5e38\u8bc6\u5148\u9a8c\u548c\u89c6\u89c9\u8bc1\u636e\uff0c\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u3002", "motivation": "\u63d0\u5347\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u73b0\u66f4\u7a33\u5065\u548c\u53ef\u4fe1\u8d56\u7684\u51b3\u7b56\u3002", "method": "\u6574\u5408ConceptNet\u548c\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u5e38\u8bc6\u5148\u9a8c\u4e0eCLIP\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u57fa\u4e8e\u80fd\u91cf\u7684\u63a8\u7406\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u591a\u5bf9\u8c61\u3001\u65e0\u6807\u7b7e\u8bbe\u7f6e\u4e2d\uff0cCRAFT\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CRAFT\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u4e14\u76ee\u6807\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u7a33\u5065\u548c\u53ef\u4fe1\u8d56\u7684\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2507.14178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14178", "abs": "https://arxiv.org/abs/2507.14178", "authors": ["Yuhang Liu", "Yuefei Wu", "Bin Shi", "Bo Dong"], "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution Detection", "comment": "8 pages, 5 figures", "summary": "Out-of-distribution (OOD) detection is critical to ensuring the reliability\nof deep learning applications and has attracted significant attention in recent\nyears. A rich body of literature has emerged to develop efficient score\nfunctions that assign high scores to in-distribution (ID) samples and low\nscores to OOD samples, thereby helping distinguish OOD samples. Among these\nmethods, distance-based score functions are widely used because of their\nefficiency and ease of use. However, deep learning often leads to a biased\ndistribution of data features, and extreme features are inevitable. These\nextreme features make the distance-based methods tend to assign too low scores\nto ID samples. This limits the OOD detection capabilities of such methods. To\naddress this issue, we propose a simple yet effective method, Feature Bank\nEnhancement (FBE), that uses statistical characteristics from dataset to\nidentify and constrain extreme features to the separation boundaries, therapy\nmaking the distance between samples inside and outside the distribution\nfarther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10\nrespectively, and the results show that our method achieves state-of-the-art\nperformance on both benchmark. Additionally, theoretical analysis and\nsupplementary experiments are conducted to provide more insights into our\nmethod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFeature Bank Enhancement (FBE)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6781\u7aef\u7279\u5f81\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6570\u636e\u7279\u5f81\u5206\u5e03\u504f\u5dee\u548c\u6781\u7aef\u7279\u5f81\u7684\u5b58\u5728\uff0c\u5bfc\u81f4\u5bf9ID\u6837\u672c\u8bc4\u5206\u8fc7\u4f4e\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "method": "FBE\u5229\u7528\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u6027\u8bc6\u522b\u5e76\u7ea6\u675f\u6781\u7aef\u7279\u5f81\uff0c\u6269\u5927\u5206\u5e03\u5185\u5916\u6837\u672c\u7684\u8ddd\u79bb\u3002", "result": "\u5728ImageNet-1k\u548cCIFAR-10\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFBE\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FBE\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8865\u5145\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.14222", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14222", "abs": "https://arxiv.org/abs/2507.14222", "authors": ["Shu-Ting Huang", "Wen-Cheng Chung", "Hao-Ting Pai"], "title": "GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics", "comment": "ACM CCS 2025 (Submitted)", "summary": "The Interpretable Generalization (IG) mechanism recently published in IEEE\nTransactions on Information Forensics and Security delivers state-of-the-art,\nevidence-based intrusion detection by discovering coherent normal and attack\npatterns through exhaustive intersect-and-subset operations-yet its cubic-time\ncomplexity and large intermediate bitsets render full-scale datasets\nimpractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads\nall pairwise intersections and subset evaluations to commodity GPUs.\nImplemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,\nIG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.\nIn the full size of NSL-KDD (148k-record), given small training data (e.g.,\n10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,\nPrecision 0.973, and AUC 0.961, whereas IG required down-sampling to\n15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision\n0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales\nand could provide millisecond-level per-flow inference once patterns are\nlearned. IG-GPU thus bridges the gap between rigorous interpretability and\nreal-time cyber-defense, offering a portable foundation for future work on\nhardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity\noptimizations.", "AI": {"tldr": "IG-GPU\u662f\u4e00\u79cd\u57fa\u4e8eGPU\u7684PyTorch\u91cd\u6784\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86IG\u673a\u5236\u5728\u5165\u4fb5\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86116\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3IG\u673a\u5236\u5728CPU\u4e0a\u8fd0\u884c\u65f6\u56e0\u7acb\u65b9\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5927\u4e2d\u95f4\u4f4d\u96c6\u5bfc\u81f4\u7684\u5168\u89c4\u6a21\u6570\u636e\u96c6\u5904\u7406\u4e0d\u5207\u5b9e\u9645\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7PyTorch\u91cd\u6784IG\u673a\u5236\uff0c\u5c06\u6240\u6709\u6210\u5bf9\u4ea4\u96c6\u548c\u5b50\u96c6\u8bc4\u4f30\u4efb\u52a1\u5378\u8f7d\u5230GPU\u4e0a\u3002", "result": "\u5728NSL-KDD\u6570\u636e\u96c6\u4e0a\uff0cIG-GPU\u5b9e\u73b0\u4e86116\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5728\u5168\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u9ad8\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cAUC\u503c\u3002", "conclusion": "IG-GPU\u586b\u8865\u4e86\u4e25\u683c\u53ef\u89e3\u91ca\u6027\u4e0e\u5b9e\u65f6\u7f51\u7edc\u9632\u5fa1\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u672a\u6765\u786c\u4ef6\u611f\u77e5\u8c03\u5ea6\u548c\u591aGPU\u5206\u7247\u63d0\u4f9b\u4e86\u4fbf\u643a\u57fa\u7840\u3002"}}
{"id": "2507.14304", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u9009\u62e9\u6027\u7ffb\u8bd1\u63d0\u5347\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\uff09\u4e0a\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u7ffb\u8bd1\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6570\u636e\u7a00\u7f3a\uff0c\u7ffb\u8bd1\u73b0\u6709\u82f1\u8bed\u6570\u636e\u65f6\u96be\u4ee5\u4fdd\u7559\u5173\u952e\u5185\u5bb9\uff08\u5982\u4ee3\u7801\u3001\u6570\u5b66\u8868\u8fbe\u5f0f\uff09\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9009\u62e9\u6027\u7ffb\u8bd1\u6280\u672f\uff0c\u4ec5\u7ffb\u8bd1\u53ef\u7ffb\u8bd1\u90e8\u5206\uff0c\u4fdd\u7559\u975e\u7ffb\u8bd1\u5185\u5bb9\u548c\u53e5\u5b50\u7ed3\u6784\uff0c\u5e76\u4e0e\u4f20\u7edf\u7ffb\u8bd1\u65b9\u6cd5\uff08\u5982GCP\uff09\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u9009\u62e9\u6027\u7ffb\u8bd1\u5728\u5370\u5730\u8bed\u4e0a\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u7ffb\u8bd1\uff0c\u4e14\u6df7\u5408\u7ffb\u8bd1\u6837\u672c\u4e0e\u539f\u59cb\u82f1\u8bed\u6570\u636e\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u9009\u62e9\u6027\u7ffb\u8bd1\u662f\u63d0\u5347\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u5b9e\u7528\u4e14\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.14432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u5e26\u5bbd\u9002\u5e94\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "3DGS\u89c6\u9891\u56e0\u6570\u636e\u91cf\u5927\u548c\u4f20\u8f93\u590d\u6742\u5ea6\u9ad8\uff0c\u5bf9\u6d41\u5a92\u4f53\u63d0\u51fa\u6311\u6218\uff0c\u9700\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u3001\u538b\u7f29\u6548\u7387\u548c\u4f20\u8f93\u901f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863DGS\u89c6\u9891\u6d41\u4f20\u8f93\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14179", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14179", "abs": "https://arxiv.org/abs/2507.14179", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u4e3a\u5c11\u91cf\u4ee3\u8868\u6027\u805a\u7c7b\uff0c\u9ad8\u6548\u9884\u6d4b\u548c\u5229\u7528LLMs\u4e2d\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u4e3a\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u76f4\u63a5\u9884\u6d4b\u795e\u7ecf\u5143\u7ea7\u6fc0\u6d3b\u6a21\u5f0f\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u805a\u7c7b\u65b9\u6cd5\uff0c\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u4e3a\u5c11\u91cf\u4ee3\u8868\u6027\u805a\u7c7b\uff0c\u901a\u8fc7\u9884\u6d4b\u805a\u7c7b\u5206\u914d\u800c\u975e\u5355\u4e2a\u795e\u7ecf\u5143\u72b6\u6001\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u805a\u7c7b\u7cbe\u5ea6\u9ad8\u8fbe79.34%\uff0c\u4f18\u4e8e\u6807\u51c6\u4e8c\u5143\u805a\u7c7b\u65b9\u6cd5\uff0c\u540c\u65f6\u56f0\u60d1\u5ea6\uff08PPL\uff09\u5f97\u5206\u6700\u4f4e\u4e3a12.49\uff0c\u8868\u660e\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6fc0\u6d3b\u6a21\u5f0f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2507.14223", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14223", "abs": "https://arxiv.org/abs/2507.14223", "authors": ["Wen-Cheng Chung", "Shu-Ting Huang", "Hao-Ting Pai"], "title": "Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification", "comment": "ACM CCS 2025 (Submitted)", "summary": "Explainable intrusion detection systems (IDS) are now recognized as essential\nfor mission-critical networks, yet most \"XAI\" pipelines still bolt an\napproximate explainer onto an opaque classifier, leaving analysts with partial\nand sometimes misleading insights. The Interpretable Generalization (IG)\nmechanism, published in IEEE Transactions on Information Forensics and\nSecurity, eliminates that bottleneck by learning coherent patterns - feature\ncombinations unique to benign or malicious traffic - and turning them into\nfully auditable rules. IG already delivers outstanding precision, recall, and\nAUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the\ndata. To raise precision further without sacrificing transparency, we introduce\nMulti-Granular Discretization (IG-MD), which represents every continuous\nfeature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts\nprecision by greater than or equal to 4 percentage points across all nine\ntrain-test splits while preserving recall approximately equal to 1.0,\ndemonstrating that a single interpretation-ready model can scale across domains\nwithout bespoke tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7Interpretable Generalization\uff08IG\uff09\u673a\u5236\u5b66\u4e60\u6e05\u6670\u7684\u7279\u5f81\u7ec4\u5408\u89c4\u5219\uff0c\u5e76\u5f15\u5165Multi-Granular Discretization\uff08IG-MD\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\u901a\u5e38\u4ec5\u5728\u5206\u7c7b\u5668\u4e0a\u9644\u52a0\u89e3\u91ca\u5668\uff0c\u5bfc\u81f4\u5206\u6790\u7ed3\u679c\u4e0d\u5b8c\u6574\u6216\u8bef\u5bfc\u3002", "method": "IG\u673a\u5236\u5b66\u4e60\u72ec\u7279\u7684\u7279\u5f81\u7ec4\u5408\u89c4\u5219\uff0cIG-MD\u901a\u8fc7\u591a\u7c92\u5ea6\u79bb\u6563\u5316\u5904\u7406\u8fde\u7eed\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cIG-MD\u5728UKM-IDS20\u4e0a\u63d0\u5347\u7cbe\u5ea64\u4e2a\u767e\u5206\u70b9\uff0c\u53ec\u56de\u7387\u4fdd\u63011.0\u3002", "conclusion": "IG-MD\u8bc1\u660e\u4e86\u5355\u4e00\u53ef\u89e3\u91ca\u6a21\u578b\u53ef\u8de8\u9886\u57df\u6269\u5c55\uff0c\u65e0\u9700\u5b9a\u5236\u8c03\u6574\u3002"}}
{"id": "2507.14307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u8bed\u8a00\u65f6\u95f4\u610f\u4e49\u7684\u65b9\u5f0f\uff0c\u53d1\u73b0\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u5dee\u5f02\uff0c\u5e76\u5f00\u53d1\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u4ee5\u4eba\u7c7b\u65b9\u5f0f\u5904\u7406\u8bed\u8a00\u65f6\u95f4\u610f\u4e49\uff0c\u63ed\u793a\u5176\u8ba4\u77e5\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u53c2\u4e0e\u5faa\u73af\u7684\u63a2\u6d4b\u6d41\u7a0b\uff0c\u8fdb\u884c\u9488\u5bf9\u6027\u5b9e\u9a8c\uff0c\u8bc4\u4f30LLMs\u7684\u8bed\u4e49\u548c\u8bed\u7528\u63a8\u7406\u80fd\u529b\u3002", "result": "LLMs\u8fc7\u5ea6\u4f9d\u8d56\u5178\u578b\u6027\uff0c\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u65f6\u95f4\u5224\u65ad\uff0c\u4e14\u56e0\u679c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u8868\u660e\u5176\u53d9\u4e8b\u7406\u89e3\u6709\u9650\u3002", "conclusion": "LLMs\u5904\u7406\u8bed\u8a00\u65f6\u95f4\u610f\u4e49\u7684\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u8ba4\u77e5\u673a\u5236\u3002"}}
{"id": "2507.14449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT\u662f\u9996\u4e2a\u9488\u5bf9\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e26\u4e07\u5bf9\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\uff08IR-TD\uff09\uff0c\u901a\u8fc7\u53cc\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u57289\u9879\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u7ea2\u5916\u56fe\u50cf\u3001\u65e0\u6cd5\u6355\u6349\u7ea2\u5916\u6a21\u6001\u72ec\u7279\u7279\u6027\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21IR-TD\u6570\u636e\u96c6\uff0c\u7ed3\u5408LLM\u751f\u6210\u548c\u89c4\u5219\u6807\u6ce8\u7684\u6587\u672c\uff0c\u63d0\u51fa\u53cc\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u57289\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u8bc6\u522b\u548c\u5b9a\u4f4d\u7b49\u3002", "conclusion": "IRGPT\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2507.14180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14180", "abs": "https://arxiv.org/abs/2507.14180", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems", "comment": null, "summary": "In line with the AI-native 6G vision, explainability and robustness are\ncrucial for building trust and ensuring reliable performance in millimeter-wave\n(mmWave) systems. Efficient beam alignment is essential for initial access, but\ndeep learning (DL) solutions face challenges, including high data collection\noverhead, hardware constraints, lack of explainability, and susceptibility to\nadversarial attacks. This paper proposes a robust and explainable DL-based beam\nalignment engine (BAE) for mmWave multiple-input multiple output (MIMO)\nsystems. The BAE uses received signal strength indicator (RSSI) measurements\nfrom wide beams to predict the best narrow beam, reducing the overhead of\nexhaustive beam sweeping. To overcome the challenge of real-world data\ncollection, this work leverages a site-specific digital twin (DT) to generate\nsynthetic channel data closely resembling real-world environments. A model\nrefinement via transfer learning is proposed to fine-tune the pre-trained model\nresiding in the DT with minimal real-world data, effectively bridging\nmismatches between the digital replica and real-world environments. To reduce\nbeam training overhead and enhance transparency, the framework uses deep\nShapley additive explanations (SHAP) to rank input features by importance,\nprioritizing key spatial directions and minimizing beam sweeping. It also\nincorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a\ncredibility metric for detecting out-of-distribution inputs and ensuring\nrobust, transparent decision-making. Experimental results show that the\nproposed framework reduces real-world data needs by 70%, beam training overhead\nby 62%, and improves outlier detection robustness by up to 8.5x, achieving\nnear-optimal spectral efficiency and transparent decision making compared to\ntraditional softmax based DL models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u6ce2\u675f\u5bf9\u51c6\u5f15\u64ce\uff08BAE\uff09\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u548c\u8fc1\u79fb\u5b66\u4e60\u51cf\u5c11\u6570\u636e\u9700\u6c42\uff0c\u5229\u7528SHAP\u548cDkNN\u63d0\u5347\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u57286G\u613f\u666f\u4e0b\uff0c\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u5efa\u7acb\u4fe1\u4efb\u548c\u786e\u4fdd\u53ef\u9760\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709DL\u89e3\u51b3\u65b9\u6848\u9762\u4e34\u6570\u636e\u6536\u96c6\u5f00\u9500\u5927\u3001\u786c\u4ef6\u9650\u5236\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u53ca\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u7b49\u95ee\u9898\u3002", "method": "BAE\u5229\u7528\u5bbd\u6ce2\u675f\u7684RSSI\u6d4b\u91cf\u9884\u6d4b\u6700\u4f73\u7a84\u6ce2\u675f\uff0c\u51cf\u5c11\u6ce2\u675f\u626b\u63cf\u5f00\u9500\uff1b\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u4f18\u5316\u6a21\u578b\uff1b\u91c7\u7528SHAP\u548cDkNN\u63d0\u5347\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u51cf\u5c1170%\u771f\u5b9e\u6570\u636e\u9700\u6c42\u300162%\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\uff0c\u5f02\u5e38\u68c0\u6d4b\u9c81\u68d2\u6027\u63d0\u53478.5\u500d\uff0c\u63a5\u8fd1\u6700\u4f18\u9891\u8c31\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u51cf\u5c11\u5f00\u9500\u3001\u63d0\u5347\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u6ce2\u675f\u5bf9\u51c6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14229", "abs": "https://arxiv.org/abs/2507.14229", "authors": ["Vanja Stojanovi\u0107", "\u017diga Lesar", "CIril Bohak"], "title": "Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently", "comment": null, "summary": "We investigate the cryptanalysis of affine ciphers using a hybrid neural\nnetwork architecture that combines modular arithmetic-aware and statistical\nfeature-based learning. Inspired by recent advances in interpretable neural\nnetworks for modular arithmetic and neural cryptanalysis of classical ciphers,\nour approach integrates a modular branch that processes raw ciphertext\nsequences and a statistical branch that leverages letter frequency features.\nExperiments on datasets derived from natural English text demonstrate that the\nhybrid model attains high key recovery accuracy for short and moderate\nciphertexts, outperforming purely statistical approaches for the affine cipher.\nHowever, performance degrades for very long ciphertexts, highlighting\nchallenges in model generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u7b97\u672f\u611f\u77e5\u548c\u7edf\u8ba1\u7279\u5f81\u5b66\u4e60\uff0c\u7528\u4e8e\u5206\u6790\u4eff\u5c04\u5bc6\u7801\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u5bc6\u6587\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u957f\u5bc6\u6587\u6548\u679c\u4e0b\u964d\u3002", "motivation": "\u53d7\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u7b97\u672f\u548c\u7ecf\u5178\u5bc6\u7801\u795e\u7ecf\u5bc6\u7801\u5206\u6790\u4e2d\u7684\u8fdb\u5c55\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u4eff\u5c04\u5bc6\u7801\u7684\u5bc6\u94a5\u6062\u590d\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff0c\u5305\u542b\u5904\u7406\u539f\u59cb\u5bc6\u6587\u7684\u6a21\u7b97\u672f\u5206\u652f\u548c\u5229\u7528\u5b57\u6bcd\u9891\u7387\u7279\u5f81\u7684\u7edf\u8ba1\u5206\u652f\u3002", "result": "\u5728\u81ea\u7136\u82f1\u8bed\u6587\u672c\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5bf9\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u5bc6\u6587\u5b9e\u73b0\u9ad8\u5bc6\u94a5\u6062\u590d\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7eaf\u7edf\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u5728\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u5bc6\u6587\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u957f\u5bc6\u6587\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.14314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija An\u0111edeli\u0107", "Dominik \u0160ipek", "Laura Majer", "Jan \u0160najder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGestalt\u539f\u5219\u7684\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc\uff08GPI-Net\uff09\uff0c\u901a\u8fc7\u6b63\u4ea4\u51e0\u4f55\u4e00\u81f4\u6027\u4f18\u5316\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u7684\u878d\u5408\uff0c\u63d0\u5347\u70b9\u4e91\u914d\u51c6\u4e2d\u9ad8\u8d28\u91cf\u5bf9\u5e94\u5173\u7cfb\u7684\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u70b9\u4e91\u914d\u51c6\u4e2d\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\u7684\u6311\u6218\uff0c\u5982\u7279\u5f81\u5197\u4f59\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u5229\u7528Gestalt\u539f\u5219\u8bbe\u8ba1GPI-Net\uff0c\u5305\u62ec\u6b63\u4ea4\u96c6\u6210\u7b56\u7565\u3001Gestalt\u7279\u5f81\u6ce8\u610f\u529b\u5757\uff08GFA\uff09\u548c\u53cc\u8def\u5f84\u591a\u7c92\u5ea6\u5e76\u884c\u4ea4\u4e92\u805a\u5408\u5757\uff08DMG\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPI-Net\u901a\u8fc7Gestalt\u539f\u5219\u6709\u6548\u4f18\u5316\u4e86\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u7684\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u914d\u51c6\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14181", "abs": "https://arxiv.org/abs/2507.14181", "authors": ["Yajiao Dai", "Jun Li", "Zhen Mei", "Yiyang Ni", "Shi Jin", "Zengxiang Li", "Sheng Guo", "Wei Xiang"], "title": "Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis", "comment": "Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\n  5 figures", "summary": "Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\noperation of industrial machinery and improving production efficiency. However,\ntraditional supervised deep learning methods require a large amount of training\ndata and labels, which are often located in different clients. Additionally,\nthe cost of data labeling is high, making labels difficult to acquire.\nMeanwhile, differences in data distribution among clients may also hinder the\nmodel's performance. To tackle these challenges, this paper proposes a\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\ncontrastive loss and soft labeling to address data and label scarcity for\ndistributed clients with few labeled samples while safeguarding user privacy.\nIt enables representation learning using unlabeled data on the client side and\nfacilitates joint learning among clients through prototypes, thereby achieving\nmutual knowledge sharing and preventing local model divergence. Specifically,\nfirst, a sample weighting function based on the Laplace distribution is\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\nsemi-supervised training process. Second, a dual contrastive loss is introduced\nto mitigate model divergence caused by different data distributions, comprising\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\naggregated on the server with weighted averaging and updated with momentum to\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\nexperiments are conducted on two publicly available datasets and a dataset\ncollected on motors from the factory. In the most challenging task, where only\n10\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\n1.15% to 7.85% over state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6SSFL-DCSL\uff0c\u901a\u8fc7\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u89e3\u51b3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u6807\u7b7e\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7684\u6837\u672c\u52a0\u6743\u51fd\u6570\u3001\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\uff08\u5c40\u90e8\u548c\u5168\u5c40\uff09\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u548c\u52a8\u91cf\u66f4\u65b0\u539f\u578b\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5728\u4ec510%\u6570\u636e\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0cSSFL-DCSL\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53471.15%\u81f37.85%\u3002", "conclusion": "SSFL-DCSL\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.14248", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG", "I.2.10; I.2.6; I.5.1; D.4.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14248", "abs": "https://arxiv.org/abs/2507.14248", "authors": ["Eldor Abdukhamidov", "Mohammed Abuhamad", "Simon S. Woo", "Hyoungshick Kim", "Tamer Abuhmed"], "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack", "comment": null, "summary": "Vision transformer (ViT) models, when coupled with interpretation models, are\nregarded as secure and challenging to deceive, making them well-suited for\nsecurity-critical domains such as medical applications, autonomous vehicles,\ndrones, and robotics. However, successful attacks on these systems can lead to\nsevere consequences. Recent research on threats targeting ViT models primarily\nfocuses on generating the smallest adversarial perturbations that can deceive\nthe models with high confidence, without considering their impact on model\ninterpretations. Nevertheless, the use of interpretation models can effectively\nassist in detecting adversarial examples. This study investigates the\nvulnerability of transformer models to adversarial attacks, even when combined\nwith interpretation models. We propose an attack called \"AdViT\" that generates\nadversarial examples capable of misleading both a given transformer model and\nits coupled interpretation model. Through extensive experiments on various\ntransformer models and two transformer-based interpreters, we demonstrate that\nAdViT achieves a 100% attack success rate in both white-box and black-box\nscenarios. In white-box scenarios, it reaches up to 98% misclassification\nconfidence, while in black-box scenarios, it reaches up to 76%\nmisclassification confidence. Remarkably, AdViT consistently generates accurate\ninterpretations in both scenarios, making the adversarial examples more\ndifficult to detect.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdViT\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u6b3a\u9a97\u89c6\u89c9Transformer\u6a21\u578b\u53ca\u5176\u89e3\u91ca\u6a21\u578b\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe100%\uff0c\u4e14\u5728\u9ed1\u767d\u76d2\u573a\u666f\u4e0b\u5747\u751f\u6210\u51c6\u786e\u89e3\u91ca\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9Transformer\u6a21\u578b\u7ed3\u5408\u89e3\u91ca\u6a21\u578b\u88ab\u89c6\u4e3a\u5b89\u5168\uff0c\u4f46\u73b0\u6709\u653b\u51fb\u7814\u7a76\u672a\u8003\u8651\u5bf9\u89e3\u91ca\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faAdViT\u653b\u51fb\u65b9\u6cd5\uff0c\u751f\u6210\u80fd\u540c\u65f6\u8bef\u5bfcTransformer\u6a21\u578b\u53ca\u5176\u89e3\u91ca\u6a21\u578b\u7684\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAdViT\u5728\u9ed1\u767d\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u8fbe100%\uff0c\u4e14\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u89e3\u91ca\u51c6\u786e\uff0c\u96be\u4ee5\u68c0\u6d4b\u3002", "conclusion": "AdViT\u63ed\u793a\u4e86Transformer\u6a21\u578b\u5373\u4f7f\u7ed3\u5408\u89e3\u91ca\u6a21\u578b\u4ecd\u5b58\u5728\u6f0f\u6d1e\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2507.14355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4eba\u683c\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5c3d\u7ba1\u6a21\u578b\u5177\u6709\u9ad8\u91cd\u6d4b\u4fe1\u5ea6\uff0c\u4f46\u5728\u6784\u5ff5\u6548\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u4eba\u683c\u7279\u8d28\u63a8\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4ee5\u5f80\u7814\u7a76\u4e2d\u5408\u6210\u6570\u636e\u6216\u7f3a\u4e4f\u5fc3\u7406\u6d4b\u91cf\u6548\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528555\u4e2a\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548cBFI-10\u81ea\u8bc4\u5206\u6570\u4f5c\u4e3a\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cdLLM\uff08GPT-4.1 Mini\u3001Meta-LLaMA\u548cDeepSeek\uff09\u7684\u96f6\u6837\u672c\u63d0\u793a\u548c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u91cd\u6d4b\u4fe1\u5ea6\u9ad8\uff0c\u4f46\u6784\u5ff5\u6548\u5ea6\u4f4e\uff08\u6700\u5927Pearson's r=0.27\uff09\uff0c\u9884\u6d4b\u504f\u5411\u4e2d\u7b49\u6216\u9ad8\u7279\u8d28\u6c34\u5e73\uff0c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u4ec5\u7565\u5fae\u6539\u5584\u5206\u5e03\u5bf9\u9f50\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u4eba\u683c\u63a8\u65ad\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u57fa\u4e8e\u8bc1\u636e\u7684\u5f00\u53d1\u4ee5\u63d0\u5347\u5fc3\u7406\u5b66\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2507.14454", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u5206\u6790\u7684\u81ea\u9002\u5e943D\u9ad8\u65af\u6e85\u5c04\u89c6\u9891\uff083DGS\uff09\u5206\u5757\u6280\u672f\uff0c\u7ed3\u5408\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\u3002", "motivation": "3DGS\u89c6\u9891\u6d41\u5728\u63d0\u4f9b\u6c89\u6d78\u5f0f3D\u89c6\u9891\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9762\u4e34\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u9002\u5e94\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\u3001\u65b0\u578b\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u4e3a3DGS\u89c6\u9891\u6d41\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.14182", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14182", "abs": "https://arxiv.org/abs/2507.14182", "authors": ["Xiaotong Luo", "Shengda Zhuo", "Min Chen", "Lichun Li", "Ruizhao Lu", "Wenqi Fan", "Shuqiang Huang", "Yin Tang"], "title": "From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling", "comment": null, "summary": "Financial markets exhibit highly dynamic and complex behaviors shaped by both\nhistorical price trajectories and exogenous narratives, such as news, policy\ninterpretations, and social media sentiment. The heterogeneity in these data\nand the diverse insight of investors introduce biases that complicate the\nmodeling of market dynamics. Unlike prior work, this paper explores the\npotential of bull and bear regimes in investor-driven market dynamics. Through\nempirical analysis on real-world financial datasets, we uncover a dynamic\nrelationship between bias variation and behavioral adaptation, which enhances\ntrend prediction under evolving market conditions. To model this mechanism, we\npropose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified\nframework that jointly embeds temporal price sequences and external contextual\nsignals into a shared latent space where opposing bull and bear forces\nnaturally emerge, forming the foundation for bias representation. Within this\nspace, an inertial pairing module pairs temporally adjacent samples to preserve\nmomentum, while the dual competition mechanism contrasts bullish and bearish\nembeddings to capture behavioral divergence. Together, these components allow\nB4 to model bias-driven asymmetry, behavioral inertia, and market\nheterogeneity. Experimental results on real-world financial datasets\ndemonstrate that our model not only achieves superior performance in predicting\nmarket trends but also provides interpretable insights into the interplay of\nbiases, investor behaviors, and market dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faB4\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u4fe1\u53f7\uff0c\u6355\u6349\u6295\u8d44\u8005\u504f\u89c1\u4e0e\u884c\u4e3a\u52a8\u6001\uff0c\u63d0\u5347\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u884c\u4e3a\u590d\u6742\uff0c\u53d7\u5386\u53f2\u4ef7\u683c\u548c\u5916\u90e8\u53d9\u4e8b\uff08\u5982\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\uff09\u5f71\u54cd\uff0c\u6295\u8d44\u8005\u504f\u89c1\u4f7f\u5efa\u6a21\u56f0\u96be\u3002\u672c\u6587\u63a2\u7d22\u725b\u5e02\u548c\u718a\u5e02\u673a\u5236\u5728\u6295\u8d44\u8005\u9a71\u52a8\u5e02\u573a\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51faB4\u6a21\u578b\uff0c\u5c06\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u4fe1\u53f7\u5d4c\u5165\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u60ef\u6027\u914d\u5bf9\u548c\u53cc\u7ade\u4e89\u673a\u5236\u6355\u6349\u504f\u89c1\u9a71\u52a8\u7684\u975e\u5bf9\u79f0\u6027\u548c\u884c\u4e3a\u60ef\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eB4\u5728\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u4f9b\u504f\u89c1\u3001\u884c\u4e3a\u548c\u5e02\u573a\u52a8\u6001\u5173\u7cfb\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "B4\u6a21\u578b\u6709\u6548\u5efa\u6a21\u5e02\u573a\u52a8\u6001\uff0c\u4e3a\u6295\u8d44\u8005\u884c\u4e3a\u548c\u504f\u89c1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.14324", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14324", "abs": "https://arxiv.org/abs/2507.14324", "authors": ["Yao Ma", "Wen Yu Kon", "Jefferson Chu", "Kevin Han Yong Loh", "Kaushik Chakraborty", "Charles Lim"], "title": "Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems", "comment": null, "summary": "Identity verification is the process of confirming an individual's claimed\nidentity, which is essential in sectors like finance, healthcare, and online\nservices to ensure security and prevent fraud. However, current\npassword/PIN-based identity solutions are susceptible to phishing or skimming\nattacks, where malicious intermediaries attempt to steal credentials using fake\nidentification portals. Alikhani et al. [Nature, 2021] began exploring identity\nverification through graph coloring-based relativistic zero-knowledge proofs\n(RZKPs), a key cryptographic primitive that enables a prover to demonstrate\nknowledge of secret credentials to a verifier without disclosing any\ninformation about the secret. Our work advances this field and addresses\nunresolved issues: From an engineering perspective, we relax further the\nrelativistic constraints from 60m to 30m, and significantly enhance the\nstability and scalability of the experimental demonstration of the 2-prover\ngraph coloring-based RZKP protocol for near-term use cases. At the same time,\nfor long-term security against entangled malicious provers, we propose a\nmodified protocol with comparable computation and communication costs, we\nestablish an upper bound on the soundness parameter for this modified protocol.\nOn the other hand, we extend the two-prover, two-verifier setup to a\nthree-prover configuration, demonstrating the security of such relativistic\nprotocols against entangled malicious provers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6539\u8fdb\u4e86\u57fa\u4e8e\u56fe\u7740\u8272\u7684\u76f8\u5bf9\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08RZKP\uff09\u534f\u8bae\uff0c\u964d\u4f4e\u4e86\u76f8\u5bf9\u7ea6\u675f\u6761\u4ef6\uff08\u4ece60\u7c73\u523030\u7c73\uff09\uff0c\u5e76\u589e\u5f3a\u4e86\u534f\u8bae\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u540c\u65f6\uff0c\u9488\u5bf9\u957f\u671f\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u534f\u8bae\u5e76\u6269\u5c55\u4e86\u591a\u9a8c\u8bc1\u8005\u914d\u7f6e\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5bc6\u7801/PIN\u7684\u8eab\u4efd\u9a8c\u8bc1\u65b9\u6cd5\u6613\u53d7\u9493\u9c7c\u653b\u51fb\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7740\u8272\u7684\u76f8\u5bf9\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08RZKP\uff09\u534f\u8bae\uff0c\u4f18\u5316\u5de5\u7a0b\u5b9e\u73b0\u5e76\u6269\u5c55\u591a\u9a8c\u8bc1\u8005\u914d\u7f6e\u3002", "result": "\u6210\u529f\u964d\u4f4e\u4e86\u76f8\u5bf9\u7ea6\u675f\u6761\u4ef6\uff0c\u63d0\u5347\u4e86\u534f\u8bae\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u591a\u9a8c\u8bc1\u8005\u914d\u7f6e\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8eab\u4efd\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8fd1\u957f\u671f\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.14372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86LinkedIn\u5f00\u53d1\u7684\u5185\u90e8\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548cText-to-SQL\u4ee3\u7406\u5b9e\u73b0\u6570\u636e\u81ea\u52a9\u67e5\u8be2\uff0c\u652f\u6301\u591a\u79cd\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Text-to-SQL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u6784\u5efa\u4f01\u4e1a\u7ea7\u89e3\u51b3\u65b9\u6848\u4ecd\u5177\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u3001\u5f00\u53d1Text-to-SQL\u4ee3\u7406\uff08\u652f\u6301\u68c0\u7d22\u3001\u6392\u540d\u3001\u67e5\u8be2\u751f\u6210\u548c\u9519\u8bef\u4fee\u6b63\uff09\u4ee5\u53ca\u8bbe\u8ba1\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\u3002", "result": "\u804a\u5929\u673a\u5668\u4eba\u6bcf\u5468\u6709300\u591a\u540d\u7528\u6237\uff0c53%\u7684\u54cd\u5e94\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u6b63\u786e\u6216\u63a5\u8fd1\u6b63\u786e\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u5173\u952e\u7ec4\u4ef6\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4f01\u4e1a\u7ea7Text-to-SQL\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2507.14456", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "GEMINUS\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4e13\u5bb6\u548c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u7ed3\u5408\u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u3002", "motivation": "\u73b0\u6709\u5355\u6a21\u5f0f\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u573a\u666f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u590d\u6742\u4ea4\u901a\u73af\u5883\u7684\u6846\u67b6\u3002", "method": "GEMINUS\u5305\u542b\u5168\u5c40\u4e13\u5bb6\u3001\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u548c\u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u52a8\u6001\u6fc0\u6d3b\u4e13\u5bb6\u6a21\u5757\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a7e\u9a76\u5206\u6570\u548c\u6210\u529f\u7387\u5747\u9886\u5148\uff0c\u4e14\u4ec5\u9700\u5355\u76ee\u89c6\u89c9\u8f93\u5165\u3002", "conclusion": "GEMINUS\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.14204", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14204", "abs": "https://arxiv.org/abs/2507.14204", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.", "AI": {"tldr": "LaCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5f62\u72b6KV\u7f13\u5b58\u6a21\u5f0f\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff0c\u63d0\u5347LLMs\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\u548c\u8fde\u7eed\u751f\u6210\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0cLLMs\u4e2d\u7684KV\u5bf9\u6570\u91cf\u6fc0\u589e\uff0c\u5bfc\u81f4\u6548\u7387\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u540c\u65f6\u89e3\u51b3\u957f\u8ddd\u79bb\u5efa\u6a21\u548c\u8fde\u7eed\u751f\u6210\u7684\u5185\u5b58\u95ee\u9898\u3002", "method": "LaCache\u7ed3\u5408\u68af\u5f62\u72b6KV\u7f13\u5b58\u6a21\u5f0f\uff08\u8de8\u5c42\u5b58\u50a8KV\u5bf9\uff09\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff08\u52a8\u6001\u538b\u7f29\u65e7\u7f13\u5b58\uff09\uff0c\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1LaCache\u80fd\u6709\u6548\u589e\u5f3aLLMs\u7684\u957f\u8ddd\u79bb\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "LaCache\u4e3aLLMs\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u548c\u8fde\u7eed\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14519", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14519", "abs": "https://arxiv.org/abs/2507.14519", "authors": ["Wenxuan Zeng", "Tianshi Xu", "Yi Chen", "Yifan Zhou", "Mingzhe Zhang", "Jin Tan", "Cheng Hong", "Meng Li"], "title": "Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives", "comment": "This work will be continuously updated to reflect the latest advances", "summary": "Privacy-preserving machine learning (PPML) based on cryptographic protocols\nhas emerged as a promising paradigm to protect user data privacy in cloud-based\nmachine learning services. While it achieves formal privacy protection, PPML\noften incurs significant efficiency and scalability costs due to orders of\nmagnitude overhead compared to the plaintext counterpart. Therefore, there has\nbeen a considerable focus on mitigating the efficiency gap for PPML. In this\nsurvey, we provide a comprehensive and systematic review of recent PPML studies\nwith a focus on cross-level optimizations. Specifically, we categorize existing\npapers into protocol level, model level, and system level, and review progress\nat each level. We also provide qualitative and quantitative comparisons of\nexisting works with technical insights, based on which we discuss future\nresearch directions and highlight the necessity of integrating optimizations\nacross protocol, model, and system levels. We hope this survey can provide an\noverarching understanding of existing approaches and potentially inspire future\nbreakthroughs in the PPML field. As the field is evolving fast, we also provide\na public GitHub repository to continuously track the developments, which is\navailable at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\uff08PPML\uff09\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u8de8\u5c42\u7ea7\u4f18\u5316\uff0c\u5305\u62ec\u534f\u8bae\u3001\u6a21\u578b\u548c\u7cfb\u7edf\u5c42\u7ea7\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "PPML\u867d\u7136\u80fd\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\uff0c\u4f46\u6548\u7387\u4f4e\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u4ee5\u7f29\u5c0f\u4e0e\u660e\u6587\u673a\u5668\u5b66\u4e60\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u6bd4\u8f83\u73b0\u6709\u7814\u7a76\uff0c\u4ece\u534f\u8bae\u3001\u6a21\u578b\u548c\u7cfb\u7edf\u4e09\u4e2a\u5c42\u7ea7\u8fdb\u884c\u7efc\u8ff0\uff0c\u5e76\u63d0\u4f9b\u6280\u672f\u89c1\u89e3\u3002", "result": "\u603b\u7ed3\u4e86PPML\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u8de8\u5c42\u7ea7\u4f18\u5316\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684GitHub\u4ed3\u5e93\u4ee5\u8ddf\u8e2a\u6700\u65b0\u53d1\u5c55\u3002", "conclusion": "PPML\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u6574\u5408\u8de8\u5c42\u7ea7\u4f18\u5316\uff0c\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5e76\u5e0c\u671b\u6fc0\u53d1\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2507.14374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14374", "abs": "https://arxiv.org/abs/2507.14374", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9519\u8bef\u611f\u77e5\u7684\u5e08\u751f\u6846\u67b6\uff0c\u901a\u8fc7GPT-4o\u7684\u6307\u5bfc\u6539\u8fdb\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u5173\u7cfb\u5206\u7c7b\uff08RC\uff09\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u5173\u7cfb\u5206\u7c7b\u5bf9\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u548c\u836f\u7269\u518d\u5229\u7528\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9519\u8bef\u7387\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5e08\u751f\u6846\u67b6\uff0c\u6559\u5e08\u6a21\u578b\u5206\u6790\u5b66\u751f\u6a21\u578b\u7684\u9884\u6d4b\u9519\u8bef\uff0c\u751f\u6210\u9488\u5bf9\u6027\u4fee\u590d\uff08\u5982\u53e5\u5b50\u6539\u5199\u548cKG\u589e\u5f3a\uff09\uff0c\u5e76\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002", "result": "\u57285\u4e2aPPI\u6570\u636e\u96c6\u4e2d\u76844\u4e2a\u548cDDI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5728ChemProt\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9519\u8bef\u5206\u6790\u548c\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u5173\u7cfb\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2507.14459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "VisGuard\u662f\u4e00\u4e2a\u6297\u7be1\u6539\u7684\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u5143\u6570\u636e\u94fe\u63a5\u89e3\u51b3\u56fe\u50cf\u4f20\u64ad\u4e2d\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u53ef\u89c6\u5316\u56fe\u50cf\u4f20\u64ad\u4e2d\u5e38\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff08\u5982\u6e90\u4ee3\u7801\u3001\u4ea4\u4e92\u529f\u80fd\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u5e38\u89c1\u56fe\u50cf\u7be1\u6539\uff08\u5982\u88c1\u526a\u3001\u7f16\u8f91\uff09\u8106\u5f31\uff0c\u7f3a\u4e4f\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faVisGuard\u6846\u67b6\uff0c\u91c7\u7528\u91cd\u590d\u6570\u636e\u5e73\u94fa\u3001\u53ef\u9006\u4fe1\u606f\u5e7f\u64ad\u548c\u57fa\u4e8e\u951a\u70b9\u7684\u88c1\u526a\u5b9a\u4f4d\u6280\u672f\uff0c\u589e\u5f3a\u5d4c\u5165\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "result": "VisGuard\u5728\u6570\u636e\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u6297\u7be1\u6539/\u9690\u5199\u5206\u6790\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VisGuard\u80fd\u6709\u6548\u4fdd\u62a4\u548c\u4fc3\u8fdb\u53ef\u89c6\u5316\u4f20\u64ad\u4e0e\u4fe1\u606f\u4f20\u9012\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u56fe\u8868\u91cd\u5efa\u3001\u7be1\u6539\u68c0\u6d4b\u548c\u7248\u6743\u4fdd\u62a4\u7b49\u5e94\u7528\u3002"}}
{"id": "2507.14215", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14215", "abs": "https://arxiv.org/abs/2507.14215", "authors": ["Jiayu", "Liu"], "title": "Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired", "comment": null, "summary": "This study aims to develop a deep learning system for an accessibility device\nfor the deaf or hearing impaired. The device will accurately localize and\nidentify sound sources in real time. This study will fill an important gap in\ncurrent research by leveraging machine learning techniques to target the\nunderprivileged community. The system includes three main components. 1.\nJerryNet: A custom designed CNN architecture that determines the direction of\narrival (DoA) for nine possible directions. 2. Audio Classification: This model\nis based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model\nto identify the exact sound classes only based on audio. 3. Multimodal\nintegration model: This is an accurate sound localization model that combines\naudio, visual, and text data to locate the exact sound sources in the images.\nThe part consists of two modules, one object detection using Yolov9 to generate\nall the bounding boxes of the objects, and an audio visual localization model\nto identify the optimal bounding box using complete Intersection over Union\n(CIoU). The hardware consists of a four-microphone rectangular formation and a\ncamera mounted on glasses with a wristband for displaying necessary information\nlike direction. On a custom collected data set, JerryNet achieved a precision\nof 91. 1% for the sound direction, outperforming all the baseline models. The\nCLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,\nrespectively. The audio-visual localization model within component 3 yielded a\ncIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are\nmany future potentials to this study, paving the way to creating a new\ngeneration of accessibility devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u804b\u4eba\u6216\u542c\u529b\u969c\u788d\u8005\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u80fd\u591f\u5b9e\u65f6\u5b9a\u4f4d\u548c\u8bc6\u522b\u58f0\u6e90\uff0c\u586b\u8865\u4e86\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u9488\u5bf9\u5f31\u52bf\u7fa4\u4f53\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5f00\u53d1\u8f85\u52a9\u8bbe\u5907\uff0c\u89e3\u51b3\u804b\u4eba\u6216\u542c\u529b\u969c\u788d\u8005\u7684\u58f0\u6e90\u5b9a\u4f4d\u548c\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1aJerryNet\uff08\u7528\u4e8e\u58f0\u6e90\u65b9\u5411\u5b9a\u4f4d\u7684CNN\u67b6\u6784\uff09\u3001\u57fa\u4e8eCLAP\u6a21\u578b\u7684\u97f3\u9891\u5206\u7c7b\u3001\u4ee5\u53ca\u7ed3\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u591a\u6a21\u6001\u96c6\u6210\u6a21\u578b\u3002\u786c\u4ef6\u5305\u62ec\u56db\u9ea6\u514b\u98ce\u9635\u5217\u548c\u6444\u50cf\u5934\u773c\u955c\u3002", "result": "JerryNet\u5728\u58f0\u6e90\u65b9\u5411\u4e0a\u8fbe\u523091.1%\u7684\u7cbe\u5ea6\uff0cCLAP\u6a21\u578b\u5728\u81ea\u5b9a\u4e49\u548cAudioSet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.5%\u548c95%\u7684\u51c6\u786e\u7387\uff0c\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u7684cIoU\u4e3a0.892\uff0cAUC\u4e3a0.658\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65b0\u4e00\u4ee3\u8f85\u52a9\u8bbe\u5907\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.14588", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14588", "abs": "https://arxiv.org/abs/2507.14588", "authors": ["Usayd Shahul", "J. Harshan"], "title": "FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum", "comment": "To appear in the Proceedings of IEEE Information Theory Workshop\n  2025, Sydney, Australia", "summary": "Secure federated learning enables collaborative model training across\ndecentralized users while preserving data privacy. A key component is secure\naggregation, which keeps individual updates hidden from both the server and\nusers, while also defending against Byzantine users who corrupt the\naggregation. To this end, Jinhyun So et al. recently developed a\nByzantine-resilient secure aggregation scheme using a secret-sharing strategy\nover finite-field arithmetic. However, such an approach can suffer from\nnumerical errors and overflows when applied to real-valued model updates,\nmotivating the need for secure aggregation methods that operate directly over\nthe real domain. We propose FORTA, a Byzantine-resilient secure aggregation\nframework that operates entirely in the real domain. FORTA leverages Discrete\nFourier Transform (DFT) codes for privacy and employs Krum-based outlier\ndetection for robustness. While DFT decoder is error-free under infinite\nprecision, finite precision introduces numerical perturbations that can distort\ndistance estimates and allow malicious updates to evade detection. To address\nthis, FORTA refines Krum using feedback from DFT decoder, improving the\nselection of trustworthy updates. Theoretical analysis and experiments show\nthat our modification of Krum offers improved robustness and more accurate\naggregation than standard Krum.", "AI": {"tldr": "FORTA\u662f\u4e00\u4e2a\u5728\u5b9e\u6570\u57df\u64cd\u4f5c\u7684\u62dc\u5360\u5ead\u5f39\u6027\u5b89\u5168\u805a\u5408\u6846\u67b6\uff0c\u5229\u7528DFT\u7f16\u7801\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684Krum\u65b9\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6709\u9650\u57df\u7b97\u672f\u7684\u5b89\u5168\u805a\u5408\u65b9\u6cd5\u5728\u5904\u7406\u5b9e\u6570\u6a21\u578b\u66f4\u65b0\u65f6\u53ef\u80fd\u5f15\u5165\u6570\u503c\u9519\u8bef\u548c\u6ea2\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u76f4\u63a5\u5728\u5b9e\u6570\u57df\u64cd\u4f5c\u7684\u5b89\u5168\u805a\u5408\u65b9\u6cd5\u3002", "method": "FORTA\u7ed3\u5408DFT\u7f16\u7801\u548c\u53cd\u9988\u6539\u8fdb\u7684Krum\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u5b9e\u6570\u57df\u5b9e\u73b0\u5b89\u5168\u805a\u5408\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u7684Krum\u65b9\u6cd5\u6bd4\u6807\u51c6Krum\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u805a\u5408\u51c6\u786e\u6027\u3002", "conclusion": "FORTA\u5728\u5b9e\u6570\u57df\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5b89\u5168\u805a\u5408\uff0c\u89e3\u51b3\u4e86\u6570\u503c\u8bef\u5dee\u95ee\u9898\u3002"}}
{"id": "2507.14430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14430", "abs": "https://arxiv.org/abs/2507.14430", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "X-Intelligence 3.0\u662f\u4e00\u4e2a\u4e13\u4e3a\u534a\u5bfc\u4f53\u663e\u793a\u884c\u4e1a\u8bbe\u8ba1\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u80fd\u529b\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u534a\u5bfc\u4f53\u663e\u793a\u884c\u4e1a\u6548\u679c\u6709\u9650\uff0c\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u884c\u4e1a\u77e5\u8bc6\u5e93\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5165\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u548c\u9886\u57df\u7279\u5b9a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u673a\u5236\u3002", "result": "X-Intelligence 3.0\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8eSOTA\u6a21\u578bDeepSeek-R1-671B\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u3002", "conclusion": "X-Intelligence 3.0\u4e3a\u534a\u5bfc\u4f53\u663e\u793a\u884c\u4e1a\u7684\u590d\u6742\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "OptiCorNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5dee\u5206\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea71D\u5377\u79ef\u7f16\u7801\u5668\u548c\u53ef\u5b66\u4e60\u7684\u5dee\u5206\u65f6\u95f4\u7b97\u5b50\uff08DSD\uff09\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u52a8\u6001\u548c\u611f\u77e5\u6df7\u6dc6\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u662f\u957f\u671f\u5b9a\u4f4d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u5355\u5e27\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u5e8f\u5217\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "OptiCorNet\u7ed3\u54081D\u5377\u79ef\u7f16\u7801\u5668\u548cDSD\u6a21\u5757\uff0c\u901a\u8fc7\u56fa\u5b9a\u6743\u91cd\u5dee\u5206\u6838\u548cLSTM\u7ec6\u5316\u751f\u6210\u7d27\u51d1\u4e14\u9c81\u68d2\u7684\u63cf\u8ff0\u7b26\uff0c\u5e76\u5f15\u5165\u56db\u5143\u7ec4\u635f\u5931\u589e\u5f3a\u7c7b\u95f4\u5206\u79bb\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptiCorNet\u5728\u5b63\u8282\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OptiCorNet\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u5e8f\u5217\u7ea7\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5730\u70b9\u8bc6\u522b\u6548\u679c\u3002"}}
{"id": "2507.14217", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14217", "abs": "https://arxiv.org/abs/2507.14217", "authors": ["Tudor Matei Opran", "Samir Loudni"], "title": "Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation", "comment": null, "summary": "We address the pattern explosion problem in pattern mining by proposing an\ninteractive learning framework that combines nonlinear utility aggregation with\ngeometry-aware query selection. Our method models user preferences through a\nChoquet integral over multiple interestingness measures and exploits the\ngeometric structure of the version space to guide the selection of informative\ncomparisons. A branch-and-bound strategy with tight distance bounds enables\nefficient identification of queries near the decision boundary. Experiments on\nUCI datasets show that our approach outperforms existing methods such as\nChoquetRank, achieving better ranking accuracy with fewer user interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6548\u7528\u805a\u5408\u548c\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\uff0c\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u7528\u6237\u4ea4\u4e92\u63d0\u5347\u6a21\u5f0f\u6316\u6398\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528Choquet\u79ef\u5206\u5efa\u6a21\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\u548c\u5206\u652f\u5b9a\u754c\u7b56\u7565\uff0c\u9ad8\u6548\u8bc6\u522b\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u67e5\u8be2\u3002", "result": "\u5728UCI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982ChoquetRank\uff09\uff0c\u4ee5\u66f4\u5c11\u7684\u7528\u6237\u4ea4\u4e92\u5b9e\u73b0\u66f4\u9ad8\u7684\u6392\u540d\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u548c\u9ad8\u6548\u67e5\u8be2\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5f0f\u6316\u6398\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14600", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14600", "abs": "https://arxiv.org/abs/2507.14600", "authors": ["MA. Khajeian"], "title": "Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords", "comment": null, "summary": "Passwords that are long and human-generated pose a challenge for both\nclassical and quantum attacks due to their irregular structure and large search\nspace. In this work, we present an enhanced classical-quantum hybrid attack\ntailored to this scenario. We build rainbow tables using dictionary-based\npassword generation with transformation rules to better model real user\nbehavior. These tables are then organized into buckets, enabling faster lookup\nand reduced space complexity. To perform quantum search within each bucket, we\nuse a distributed exact variant of Grover's algorithm, which offers lower\ncircuit depth and deterministic success. As a result, the overall quantum\ncircuit is shallower and more robust against noise, particularly from\ndepolarizing channels commonly found in near-term quantum devices. Through this\nwork, Overall, we propose a hybrid framework that combines structured rainbow\ntables with efficient quantum search to enhance password recovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f69\u8679\u8868\u548c\u91cf\u5b50\u641c\u7d22\u7684\u6df7\u5408\u653b\u51fb\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7834\u89e3\u957f\u4e14\u4e0d\u89c4\u5219\u7684\u4eba\u7c7b\u751f\u6210\u5bc6\u7801\u3002", "motivation": "\u957f\u4e14\u4e0d\u89c4\u5219\u7684\u4eba\u7c7b\u751f\u6210\u5bc6\u7801\u5bf9\u7ecf\u5178\u548c\u91cf\u5b50\u653b\u51fb\u90fd\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7834\u89e3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5b57\u5178\u7684\u5bc6\u7801\u751f\u6210\u548c\u8f6c\u6362\u89c4\u5219\u6784\u5efa\u5f69\u8679\u8868\uff0c\u5e76\u901a\u8fc7\u5206\u6876\u4f18\u5316\u67e5\u627e\u6548\u7387\uff1b\u91c7\u7528\u5206\u5e03\u5f0f\u7cbe\u786eGrover\u7b97\u6cd5\u8fdb\u884c\u91cf\u5b50\u641c\u7d22\u3002", "result": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u964d\u4f4e\u4e86\u91cf\u5b50\u7535\u8def\u7684\u6df1\u5ea6\uff0c\u63d0\u9ad8\u4e86\u6297\u566a\u58f0\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u3002", "conclusion": "\u7ed3\u5408\u5f69\u8679\u8868\u548c\u9ad8\u6548\u91cf\u5b50\u641c\u7d22\u7684\u6df7\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u7801\u6062\u590d\u6548\u7387\u3002"}}
{"id": "2507.14578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14578", "abs": "https://arxiv.org/abs/2507.14578", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "XL-DURel\u662f\u4e00\u79cd\u4f18\u5316\u7684\u591a\u8bed\u8a00Sentence Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u5e8f\u6570Word-in-Context\u5206\u7c7b\uff0c\u901a\u8fc7\u57fa\u4e8e\u89d2\u5ea6\u8ddd\u79bb\u7684\u6392\u540d\u76ee\u6807\u5728\u590d\u6742\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u5148\u524d\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u5e8f\u6570\u4efb\u52a1\u6765\u63d0\u5347\u4e8c\u5143\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u4e3aWiC\u5efa\u6a21\u63d0\u4f9b\u7edf\u4e00\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u6d4b\u8bd5\u591a\u79cd\u56de\u5f52\u548c\u6392\u540d\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\uff0c\u91c7\u7528\u57fa\u4e8e\u590d\u6742\u7a7a\u95f4\u89d2\u5ea6\u8ddd\u79bb\u7684\u6392\u540d\u76ee\u6807\u3002", "result": "\u5728\u5e8f\u6570\u548c\u4e8c\u5143\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u6a21\u578b\uff0c\u4e14\u4f18\u5316\u5e8f\u6570\u4efb\u52a1\u63d0\u5347\u4e86\u4e8c\u5143\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "XL-DURel\u4e3a\u4e0d\u540c\u4efb\u52a1\u5f62\u5f0f\u7684WiC\u5efa\u6a21\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2507.14481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "DFQ-ViT\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u7684ViT\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u548c\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5408\u6210\u6837\u672c\u65f6\u672a\u80fd\u5145\u5206\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u4e14\u91cf\u5316\u6a21\u578b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "DFQ-ViT\u901a\u8fc7\u6309\u96be\u5ea6\u9012\u589e\u987a\u5e8f\u5408\u6210\u6837\u672c\uff0c\u5e76\u5f15\u5165\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\u6765\u5bf9\u9f50\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDFQ-ViT\u6027\u80fd\u4f18\u4e8e\u73b0\u6709DFQ\u65b9\u6cd5\uff0c\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u91cf\u5316\u6a21\u578b\uff0c\u59823\u4f4d\u91cf\u5316DeiT-T\u6027\u80fd\u63d0\u53474.29%\u3002", "conclusion": "DFQ-ViT\u65e0\u9700\u5fae\u8c03\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u90e8\u7f72\u95e8\u69db\uff0c\u7b26\u5408\u7eff\u8272\u5b66\u4e60\u539f\u5219\u3002"}}
{"id": "2507.14219", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14219", "abs": "https://arxiv.org/abs/2507.14219", "authors": ["Obumneme Zimuzor Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman", "comment": null, "summary": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u7eff\u8272\u6c22\u4ea7\u91cf\u548c\u9009\u5740\u9002\u5b9c\u6027\u6307\u6570\uff0c\u7ed3\u5408\u4e86\u65e0\u76d1\u7763\u805a\u7c7b\u3001\u76d1\u7763\u5b66\u4e60\u548cSHAP\u7b97\u6cd5\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u5730\u533a\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u51b3\u7b56\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u5404\u56fd\u5bfb\u6c42\u5316\u77f3\u71c3\u6599\u7684\u53ef\u6301\u7eed\u66ff\u4ee3\u54c1\uff0c\u7eff\u8272\u6c22\u6210\u4e3a\u8131\u78b3\u7684\u91cd\u8981\u9014\u5f84\uff0c\u4f46\u9009\u5740\u9700\u7efc\u5408\u8003\u8651\u590d\u6742\u56e0\u7d20\uff0c\u4e14\u7f3a\u4e4f\u76f4\u63a5\u4ea7\u91cf\u6570\u636e\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5AI\u6846\u67b6\uff0c\u5305\u62ec\u65e0\u76d1\u7763\u591a\u53d8\u91cf\u805a\u7c7b\u3001\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548cSHAP\u7b97\u6cd5\uff0c\u6574\u5408\u6c14\u8c61\u3001\u5730\u5f62\u548c\u65f6\u95f4\u6570\u636e\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe98%\uff0c\u663e\u793a\u6c34\u6e90\u8ddd\u79bb\u3001\u6d77\u62d4\u548c\u5b63\u8282\u53d8\u5316\u662f\u963f\u66fc\u7eff\u8272\u6c22\u9009\u5740\u7684\u6700\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u636e\u7a00\u7f3a\u5730\u533a\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u53ef\u590d\u5236\u7684\u9009\u5740\u5de5\u5177\uff0c\u66ff\u4ee3\u4e3b\u89c2\u4e13\u5bb6\u6743\u91cd\uff0c\u52a9\u529b\u7eff\u8272\u6c22\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u3002"}}
{"id": "2507.14625", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14625", "abs": "https://arxiv.org/abs/2507.14625", "authors": ["Juntao Tan", "Anran Li", "Quanchao Liu", "Peng Ran", "Lan Zhang"], "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning", "comment": null, "summary": "Vertical federated learning (VFL) enables multiple parties with disjoint\nfeatures to collaboratively train models without sharing raw data. While\nprivacy vulnerabilities of VFL are extensively-studied, its security\nthreats-particularly targeted label attacks-remain underexplored. In such\nattacks, a passive party perturbs inputs at inference to force\nmisclassification into adversary-chosen labels. Existing methods rely on\nunrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore\nanomaly detectors deployed in real-world systems. To bridge this gap, we\nintroduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly\ndesigned to evade detector-enhanced VFL inference. During the preparation\nstage, the attacker selects a minimal set of high-expressiveness samples (via\nmaximum mean discrepancy), submits them through VFL protocol to collect\npredicted labels, and uses these pseudo-labels to train estimated detector and\nsurrogate model on local features. In attack stage, these models guide\ngradient-based perturbations of remaining samples, crafting adversarial\ninstances that induce targeted misclassifications and evade detection. We\nimplement VTarbel and evaluate it against four model architectures, seven\nmultimodal datasets, and two anomaly detectors. Across all settings, VTarbel\noutperforms four state-of-the-art baselines, evades detection, and retains\neffective against three representative privacy-preserving defenses. These\nresults reveal critical security blind spots in current VFL deployments and\nunderscore urgent need for robust, attack-aware defenses.", "AI": {"tldr": "VTarbel\u662f\u4e00\u79cd\u9488\u5bf9\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u7684\u4e24\u9636\u6bb5\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u7ed5\u8fc7\u68c0\u6d4b\u5668\u5e76\u5b9e\u73b0\u76ee\u6807\u6807\u7b7e\u653b\u51fb\u3002", "motivation": "\u73b0\u6709VFL\u5b89\u5168\u7814\u7a76\u591a\u5173\u6ce8\u9690\u79c1\u6f0f\u6d1e\uff0c\u800c\u9488\u5bf9\u6807\u7b7e\u653b\u51fb\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u4e0d\u73b0\u5b9e\u3002", "method": "VTarbel\u901a\u8fc7\u4e24\u9636\u6bb5\u653b\u51fb\uff1a\u51c6\u5907\u9636\u6bb5\u9009\u62e9\u9ad8\u8868\u73b0\u6837\u672c\u8bad\u7ec3\u672c\u5730\u6a21\u578b\uff0c\u653b\u51fb\u9636\u6bb5\u751f\u6210\u5bf9\u6297\u6837\u672c\u4ee5\u7ed5\u8fc7\u68c0\u6d4b\u3002", "result": "VTarbel\u5728\u591a\u79cd\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u68c0\u6d4b\u5668\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u62b5\u5fa1\u9690\u79c1\u4fdd\u62a4\u9632\u5fa1\u3002", "conclusion": "VFL\u5f53\u524d\u90e8\u7f72\u5b58\u5728\u5b89\u5168\u76f2\u70b9\uff0c\u4e9f\u9700\u653b\u51fb\u611f\u77e5\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2507.14579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14579", "abs": "https://arxiv.org/abs/2507.14579", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528AudiBERT\u6a21\u578b\u5728\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u8bca\u65ad\u4e2d\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u793e\u4f1a\u8ba4\u77e5\u7ef4\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8eBERT\u6a21\u578b\uff0c\u4f46\u60c5\u611f\u7ef4\u5ea6\u672a\u89c2\u5bdf\u5230\u7c7b\u4f3c\u63d0\u5347\u3002\u6570\u636e\u91cf\u548c\u4eba\u7c7b\u8bc4\u5206\u4e00\u81f4\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u8bca\u65ad\u4e2d\u591a\u6a21\u6001\u6570\u636e\u5229\u7528\u4e0d\u8db3\u548c\u4eba\u7c7b\u4e0eAI\u4e92\u8865\u6027\u7f3a\u4e4f\u6307\u5bfc\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001BERT\u53d8\u4f53AudiBERT\uff0c\u6574\u5408\u8bed\u97f3\u548c\u58f0\u5b66-\u97f5\u5f8b\u7279\u5f81\uff0c\u5e76\u4e0eBERT\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "AudiBERT\u5728\u793e\u4f1a\u8ba4\u77e5\u7ef4\u5ea6\u5206\u7c7b\u4e0a\u663e\u8457\u4f18\u4e8eBERT\uff0c\u4f46\u60c5\u611f\u7ef4\u5ea6\u65e0\u663e\u8457\u5dee\u5f02\uff1b\u6570\u636e\u91cf\u548c\u4eba\u7c7b\u8bc4\u5206\u4e00\u81f4\u6027\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7ed3\u6784\u5316\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u4e92\u8865\u6027\uff0c\u5f3a\u8c03\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4ee5\u652f\u6301\u4eba\u7c7b\u5728\u53cd\u601d\u6027\u7f16\u7801\u4e2d\u7684\u53c2\u4e0e\u3002"}}
{"id": "2507.14485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u68c0\u7d22\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\uff0c\u7ed3\u5408\u53cc\u901a\u9053\u63a7\u5236\u95e8\u548c\u5206\u5c42\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u751f\u6210\u7cbe\u7ec6\u70b9\u4e91\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4e0d\u5b8c\u6574\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5178\u578b\u7ed3\u6784\u7279\u5f81\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u751f\u6210\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\uff08SSFE\uff09\u63d0\u53d6\u8de8\u6a21\u6001\u7279\u5f81\u5e76\u91cd\u6784\u53c2\u8003\u7279\u5f81\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7ed3\u5408\u53cc\u901a\u9053\u63a7\u5236\u95e8\u589e\u5f3a\u76f8\u5173\u7279\u5f81\uff1b\u63d0\u51fa\u6e10\u8fdb\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\uff08PRAG\uff09\u5206\u5c42\u878d\u5408\u53c2\u8003\u5148\u9a8c\u4e0e\u8f93\u5165\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u7cbe\u7ec6\u70b9\u4e91\uff0c\u5e76\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u5206\u5c42\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14227", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14227", "abs": "https://arxiv.org/abs/2507.14227", "authors": ["Khoi Do", "Duong Nguyen", "Nam-Khanh Le", "Quoc-Viet Pham", "Binh-Son Hua", "Won-Joo Hwang"], "title": "Domain Generalization via Pareto Optimal Gradient Matching", "comment": null, "summary": "In this study, we address the gradient-based domain generalization problem,\nwhere predictors aim for consistent gradient directions across different\ndomains. Existing methods have two main challenges. First, minimization of\ngradient empirical distance or gradient inner products (GIP) leads to gradient\nfluctuations among domains, thereby hindering straightforward learning. Second,\nthe direct application of gradient learning to the joint loss function can\nincur high computation overheads due to second-order derivative approximation.\nTo tackle these challenges, we propose a new Pareto Optimality Gradient\nMatching (POGM) method. In contrast to existing methods that add gradient\nmatching as regularization, we leverage gradient trajectories as collected data\nand apply independent training at the meta-learner. In the meta-update, we\nmaximize GIP while limiting the learned gradient from deviating too far from\nthe empirical risk minimization gradient trajectory. By doing so, the aggregate\ngradient can incorporate knowledge from all domains without suffering gradient\nfluctuation towards any particular domain. Experimental evaluations on datasets\nfrom DomainBed demonstrate competitive results yielded by POGM against other\nbaselines while achieving computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684POGM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u68af\u5ea6\u5185\u79ef\u5e76\u9650\u5236\u68af\u5ea6\u504f\u79bb\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u68af\u5ea6\u5339\u914d\u4e2d\u5b58\u5728\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u68af\u5ea6\u8f68\u8ff9\u4f5c\u4e3a\u6570\u636e\uff0c\u5728\u5143\u5b66\u4e60\u5668\u4e2d\u8fdb\u884c\u72ec\u7acb\u8bad\u7ec3\uff0c\u6700\u5927\u5316GIP\u5e76\u9650\u5236\u68af\u5ea6\u504f\u79bb\u3002", "result": "\u5728DomainBed\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "POGM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14629", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14629", "abs": "https://arxiv.org/abs/2507.14629", "authors": ["Juntao Tan", "Lan Zhang", "Zhonghao Hu", "Kai Yang", "Peng Ran", "Bo Li"], "title": "VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking", "comment": null, "summary": "Though vertical federated learning (VFL) is generally considered to be\nprivacy-preserving, recent studies have shown that VFL system is vulnerable to\nlabel inference attacks originating from various attack surfaces. Among these\nattacks, the model completion (MC) attack is currently the most powerful one.\nExisting defense methods against it either sacrifice model accuracy or incur\nimpractical computational overhead. In this paper, we propose VMask, a novel\nlabel privacy protection framework designed to defend against MC attack from\nthe perspective of layer masking. Our key insight is to disrupt the strong\ncorrelation between input data and intermediate outputs by applying the secret\nsharing (SS) technique to mask layer parameters in the attacker's model. We\ndevise a strategy for selecting critical layers to mask, reducing the overhead\nthat would arise from naively applying SS to the entire model. Moreover, VMask\nis the first framework to offer a tunable privacy budget to defenders, allowing\nfor flexible control over the levels of label privacy according to actual\nrequirements. We built a VFL system, implemented VMask on it, and extensively\nevaluated it using five model architectures and 13 datasets with different\nmodalities, comparing it to 12 other defense methods. The results demonstrate\nthat VMask achieves the best privacy-utility trade-off, successfully thwarting\nthe MC attack (reducing the label inference accuracy to a random guessing\nlevel) while preserving model performance (e.g., in Transformer-based model,\nthe averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up\nto 60,846 times faster than cryptography-based methods, and it only marginally\nexceeds that of standard VFL by 1.8 times in a large Transformer-based model,\nwhich is generally acceptable.", "AI": {"tldr": "VMask\u662f\u4e00\u79cd\u65b0\u578b\u6807\u7b7e\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u63a9\u7801\u6280\u672f\u9632\u5fa1\u6a21\u578b\u5b8c\u6210\u653b\u51fb\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4e0e\u6548\u7528\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u6613\u53d7\u6807\u7b7e\u63a8\u65ad\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u6a21\u578b\u51c6\u786e\u6027\uff0c\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u91c7\u7528\u79d8\u5bc6\u5171\u4eab\uff08SS\uff09\u6280\u672f\u63a9\u7801\u653b\u51fb\u8005\u6a21\u578b\u7684\u5173\u952e\u5c42\u53c2\u6570\uff0c\u7834\u574f\u8f93\u5165\u6570\u636e\u4e0e\u4e2d\u95f4\u8f93\u51fa\u7684\u5f3a\u76f8\u5173\u6027\u3002", "result": "VMask\u6210\u529f\u5c06\u6807\u7b7e\u63a8\u65ad\u51c6\u786e\u7387\u964d\u81f3\u968f\u673a\u731c\u6d4b\u6c34\u5e73\uff0c\u6a21\u578b\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff08\u5982Transformer\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.09%\uff09\uff0c\u8fd0\u884c\u901f\u5ea6\u8fdc\u8d85\u52a0\u5bc6\u65b9\u6cd5\u3002", "conclusion": "VMask\u662f\u9996\u4e2a\u63d0\u4f9b\u53ef\u8c03\u9690\u79c1\u9884\u7b97\u7684\u6846\u67b6\uff0c\u5728\u9ad8\u6548\u9632\u5fa1\u653b\u51fb\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002"}}
{"id": "2507.14584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14584", "abs": "https://arxiv.org/abs/2507.14584", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528SHAP\u5206\u6790BERT\u6a21\u578b\u5728CPS\u5206\u7c7b\u4e2d\u8bcd\u6807\u8bb0\u7684\u8d21\u732e\uff0c\u53d1\u73b0\u9ad8\u5206\u7c7b\u6027\u80fd\u4e0d\u4e00\u5b9a\u6709\u5408\u7406\u89e3\u91ca\uff0c\u5e76\u8bc6\u522b\u4e86\u65e0\u610f\u4e49\u7684\u8bcd\u6807\u8bb0\u5f71\u54cd\u5206\u7c7b\u3002", "motivation": "\u589e\u5f3aBERT\u6a21\u578b\u5728CPS\u8bca\u65ad\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u63d0\u5347\u6559\u5e08\u7b49\u7ec8\u7aef\u7528\u6237\u7684\u4fe1\u4efb\u548c\u91c7\u7528\u7387\u3002", "method": "\u4f7f\u7528SHAP\u5206\u6790BERT\u6a21\u578b\u5bf9CPS\u8f6c\u5f55\u6570\u636e\u4e2d\u8bcd\u6807\u8bb0\u7684\u5206\u7c7b\u8d21\u732e\u3002", "result": "\u53d1\u73b0\u5206\u7c7b\u6027\u80fd\u9ad8\u4f46\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u90e8\u5206\u8bcd\u6807\u8bb0\u9891\u7e41\u5f71\u54cd\u5206\u7c7b\uff0c\u751a\u81f3\u5305\u62ec\u65e0\u610f\u4e49\u7684\u8bcd\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u96c6\u6210\u6a21\u578b\u67b6\u6784\u548c\u4eba\u673a\u4e92\u8865\uff0c\u4ee5\u63d0\u5347CPS\u8bca\u65ad\u7684\u7ec6\u7c92\u5ea6\u533a\u5206\u80fd\u529b\u3002"}}
{"id": "2507.14497", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "TCP-LLaVA\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u6280\u672f\u89e3\u51b3\u75c5\u7406\u5b66\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u7684\u9ad8\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u75c5\u7406\u5b66\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u7684\u9ad8\u5206\u8fa8\u7387\uff08\u598210,000 x 10,000\u50cf\u7d20\uff09\u5bfc\u81f4\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5904\u7406\u65f6\u9762\u4e34\u957f\u4e0a\u4e0b\u6587\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\u6216\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u3002", "method": "\u63d0\u51faTCP-LLaVA\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u538b\u7f29\u4ee4\u724c\u548c\u6a21\u6001\u538b\u7f29\u6a21\u5757\u805a\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\uff0c\u4ec5\u5c06\u538b\u7f29\u540e\u7684\u4ee4\u724c\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u3002", "result": "\u572810\u79cdTCGA\u80bf\u7624\u4e9a\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0cTCP-LLaVA\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709MLLM\u57fa\u7ebf\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "TCP-LLaVA\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u6709\u6548\u89e3\u51b3\u4e86WSI VQA\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.14245", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE", "q-bio.BM", "I.6.5; J.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.14245", "abs": "https://arxiv.org/abs/2507.14245", "authors": ["Hengjie Yu", "Kenneth A. Dawson", "Haiyun Yang", "Shuya Liu", "Yan Yan", "Yaochu Jin"], "title": "A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions", "comment": "31 pages, 6 figures", "summary": "Unlocking the potential of nanomaterials in medicine and environmental\nscience hinges on understanding their interactions with proteins, a complex\ndecision space where AI is poised to make a transformative impact. However,\nprogress has been hindered by limited datasets and the restricted\ngeneralizability of existing models. Here, we propose NanoPro-3M, the largest\nnanomaterial-protein interaction dataset to date, comprising over 3.2 million\nsamples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,\na foundational model that predicts nanomaterial-protein affinities through\nmultimodal representation learning, demonstrating strong generalization,\nhandling missing features, and unseen nanomaterials or proteins. We show that\nmultimodal modeling significantly outperforms single-modality approaches and\nidentifies key determinants of corona formation. Furthermore, we demonstrate\nits applicability to a range of downstream tasks through zero-shot inference\nand fine-tuning. Together, this work establishes a solid foundation for\nhigh-performance and generalized prediction of nanomaterial-protein interaction\nendpoints, reducing experimental reliance and accelerating various in vitro\napplications.", "AI": {"tldr": "NanoPro-3M\u662f\u6700\u5927\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u96c6\uff0c\u7ed3\u5408NanoProFormer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u9002\u7528\u6027\u3002", "motivation": "\u7eb3\u7c73\u6750\u6599\u5728\u533b\u5b66\u548c\u73af\u5883\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u53d7\u9650\u4e8e\u5bf9\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u800c\u73b0\u6709\u6a21\u578b\u56e0\u6570\u636e\u96c6\u6709\u9650\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u800c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u63d0\u51faNanoPro-3M\u6570\u636e\u96c6\uff08320\u4e07\u6837\u672c\uff0c3.7\u4e07\u72ec\u7279\u86cb\u767d\u8d28\uff09\uff0c\u5e76\u5f00\u53d1NanoProFormer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u9884\u6d4b\u4eb2\u548c\u529b\u3002", "result": "\u591a\u6a21\u6001\u5efa\u6a21\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u80fd\u5904\u7406\u7f3a\u5931\u7279\u5f81\u548c\u672a\u77e5\u6837\u672c\uff0c\u5e76\u8bc6\u522b\u51fa\u51a0\u72b6\u5f62\u6210\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u6027\u80fd\u3001\u6cdb\u5316\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u51cf\u5c11\u5b9e\u9a8c\u4f9d\u8d56\u5e76\u52a0\u901f\u4f53\u5916\u5e94\u7528\u3002"}}
{"id": "2507.14739", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14739", "abs": "https://arxiv.org/abs/2507.14739", "authors": ["Franco Oberti", "Stefano Di Carlo", "Alessandro Savino"], "title": "CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus", "comment": "Accepted for publication at the 31st IEEE International Symposium on\n  On-Line Testing and Robust System Design 2025 (IOLTS25)", "summary": "The Controller Area Network (CAN) protocol, essential for automotive embedded\nsystems, lacks inherent security features, making it vulnerable to cyber\nthreats, especially with the rise of autonomous vehicles. Traditional security\nmeasures offer limited protection, such as payload encryption and message\nauthentication. This paper presents a novel Intrusion Detection System (IDS)\ndesigned for the CAN environment, utilizing Hardware Performance Counters\n(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN\nreceiver is simulated using the gem5 simulator, processing CAN frame payloads\nwith AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC\nresponses. Key HPC features are optimized through data extraction and\ncorrelation analysis to enhance classification efficiency. Results indicate\nthat this approach could significantly improve CAN security and address\nemerging challenges in automotive cybersecurity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\uff08HPCs\uff09\u7684\u65b0\u578b\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\uff0c\u7528\u4e8e\u63d0\u5347CAN\u534f\u8bae\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u6a21\u62dfRISC-V CAN\u63a5\u6536\u5668\u5e76\u4f18\u5316HPC\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u7387\u3002", "motivation": "CAN\u534f\u8bae\u7f3a\u4e4f\u5185\u7f6e\u5b89\u5168\u529f\u80fd\uff0c\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u666e\u53ca\uff0c\u5176\u9762\u4e34\u7684\u5b89\u5168\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u6548\u679c\u6709\u9650\u3002", "method": "\u4f7f\u7528gem5\u6a21\u62df\u5668\u6a21\u62dfRISC-V CAN\u63a5\u6536\u5668\uff0c\u901a\u8fc7AES-128\u52a0\u5bc6\u5904\u7406CAN\u5e27\u8f7d\u8377\uff0c\u5e76\u5229\u7528HPCs\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\u3002\u901a\u8fc7\u6570\u636e\u63d0\u53d6\u548c\u76f8\u5173\u6027\u5206\u6790\u4f18\u5316HPC\u7279\u5f81\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86CAN\u534f\u8bae\u7684\u5b89\u5168\u6027\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u6c7d\u8f66\u7f51\u7edc\u5b89\u5168\u7684\u65b0\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8eHPCs\u7684IDS\u4e3aCAN\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14590", "abs": "https://arxiv.org/abs/2507.14590", "authors": ["\u0141ukasz Radli\u0144ski", "Mateusz Gu\u015bciora", "Jan Koco\u0144"], "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT\uff09\u8fdb\u884cNLP\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u590d\u8ff0\u548c\u56de\u8bd1\uff09\u4e0e\u7eaf\u751f\u6210\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u9009\u62e9\u4e86\u57fa\u4e8eChatGPT\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u4e0d\u540c\u65b9\u6cd5\u5728\u591a\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u56de\u8bd1\u548c\u590d\u8ff0\u65b9\u6cd5\u5728\u751f\u6210\u6570\u636e\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u5c11\u6837\u672c\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7ed3\u5408\u65b0\u6a21\u578b\u53ef\u4ee5\u53d6\u5f97\u4e0e\u7eaf\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6d41\u6570\u636e\u7684\u8fd0\u52a8\u5206\u5272\u4e0e\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u65e0\u9700\u5b8c\u6574\u5149\u6d41\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5149\u6d41\u6216\u6df1\u5ea6\u4f30\u8ba1\uff0c\u800c\u4e8b\u4ef6\u6570\u636e\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4e0e\u7a00\u758f\u6027\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u6d41\u7a0b\u8fdb\u884c\u4e8b\u4ef6\u8fc7\u5206\u5272\uff0c\u5229\u7528\u6b8b\u5dee\u5206\u6790\u5206\u79bb\u8fd0\u52a8\u7269\u4f53\uff0c\u5e76\u7ed3\u5408\u8fd0\u52a8\u76f8\u4f3c\u6027\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u8fdb\u884c\u5c42\u6b21\u805a\u7c7b\u3002", "result": "\u5728EVIMO2v2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u51c6\u786e\u7684\u5206\u5272\u4e0e\u5e73\u79fb\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5c24\u5176\u5728\u7269\u4f53\u8fb9\u754c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u53ca\u5bfc\u822a\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2507.14257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14257", "abs": "https://arxiv.org/abs/2507.14257", "authors": ["Julio Candanedo"], "title": "Linearized Diffusion Map", "comment": null, "summary": "We introduce the Linearized Diffusion Map (LDM), a novel linear\ndimensionality reduction method constructed via a linear approximation of the\ndiffusion-map kernel. LDM integrates the geometric intuition of diffusion-based\nnonlinear methods with the computational simplicity, efficiency, and\ninterpretability inherent in linear embeddings such as PCA and classical MDS.\nThrough comprehensive experiments on synthetic datasets (Swiss roll and\nhyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that\nLDM captures distinct geometric features of datasets compared to PCA, offering\ncomplementary advantages. Specifically, LDM embeddings outperform PCA in\ndatasets exhibiting explicit manifold structures, particularly in\nhigh-dimensional regimes, whereas PCA remains preferable in scenarios dominated\nby variance or noise. Furthermore, the complete positivity of LDM's kernel\nmatrix allows direct applicability of Non-negative Matrix Factorization (NMF),\nsuggesting opportunities for interpretable latent-structure discovery. Our\nanalysis positions LDM as a valuable new linear dimensionality reduction\ntechnique with promising theoretical and practical extensions.", "AI": {"tldr": "LDM\u662f\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u8fd1\u4f3c\u6269\u6563\u6620\u5c04\u6838\uff0c\u7ed3\u5408\u51e0\u4f55\u76f4\u89c9\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u7ed3\u5408\u975e\u7ebf\u6027\u6269\u6563\u65b9\u6cd5\u7684\u51e0\u4f55\u76f4\u89c9\u4e0e\u7ebf\u6027\u65b9\u6cd5\uff08\u5982PCA\uff09\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u8fd1\u4f3c\u6269\u6563\u6620\u5c04\u6838\u6784\u5efaLDM\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "LDM\u5728\u663e\u5f0f\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u4e2d\u4f18\u4e8ePCA\uff0c\u800cPCA\u5728\u566a\u58f0\u6216\u65b9\u5dee\u4e3b\u5bfc\u7684\u573a\u666f\u4e2d\u66f4\u4f18\u3002LDM\u7684\u6838\u77e9\u9635\u5b8c\u5168\u6b63\u5b9a\uff0c\u9002\u7528\u4e8eNMF\u3002", "conclusion": "LDM\u662f\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u7ebf\u6027\u964d\u7ef4\u6280\u672f\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2507.14796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14796", "abs": "https://arxiv.org/abs/2507.14796", "authors": ["Ceren Kocao\u011fullar", "Gustavo Petri", "Dominic P. Mulligan", "Derek Miller", "Hugo J. M. Vincent", "Shale Xiong", "Alastair R. Beresford"], "title": "Careful Whisper: Attestation for peer-to-peer Confidential Computing networks", "comment": null, "summary": "Trusted Execution Environments (TEEs) are designed to protect the privacy and\nintegrity of data in use. They enable secure data processing and sharing in\npeer-to-peer networks, such as vehicular ad hoc networks of autonomous\nvehicles, without compromising confidentiality. In these networks, nodes must\nestablish mutual trust to collaborate securely. TEEs can achieve this through\nremote attestation, where a prover presents evidence of its trustworthiness to\na verifier, which then decides whether or not to trust the prover. However, a\nnaive peer-to-peer attestation approach, where every TEE directly attests every\nother TEE, results in quadratic communication overhead. This is inefficient in\ndynamic environments, where nodes frequently join and leave the network.\n  To address this, we present Careful Whisper, a gossip-based protocol that\ndisseminates trust efficiently, reducing attestation overhead to linear\ncomplexity under ideal conditions. It enables interoperability by enabling\ntransitive trust across heterogeneous networks, and supports trust\nestablishment with offline nodes via relayed attestations. Using a custom\ndiscrete-event simulator, we show that Careful Whisper propagates trust both\nfaster and more widely than naive approaches across various network topologies.\nOur results demonstrate that our protocol is resource efficient, sending ~21.5\nKiB and requiring 0.158 seconds per round in a 200-node network, and that our\nprotocol is resilient to attestation failures across various network\ntopologies.", "AI": {"tldr": "Careful Whisper\u534f\u8bae\u901a\u8fc7\u57fa\u4e8egossip\u7684\u65b9\u6cd5\u9ad8\u6548\u4f20\u64ad\u4fe1\u4efb\uff0c\u51cf\u5c11\u9a8c\u8bc1\u5f00\u9500\u81f3\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u7f51\u7edc\u73af\u5883\u3002", "motivation": "\u89e3\u51b3TEE\u5728\u70b9\u5bf9\u70b9\u7f51\u7edc\u4e2d\u76f4\u63a5\u9a8c\u8bc1\u5bfc\u81f4\u7684\u4e8c\u6b21\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u9002\u5e94\u8282\u70b9\u9891\u7e41\u53d8\u5316\u7684\u52a8\u6001\u73af\u5883\u3002", "method": "\u63d0\u51faCareful Whisper\u534f\u8bae\uff0c\u5229\u7528gossip\u673a\u5236\u4f20\u64ad\u4fe1\u4efb\uff0c\u652f\u6301\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u4f20\u9012\u4fe1\u4efb\u548c\u79bb\u7ebf\u8282\u70b9\u9a8c\u8bc1\u3002", "result": "\u534f\u8bae\u5728200\u8282\u70b9\u7f51\u7edc\u4e2d\u6bcf\u8f6e\u4ec5\u53d1\u900121.5 KiB\u6570\u636e\uff0c\u8017\u65f60.158\u79d2\uff0c\u4e14\u5bf9\u9a8c\u8bc1\u5931\u8d25\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "Careful Whisper\u534f\u8bae\u9ad8\u6548\u3001\u8d44\u6e90\u8282\u7ea6\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u4fe1\u4efb\u4f20\u64ad\u3002"}}
{"id": "2507.14615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14615", "abs": "https://arxiv.org/abs/2507.14615", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9\u80af\u5c3c\u4e9a\u521d\u7ea7\u533b\u7597\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u975e\u6d32\u533b\u7597\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u975e\u6d32\u4f4e\u8d44\u6e90\u533b\u7597\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u5176\u5728\u672c\u5730\u5316\u5e94\u7528\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528RAG\u6280\u672f\u5c06\u4e34\u5e8a\u95ee\u9898\u4e0e\u80af\u5c3c\u4e9a\u56fd\u5bb6\u6307\u5357\u5bf9\u9f50\uff0c\u751f\u6210\u591a\u8bed\u8a00\uff08\u82f1\u8bed\u548c\u65af\u74e6\u5e0c\u91cc\u8bed\uff09\u7684\u4e34\u5e8a\u573a\u666f\u3001\u9009\u62e9\u9898\u53ca\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u5ba1\u6838\u786e\u4fdd\u51c6\u786e\u6027\u548c\u6587\u5316\u9002\u5e94\u6027\u3002", "result": "\u53d1\u73b0LLMs\u5728\u672c\u5730\u5316\u533b\u7597\u573a\u666f\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u7f8e\u56fd\u57fa\u51c6\uff0c\u7a81\u51fa\u4e86\u672c\u5730\u5316\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u6d32\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u7684\u6307\u5357\u9a71\u52a8\u52a8\u6001\u8bc4\u4f30\u6a21\u578b\uff0c\u652f\u6301AI\u7684\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2507.14501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u57fa\u4e8e\u524d\u9988\u65b9\u6cd5\u76843D\u91cd\u5efa\u4e0e\u89c6\u56fe\u5408\u6210\u6280\u672f\uff0c\u5206\u7c7b\u8ba8\u8bba\u4e86\u4e0d\u540c\u8868\u793a\u67b6\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u524d\u9988\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5feb\u901f\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e863D\u89c6\u89c9\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8ba8\u8bba\u70b9\u4e91\u30013D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u3001\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7b49\u8868\u793a\u67b6\u6784\uff0c\u5206\u6790\u59ff\u6001\u65e0\u5173\u91cd\u5efa\u3001\u52a8\u60013D\u91cd\u5efa\u7b49\u5173\u952e\u4efb\u52a1\u3002", "result": "\u603b\u7ed3\u4e86\u524d\u9988\u65b9\u6cd5\u7684\u4f18\u52bf\u53ca\u5176\u5728\u6570\u5b57\u4eba\u3001SLAM\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u8be6\u7ec6\u7edf\u8ba1\u3002", "conclusion": "\u524d\u9988\u65b9\u6cd5\u57283D\u89c6\u89c9\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5f00\u653e\u6027\u7814\u7a76\u6311\u6218\uff0c\u672a\u6765\u65b9\u5411\u503c\u5f97\u5173\u6ce8\u3002"}}
{"id": "2507.14295", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14295", "abs": "https://arxiv.org/abs/2507.14295", "authors": ["Licheng Liu", "Zihan Wang", "Linjie Li", "Chenwei Xu", "Yiping Lu", "Han Liu", "Avirup Sil", "Manling Li"], "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning", "comment": null, "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUFO\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7684\u7528\u6237\u53cd\u9988\u63d0\u5347\u5927\u6a21\u578b\u7684\u5355\u8f6e\u548c\u591a\u8f6e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5355\u8f6e\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8f6e\u63a8\u7406\u548c\u53cd\u9988\u4fee\u6b63\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4\u91cd\u590d\u56de\u7b54\u3002", "method": "\u5f15\u5165Unary Feedback as Observation (UFO)\uff0c\u5229\u7528\u7b80\u5355\u7684\u7528\u6237\u53cd\u9988\uff08\u5982\u201c\u518d\u8bd5\u4e00\u6b21\u201d\uff09\u8fdb\u884c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cUFO\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u8f6e\u6027\u80fd\u7684\u540c\u65f6\uff0c\u591a\u8f6e\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe14%\u3002", "conclusion": "UFO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u7684\u53cd\u9988\u54cd\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u5956\u52b1\u673a\u5236\u4ee5\u51cf\u5c11\u4fee\u6b63\u8f6e\u6b21\u5e76\u9f13\u52b1\u591a\u6837\u6027\u63a8\u7406\u3002"}}
{"id": "2507.14799", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14799", "abs": "https://arxiv.org/abs/2507.14799", "authors": ["Sam Johnson", "Viet Pham", "Thai Le"], "title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree", "comment": "EMNLP 2025 System Demonstrations Submission", "summary": "This work demonstrates that LLM-based web navigation agents offer powerful\nautomation capabilities but are vulnerable to Indirect Prompt Injection (IPI)\nattacks. We show that adversaries can embed universal adversarial triggers in\nwebpage HTML to hijack agent behavior that utilizes the accessibility tree to\nparse HTML, causing unintended or malicious actions. Using the Greedy\nCoordinate Gradient (GCG) algorithm and a Browser Gym agent powered by\nLlama-3.1, our system demonstrates high success rates across real websites in\nboth targeted and general attacks, including login credential exfiltration and\nforced ad clicks. Our empirical results highlight critical security risks and\nthe need for stronger defenses as LLM-driven autonomous web agents become more\nwidely adopted. The system software\n(https://github.com/sej2020/manipulating-web-agents) is released under the MIT\nLicense, with an accompanying publicly available demo website\n(http://lethaiq.github.io/attack-web-llm-agent).", "AI": {"tldr": "LLM-based web navigation agents are vulnerable to Indirect Prompt Injection (IPI) attacks, where adversaries can hijack agent behavior via HTML triggers, leading to security risks like credential theft.", "motivation": "To expose vulnerabilities in LLM-based web navigation agents, particularly their susceptibility to IPI attacks, and highlight the need for stronger defenses.", "method": "Utilized the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1 to demonstrate IPI attacks on real websites.", "result": "High success rates in targeted and general attacks, including credential exfiltration and forced ad clicks.", "conclusion": "LLM-driven web agents pose significant security risks, necessitating improved defenses as their adoption grows."}}
{"id": "2507.14640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14640", "abs": "https://arxiv.org/abs/2507.14640", "authors": ["Eric Xia", "Jugal Kalita"], "title": "Linear Relational Decoding of Morphology in Language Models", "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\uff0c\u901a\u8fc7\u4e24\u90e8\u5206\u7684\u4eff\u5c04\u8fd1\u4f3c\u53ef\u4ee5\u5f88\u597d\u5730\u8fd1\u4f3c\u67d0\u4e9b\u4e3b\u5ba2\u4f53\u5173\u7cfb\u7684Transformer\u8ba1\u7b97\uff0c\u7ebf\u6027\u53d8\u6362Ws\u80fd\u51c6\u786e\u91cd\u73b0\u6700\u7ec8\u5ba2\u4f53\u72b6\u6001\uff0c\u5c24\u5176\u5728\u5f62\u6001\u5b66\u5173\u7cfb\u4e0a\u8fbe\u523090%\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u63a2\u7d22Transformer\u6a21\u578b\u4e2d\u4e3b\u5ba2\u4f53\u5173\u7cfb\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9a8c\u8bc1\u7ebf\u6027\u53d8\u6362\u662f\u5426\u80fd\u6709\u6548\u6355\u6349\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u5173\u7cfb\u3002", "method": "\u4f7f\u7528Bigger Analogy Test Set\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362Ws\uff08s\u4e3a\u4e3b\u8bcd\u7684\u4e2d\u5c42\u8868\u793a\uff0cW\u4e3a\u6a21\u578b\u5bfc\u6570\uff09\u8fd1\u4f3c\u6700\u7ec8\u5ba2\u4f53\u72b6\u6001\u3002", "result": "\u7ebf\u6027\u6280\u672f\u5728\u5f62\u6001\u5b66\u5173\u7cfb\u4e0a\u8fbe\u523090%\u7684\u5fe0\u5b9e\u5ea6\uff0c\u591a\u8bed\u8a00\u548c\u591a\u6a21\u578b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7c7b\u4f3c\u7ed3\u679c\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u67d0\u4e9b\u6982\u5ff5\u5173\u7cfb\uff08\u5982\u5f62\u6001\u5b66\uff09\u53ef\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7684\u7ebf\u6027\u53d8\u6362\u7a00\u758f\u7f16\u7801\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.14505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCHM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u4e00\u81f4\u7684\u4eba\u4f53\u5efa\u6a21\u548c\u591a\u89c6\u56fe\u878d\u5408\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86\u884c\u4eba\u68c0\u6d4b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u884c\u4eba\u68c0\u6d4b\u4e2d\u5e38\u5f15\u5165\u566a\u58f0\u4e14\u7cbe\u5ea6\u4f4e\uff0c\u4e14\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u573a\u666f\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u4e00\u81f4\u7684\u4eba\u4f53\u5efa\u6a21\uff08DCHM\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u8d85\u50cf\u7d20\u7ea7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u591a\u89c6\u56fe\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u7cbe\u786e\u70b9\u4e91\u751f\u6210\u3002", "result": "DCHM\u663e\u8457\u51cf\u5c11\u4e86\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u7a00\u758f\u89c6\u56fe\u3001\u5927\u89c4\u6a21\u548c\u62e5\u6324\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DCHM\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u591a\u89c6\u56fe\u884c\u4eba\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u548c\u5206\u5272\uff0c\u662f\u9996\u4e2a\u5728\u6b64\u7c7b\u6311\u6218\u6027\u573a\u666f\u4e2d\u91cd\u5efa\u884c\u4eba\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.14322", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11; C.2.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14322", "abs": "https://arxiv.org/abs/2507.14322", "authors": ["Md Rafid Haque", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning", "comment": "24 pages, 8 figures. This work is intended for a journal submission", "summary": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems.", "AI": {"tldr": "FedStrategist\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f18\u805a\u5408\u89c4\u5219\u6765\u9632\u5fa1\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u4f7f\u5176\u6613\u53d7\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u9759\u6001\u9632\u5fa1\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u6297\u6027\u73af\u5883\u6216\u5f02\u6784\u6570\u636e\u4e2d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u4ee3\u7406\uff0c\u5b9e\u65f6\u9009\u62e9\u6700\u4f18\u805a\u5408\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedStrategist\u80fd\u9002\u5e94\u591a\u6837\u573a\u666f\uff0c\u5305\u62ec\u5bf9\u6297\u6027\u653b\u51fb\u548c\u5f02\u6784\u6570\u636e\u73af\u5883\uff0c\u5e76\u5728\u6a21\u578b\u5b8c\u6574\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "FedStrategist\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u5206\u6790\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u667a\u80fd\u6027\u3002"}}
{"id": "2507.14822", "categories": ["cs.CR", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14822", "abs": "https://arxiv.org/abs/2507.14822", "authors": ["Zeeshan Kaleem", "Misha Urooj Khan", "Ahmad Suleman", "Waqas Khalid", "Kai-Kit Wong", "Chau Yuen"], "title": "Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies", "comment": null, "summary": "Recently, low-altitude wireless networks (LAWNs) have emerged as a critical\nbackbone for supporting the low-altitude economy, particularly with the\ndensification of unmanned aerial vehicles (UAVs) and high-altitude platforms\n(HAPs). To meet growing data demands, some LAWN deployments incorporate\nfree-space optical (FSO) links, which offer exceptional bandwidth and beam\ndirectivity. However, without strong security measures in place, both\nconventional radio frequency channels and FSO beams remain vulnerable to\ninterception and spoofing and FSO in particular can suffer from turbulence,\nmisalignment, and weather-related attenuation. To address these challenges in\nthe quantum era, a quantum-secure architecture called Quantum Skyshield is\nproposed to enable reliable communication between the base transceiver station\n(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution\n(QKD) with post-quantum authentication mechanisms. Simulation results confirm\nthe reliable generation of a 128-bit symmetric key when the quantum bit error\nrate (QBER) remains below the threshold of 11%. Authentication is enforced\nusing Lamport one-time signatures and hash-based message authentication codes\n(HMAC) to ensure message integrity. A Grover-inspired threat detection\nmechanism identifies anomalies with up to 89% probability in a single\niteration, enabling real-time trust evaluation. Lastly, future research\nchallenges have also been identified and discussed to guide further development\nin this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuantum Skyshield\u7684\u91cf\u5b50\u5b89\u5168\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\u4e2d\u7684\u5b89\u5168\u6311\u6218\uff0c\u7ed3\u5408\u4e86BB84\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u548c\u540e\u91cf\u5b50\u8ba4\u8bc1\u673a\u5236\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u548c\u9ad8\u7a7a\u5e73\u53f0\u7684\u5bc6\u96c6\u90e8\u7f72\uff0c\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u7684\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u81ea\u7531\u7a7a\u95f4\u5149\u901a\u4fe1\uff08FSO\uff09\u6613\u53d7\u62e6\u622a\u3001\u6e4d\u6d41\u548c\u5929\u6c14\u5f71\u54cd\u3002", "method": "\u91c7\u7528BB84\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u548c\u540e\u91cf\u5b50\u8ba4\u8bc1\u673a\u5236\uff08\u5982Lamport\u4e00\u6b21\u6027\u7b7e\u540d\u548cHMAC\uff09\uff0c\u5e76\u5f15\u5165Grover\u542f\u53d1\u7684\u5a01\u80c1\u68c0\u6d4b\u673a\u5236\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u7387\u4f4e\u4e8e11%\u65f6\u80fd\u53ef\u9760\u751f\u6210128\u4f4d\u5bf9\u79f0\u5bc6\u94a5\uff0c\u5a01\u80c1\u68c0\u6d4b\u673a\u5236\u5355\u6b21\u8fed\u4ee3\u8bc6\u522b\u5f02\u5e38\u6982\u7387\u8fbe89%\u3002", "conclusion": "Quantum Skyshield\u4e3aLAWN\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u8be5\u67b6\u6784\u3002"}}
{"id": "2507.14649", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14649", "abs": "https://arxiv.org/abs/2507.14649", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCleanse\u7684\u805a\u7c7b\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u68c0\u6d4b\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLM\u5728\u751f\u6210\u5185\u5bb9\u65f6\u53ef\u80fd\u51fa\u73b0\u5e7b\u89c9\uff08\u4e0d\u51c6\u786e\u56de\u7b54\uff09\uff0c\u5f71\u54cd\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "Cleanse\u901a\u8fc7\u805a\u7c7bLLM\u9690\u85cf\u5d4c\u5165\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u8ba1\u7b97\u7c07\u5185\u4e00\u81f4\u6027\u5360\u603b\u4e00\u81f4\u6027\u7684\u6bd4\u4f8b\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728LLaMA-7B\u3001LLaMA-13B\u3001LLaMA2-7B\u548cMistral-7B\u6a21\u578b\u53caSQuAD\u548cCoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86Cleanse\u7684\u6709\u6548\u6027\u3002", "conclusion": "Cleanse\u662f\u4e00\u79cd\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLM\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.14533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "\u63d0\u51faArtiMuse\u6a21\u578b\u548cArtiMuse-10K\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u4e2d\u7684\u6a21\u6001\u504f\u5dee\u548c\u7ec6\u7c92\u5ea6\u5c5e\u6027\u5206\u89e3\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u6559\u80b2\u3001\u827a\u672f\u521b\u4f5c\u548cAI\u751f\u6210\u5185\u5bb9\u7684\u53d1\u5c55\u5bf9\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u504f\u5dee\u548c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "method": "\u5f00\u53d1ArtiMuse\u6a21\u578b\uff0c\u7ed3\u5408\u8bc4\u5206\u548c\u4e13\u5bb6\u7ea7\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u6784\u5efaArtiMuse-10K\u6570\u636e\u96c6\uff0c\u5305\u542b10,000\u5f20\u4e13\u5bb6\u6807\u6ce8\u7684\u56fe\u50cf\u3002", "result": "ArtiMuse\u6a21\u578b\u548cArtiMuse-10K\u6570\u636e\u96c6\u5c06\u516c\u5f00\uff0c\u63a8\u52a8\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u9886\u57df\u53d1\u5c55\u3002", "conclusion": "ArtiMuse\u6a21\u578b\u548c\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14326", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14326", "abs": "https://arxiv.org/abs/2507.14326", "authors": ["Aryana Hou", "Li Lin", "Justin Li", "Shu Hu"], "title": "Rethinking Individual Fairness in Deepfake Detection", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Generative AI models have substantially improved the realism of synthetic\nmedia, yet their misuse through sophisticated DeepFakes poses significant\nrisks. Despite recent advances in deepfake detection, fairness remains\ninadequately addressed, enabling deepfake markers to exploit biases against\nspecific populations. While previous studies have emphasized group-level\nfairness, individual fairness (i.e., ensuring similar predictions for similar\nindividuals) remains largely unexplored. In this work, we identify for the\nfirst time that the original principle of individual fairness fundamentally\nfails in the context of deepfake detection, revealing a critical gap previously\nunexplored in the literature. To mitigate it, we propose the first\ngeneralizable framework that can be integrated into existing deepfake detectors\nto enhance individual fairness and generalization. Extensive experiments\nconducted on leading deepfake datasets demonstrate that our approach\nsignificantly improves individual fairness while maintaining robust detection\nperformance, outperforming state-of-the-art methods. The code is available at\nhttps://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u901a\u7528\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u6ee5\u7528\u5e26\u6765\u4e86\u4e25\u91cd\u98ce\u9669\uff0c\u800c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e2a\u4f53\u516c\u5e73\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7279\u5b9a\u7fa4\u4f53\u53ef\u80fd\u53d7\u5230\u4e0d\u516c\u5e73\u5bf9\u5f85\u3002", "method": "\u4f5c\u8005\u9996\u6b21\u6307\u51fa\u4e2a\u4f53\u516c\u5e73\u6027\u539f\u5219\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6839\u672c\u6027\u5931\u8d25\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u96c6\u6210\u5230\u73b0\u6709\u68c0\u6d4b\u5668\u4e2d\u7684\u901a\u7528\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u9996\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.14853", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14853", "abs": "https://arxiv.org/abs/2507.14853", "authors": ["Khoa Nguyen", "Tanveer Khan", "Antonis Michalas"], "title": "A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training without sharing\nraw data, making it a promising approach for privacy-sensitive domains. Despite\nits potential, FL faces significant challenges, particularly in terms of\ncommunication overhead and data privacy. Privacy-preserving Techniques (PPTs)\nsuch as Homomorphic Encryption (HE) have been used to mitigate these concerns.\nHowever, these techniques introduce substantial computational and communication\ncosts, limiting their practical deployment. In this work, we explore how Hybrid\nHomomorphic Encryption (HHE), a cryptographic protocol that combines symmetric\nencryption with HE, can be effectively integrated with FL to address both\ncommunication and privacy challenges, paving the way for scalable and secure\ndecentralized learning system.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u6df7\u5408\u540c\u6001\u52a0\u5bc6\uff08HHE\uff09\u4e0e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u901a\u4fe1\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7cfb\u7edf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u654f\u611f\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u5f00\u9500\u548c\u6570\u636e\u9690\u79c1\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff08\u5982\u5168\u540c\u6001\u52a0\u5bc6\uff09\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5c06\u6df7\u5408\u540c\u6001\u52a0\u5bc6\uff08HHE\uff09\u4e0e\u8054\u90a6\u5b66\u4e60\u7ed3\u5408\uff0cHHE\u7ed3\u5408\u4e86\u5bf9\u79f0\u52a0\u5bc6\u548c\u5168\u540c\u6001\u52a0\u5bc6\u7684\u4f18\u52bf\u3002", "result": "\u901a\u8fc7HHE\u4e0eFL\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u969c\u6570\u636e\u9690\u79c1\u3002", "conclusion": "HHE\u4e0eFL\u7684\u6574\u5408\u4e3a\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u548c\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.14664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14664", "abs": "https://arxiv.org/abs/2507.14664", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "AI": {"tldr": "Mangosteen\u662f\u4e00\u4e2a47B\u6cf0\u8bed\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u5b9a\u5236\u5316Dolma\u6d41\u7a0b\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cf0\u8bed\u8bed\u8a00\u6a21\u578b\u7684\u8d28\u91cf\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u5904\u7406\u6cf0\u8bed\u65f6\u7f3a\u4e4f\u6587\u5316\u654f\u611f\u6027\u548c\u811a\u672c\u9002\u5e94\u6027\uff0c\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "\u91c7\u7528\u6cf0\u8bed\u9002\u5e94\u7684Dolma\u6d41\u7a0b\uff0c\u5305\u62ec\u81ea\u5b9a\u4e49\u8bed\u8a00\u8bc6\u522b\u3001\u8d28\u91cf\u8fc7\u6ee4\u5668\u548c\u5185\u5bb9\u8fc7\u6ee4\u5668\uff0c\u5e76\u7ed3\u5408\u975e\u7f51\u7edc\u6765\u6e90\u6570\u636e\u3002", "result": "Mangosteen\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5728\u6cf0\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Mangosteen\u4e3a\u6cf0\u8bed\u53ca\u533a\u57df\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u9ad8\u8d28\u91cf\u7684\u8bed\u6599\u5e93\u57fa\u7840\u3002"}}
{"id": "2507.14543", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d4f\u89c8\u5668\u6269\u5c55\uff0c\u5c06\u624b\u8bed\u81ea\u52a8\u7ffb\u8bd1\u4e3a\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u5b57\u5e55\uff0c\u4ee5\u5e2e\u52a9\u542c\u529b\u969c\u788d\u8005\u4e0e\u666e\u901a\u4eba\u6c9f\u901a\u3002", "motivation": "\u542c\u529b\u969c\u788d\u8005\u4e0e\u666e\u901a\u4eba\u6c9f\u901a\u56f0\u96be\uff0c\u75ab\u60c5\u671f\u95f4\u89c6\u9891\u4f1a\u8bae\u6210\u4e3a\u4e3b\u8981\u6c9f\u901a\u65b9\u5f0f\uff0c\u542c\u529b\u969c\u788d\u8005\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u624b\u8bed\u800c\u975e\u6253\u5b57\u3002", "method": "\u5229\u7528\u5305\u542b2000\u591a\u4e2a\u5355\u8bcd\u7ea7ASL\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u6d4f\u89c8\u5668\u6269\u5c55\u5b9e\u73b0\u624b\u8bed\u5230\u5b57\u5e55\u7684\u81ea\u52a8\u7ffb\u8bd1\u3002", "result": "\u901a\u8fc7\u6d4f\u89c8\u5668\u6269\u5c55\u5b9e\u73b0\u624b\u8bed\u5b9e\u65f6\u7ffb\u8bd1\uff0c\u63d0\u5347\u542c\u529b\u969c\u788d\u8005\u5728\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u6c9f\u901a\u6548\u7387\u3002", "conclusion": "\u8be5\u6280\u672f\u6709\u671b\u6d88\u9664\u542c\u529b\u969c\u788d\u8005\u4e0e\u666e\u901a\u4eba\u4e4b\u95f4\u7684\u6c9f\u901a\u969c\u788d\uff0c\u5c24\u5176\u5728\u8fdc\u7a0b\u4ea4\u6d41\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.14332", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14332", "abs": "https://arxiv.org/abs/2507.14332", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert Salko", "Xu Wu"], "title": "Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries", "comment": "Accepted for inclusion in Transactions of the American Nuclear\n  Society for the 2025 ANS Winter Conference", "summary": "Accurate prediction of critical heat flux (CHF) is an essential component of\nsafety analysis in pressurized and boiling water reactors. To support reliable\nprediction of this quantity, several empirical correlations and lookup tables\nhave been constructed from physical experiments over the past several decades.\nWith the onset of accessible machine learning (ML) frameworks, multiple\ninitiatives have been established with the goal of predicting CHF more\naccurately than these traditional methods. While purely data-driven surrogate\nmodeling has been extensively investigated, these approaches lack\ninterpretability, lack resilience to data scarcity, and have been developed\nmostly using data from tube experiments. As a result, bias-correction hybrid\napproaches have become increasingly popular, which correct initial\n\"low-fidelity\" estimates provided by deterministic base models by using\nML-predicted residuals. This body of work has mostly considered round tube\ngeometries; annular geometry-specific ML models have not yet been deployed in\nthermal hydraulic codes. This study developed, deployed, and validated four ML\nmodels to predict CHF in annular geometries using the CTF subchannel code.\nThree empirical correlation models, Biasi, Bowring, and Katto, were used as\nbase models for comparison. The ML models were trained and tested using 577\nexperimental annulus data points from four datasets: Becker, Beus, Janssen, and\nMortimore. Baseline CHF predictions were obtained from the empirical\ncorrelations, with mean relative errors above 26%. The ML-driven models\nachieved mean relative errors below 3.5%, with no more than one point exceeding\nthe 10% error envelope. In all cases, the hybrid ML models significantly\noutperformed their empirical counterparts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u73af\u5f62\u51e0\u4f55\u4e2d\u7684\u4e34\u754c\u70ed\u901a\u91cf\uff08CHF\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ecf\u9a8c\u6a21\u578b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4bCHF\u5bf9\u53cd\u5e94\u5806\u5b89\u5168\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6539\u8fdb\u673a\u4f1a\u3002", "method": "\u4f7f\u7528CTF\u5b50\u901a\u9053\u4ee3\u7801\uff0c\u57fa\u4e8e\u4e09\u79cd\u7ecf\u9a8c\u6a21\u578b\uff08Biasi\u3001Bowring\u3001Katto\uff09\u5f00\u53d1\u4e86\u56db\u79cd\u6df7\u5408ML\u6a21\u578b\uff0c\u5229\u7528577\u4e2a\u5b9e\u9a8c\u6570\u636e\u70b9\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002", "result": "ML\u6a21\u578b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e3.5%\uff0c\u663e\u8457\u4f18\u4e8e\u7ecf\u9a8c\u6a21\u578b\u768426%\u8bef\u5dee\u3002", "conclusion": "\u6df7\u5408ML\u6a21\u578b\u5728\u73af\u5f62\u51e0\u4f55\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3aCHF\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14893", "categories": ["cs.CR", "math.NT", "11T71, 94A60, 68P25, 14G50, 81P94"], "pdf": "https://arxiv.org/pdf/2507.14893", "abs": "https://arxiv.org/abs/2507.14893", "authors": ["Farzin Renan"], "title": "A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies", "comment": null, "summary": "Digital signatures are essential cryptographic tools that provide\nauthentication and integrity in digital communications. However,\nprivacy-sensitive applications, such as e-voting and digital cash, require more\nrestrictive verification models to ensure confidentiality and control. Strong\nDesignated Verifier Signature (SDVS) schemes address this need by enabling the\nsigner to designate a specific verifier, ensuring that only this party can\nvalidate the signature. Existing SDVS constructions are primarily based on\nnumber-theoretic assumptions and are therefore vulnerable to quantum attacks.\nAlthough post-quantum alternatives, particularly those based on lattices, have\nbeen proposed, they often entail large key and signature sizes. In this work,\nwe introduce $\\mathsf{CSI\\text{-}SDVS}$, a novel isogeny-based SDVS scheme that\noffers a compact, quantum-resistant alternative. Our construction builds on the\nideal class group action framework of CSIDH and the signature techniques of\nCSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse\nProblem (MT-GAIP). $\\mathsf{CSI\\text{-}SDVS}$ achieves strong security\nguarantees; namely, Strong Unforgeability under Chosen-Message Attacks\n(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in\nthe random oracle model. Remarkably, both the keys and signatures in\n$\\mathsf{CSI\\text{-}SDVS}$ are of size $\\mathcal{O}(\\lambda)$, representing a\nsignificant improvement over the typical $\\mathcal{O}(\\lambda^2)$ bounds in\nexisting post-quantum SDVS schemes, thereby making it among the most compact\nPQC-based SDVS schemes and the only post-quantum secure construction based on\nisogenies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540c\u6e90\u7684\u65b0\u578b\u5f3a\u6307\u5b9a\u9a8c\u8bc1\u8005\u7b7e\u540d\u65b9\u6848\uff08CSI-SDVS\uff09\uff0c\u5177\u6709\u7d27\u51d1\u7684\u5bc6\u94a5\u548c\u7b7e\u540d\u5927\u5c0f\uff0c\u4e14\u80fd\u62b5\u6297\u91cf\u5b50\u653b\u51fb\u3002", "motivation": "\u9690\u79c1\u654f\u611f\u5e94\u7528\uff08\u5982\u7535\u5b50\u6295\u7968\u548c\u6570\u5b57\u73b0\u91d1\uff09\u9700\u8981\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u6a21\u578b\u4ee5\u786e\u4fdd\u673a\u5bc6\u6027\u548c\u63a7\u5236\uff0c\u800c\u73b0\u6709SDVS\u65b9\u6848\u6613\u53d7\u91cf\u5b50\u653b\u51fb\u4e14\u5bc6\u94a5\u548c\u7b7e\u540d\u8f83\u5927\u3002", "method": "\u57fa\u4e8eCSIDH\u7684\u7406\u60f3\u7c7b\u7fa4\u52a8\u4f5c\u6846\u67b6\u548cCSI-FiSh\u7684\u7b7e\u540d\u6280\u672f\uff0c\u4f9d\u8d56\u4e8e\u591a\u76ee\u6807\u7fa4\u52a8\u4f5c\u9006\u95ee\u9898\uff08MT-GAIP\uff09\u7684\u56f0\u96be\u6027\u3002", "result": "CSI-SDVS\u5b9e\u73b0\u4e86\u5f3a\u5b89\u5168\u6027\uff08SUF-CMA\u3001NT\u3001PSI\uff09\uff0c\u5bc6\u94a5\u548c\u7b7e\u540d\u5927\u5c0f\u4e3aO(\u03bb)\uff0c\u4f18\u4e8e\u73b0\u6709\u540e\u91cf\u5b50SDVS\u65b9\u6848\u7684O(\u03bb\u00b2)\u3002", "conclusion": "CSI-SDVS\u662f\u76ee\u524d\u6700\u7d27\u51d1\u7684\u540e\u91cf\u5b50SDVS\u65b9\u6848\u4e4b\u4e00\uff0c\u4e5f\u662f\u552f\u4e00\u57fa\u4e8e\u540c\u6e90\u7684\u540e\u91cf\u5b50\u5b89\u5168\u6784\u9020\u3002"}}
{"id": "2507.14681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14681", "abs": "https://arxiv.org/abs/2507.14681", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel G\u00f3mez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u7f16\u7801\uff08ICPC-2\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u641c\u7d22\u5f15\u64ce\u7684\u8f93\u51fa\uff0c\u7ed3\u679c\u663e\u793a\u591a\u6570\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u9a8c\u8bc1\u3002", "motivation": "\u533b\u7597\u7f16\u7801\u5bf9\u533b\u7597\u6570\u636e\u7684\u7814\u7a76\u3001\u8d28\u91cf\u76d1\u63a7\u548c\u653f\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5728\u81ea\u52a8\u5316\u7f16\u7801\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528437\u4e2a\u5df4\u897f\u8461\u8404\u7259\u8bed\u4e34\u5e8a\u8868\u8fbe\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bed\u4e49\u641c\u7d22\u5f15\u64ce\u68c0\u7d22\u5019\u9009\u7f16\u7801\uff0c33\u4e2aLLMs\u6a21\u578b\u8fdb\u884c\u5339\u914d\u9009\u62e9\uff0c\u8bc4\u4f30\u6027\u80fd\u6307\u6807\u5305\u62ecF1\u5206\u6570\u3001\u6210\u672c\u7b49\u3002", "result": "28\u4e2a\u6a21\u578bF1\u5206\u6570>0.8\uff0c10\u4e2a>0.85\uff1b\u4f18\u5316\u68c0\u7d22\u5668\u53ef\u63d0\u5347\u6027\u80fd4\u5206\uff1b\u5c0f\u6a21\u578b\u5728\u683c\u5f0f\u548c\u8f93\u5165\u957f\u5ea6\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "LLMs\u5728\u81ea\u52a8\u5316ICPC-2\u7f16\u7801\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u9700\u66f4\u5e7f\u6cdb\u7684\u591a\u8bed\u8a00\u548c\u7aef\u5230\u7aef\u4e34\u5e8a\u9a8c\u8bc1\u3002"}}
{"id": "2507.14544", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9ImageCLEFmed MEDVQA 2025\u6311\u6218\u8d5b\u5b50\u4efb\u52a11\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u65b9\u6cd5\uff0c\u91c7\u7528Florence\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\uff0c\u7ed3\u5408\u9886\u57df\u589e\u5f3a\u6280\u672f\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u533b\u5b66VQA\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Florence\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5bf9KASVIR\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u5b98\u65b9\u6311\u6218\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u533b\u5b66VQA\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u4e3a\u672a\u6765\u5728\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e34\u5e8a\u6574\u5408\u65b9\u9762\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.14344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14344", "abs": "https://arxiv.org/abs/2507.14344", "authors": ["Daniel Fein", "Gabriela Aranguiz-Dias"], "title": "Influence Functions for Preference Dataset Pruning", "comment": null, "summary": "Language models are commonly fine-tuned via reinforcement learning to alter\ntheir behavior or elicit new capabilities. Datasets used for these purposes,\nand particularly human preference datasets, are often noisy. The relatively\nsmall size post-training datasets, combined with parameter-efficient\nfine-tuning methods, enable the use of influence functions approximations to\ndetect and prune training examples that are harmful to performance on a\nvalidation set. In this work, we adapt the TL;DR dataset for reward model\ntraining to demonstrate how conjugate-gradient approximated influence functions\ncan be used to filter datasets. In our experiments, influence function\nfiltering yields a small retraining accuracy uplift of 1.5% after removing 10%\nof training examples. We also show that gradient similarity outperforms\ninfluence functions for detecting helpful training examples. This suggests that\nlocal curvature is important for detecting harmful training examples, but less\nso for identifying helpful examples.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u8fc7\u6ee4\u566a\u58f0\u6570\u636e\uff0c\u63d0\u5347\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u79fb\u966410%\u8bad\u7ec3\u6570\u636e\u540e\u51c6\u786e\u7387\u63d0\u53471.5%\u3002", "motivation": "\u4eba\u7c7b\u504f\u597d\u6570\u636e\u901a\u5e38\u566a\u58f0\u8f83\u5927\uff0c\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u6548\u679c\uff0c\u9700\u6709\u6548\u65b9\u6cd5\u68c0\u6d4b\u548c\u8fc7\u6ee4\u6709\u5bb3\u6570\u636e\u3002", "method": "\u4f7f\u7528\u5171\u8f6d\u68af\u5ea6\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u8fc7\u6ee4TL;DR\u6570\u636e\u96c6\u4e2d\u7684\u566a\u58f0\u6570\u636e\uff0c\u5e76\u6bd4\u8f83\u68af\u5ea6\u76f8\u4f3c\u6027\u4e0e\u5f71\u54cd\u51fd\u6570\u7684\u6548\u679c\u3002", "result": "\u79fb\u966410%\u8bad\u7ec3\u6570\u636e\u540e\uff0c\u51c6\u786e\u7387\u63d0\u53471.5%\uff1b\u68af\u5ea6\u76f8\u4f3c\u6027\u5728\u68c0\u6d4b\u6709\u76ca\u6570\u636e\u65f6\u4f18\u4e8e\u5f71\u54cd\u51fd\u6570\u3002", "conclusion": "\u5c40\u90e8\u66f2\u7387\u5bf9\u68c0\u6d4b\u6709\u5bb3\u6570\u636e\u91cd\u8981\uff0c\u4f46\u5bf9\u6709\u76ca\u6570\u636e\u5f71\u54cd\u8f83\u5c0f\uff0c\u68af\u5ea6\u76f8\u4f3c\u6027\u662f\u66f4\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2507.14985", "categories": ["cs.CR", "cs.ET", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14985", "abs": "https://arxiv.org/abs/2507.14985", "authors": ["Argianto Rahartomo", "Leonel Merino", "Mohammad Ghafari"], "title": "Metaverse Security and Privacy Research: A Systematic Review", "comment": "The paper is accepted for publication at Computers & Security Journal", "summary": "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e862013-2024\u5e74\u95f4\u5173\u4e8e\u5143\u5b87\u5b99\u5b89\u5168\u4e0e\u9690\u79c1\u7684\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u65b9\u6cd5\u3001\u91cd\u70b9\u9886\u57df\u53ca\u7814\u7a76\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4ecd\u9700\u89e3\u51b3\u7684\u6311\u6218\u3002", "motivation": "\u5143\u5b87\u5b99\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u73b0\u6709\u7814\u7a76\u4ee5\u6307\u5bfc\u672a\u6765\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u6309\u65b9\u6cd5\u3001\u5b89\u5168\u9690\u79c1\u5c5e\u6027\u3001\u6c89\u6d78\u5f0f\u7ec4\u4ef6\u548c\u8bc4\u4f30\u7b56\u7565\u5bf9\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u8fc7\u53bb\u4e94\u5e74\u7814\u7a76\u6fc0\u589e\uff0c\u91cd\u70b9\u5173\u6ce8\u7528\u6237\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u4f46\u653f\u7b56\u5408\u89c4\u3001\u53ef\u8bbf\u95ee\u6027\u7b49\u9886\u57df\u4ecd\u5b58\u7a7a\u767d\u3002", "conclusion": "\u5143\u5b87\u5b99\u5b89\u5168\u9700\u7ed3\u5408\u6280\u672f\u4e0e\u4eba\u6587\u56e0\u7d20\uff0c\u547c\u5401\u8de8\u5b66\u79d1\u5408\u4f5c\u4ee5\u6784\u5efa\u53ef\u4fe1\u7684\u6c89\u6d78\u5f0f\u73af\u5883\u3002"}}
{"id": "2507.14683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14683", "abs": "https://arxiv.org/abs/2507.14683", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MiroMind-M1\u7cfb\u5217\uff0c\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c01\u95ed\u6e90\u4ee3\u7801\u7684\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-3\uff09\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u590d\u73b0\u6027\uff0c\u800c\u73b0\u6709\u5f00\u6e90\u9879\u76ee\u4e5f\u5e38\u56e0\u7f3a\u5c11\u5173\u952e\u8d44\u6e90\uff08\u5982\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u914d\u7f6e\uff09\u800c\u53d7\u9650\u3002", "method": "\u6a21\u578b\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728719K\u6570\u5b66\u63a8\u7406\u95ee\u9898\u7684\u7cbe\u9009\u8bed\u6599\u4e0a\u8fdb\u884cSFT\uff0c\u968f\u540e\u572862K\u53ef\u9a8c\u8bc1\u95ee\u9898\u4e0a\u8fdb\u884cRLVR\uff0c\u5e76\u5f15\u5165Context-Aware Multi-Stage Policy Optimization\u7b97\u6cd5\u3002", "result": "\u6a21\u578b\u5728AIME24\u3001AIME25\u548cMATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "MiroMind-M1\u7cfb\u5217\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u8d44\u6e90\uff08\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u914d\u7f6e\uff09\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2507.14549", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u4e0e\u4eba\u7c7b\u60c5\u611f\u611f\u77e5\u7684\u5173\u8054\uff0c\u53d1\u73b0ANN\u5206\u7c7b\u6a21\u7cca\u7684\u523a\u6fc0\u540c\u6837\u5f15\u53d1\u4eba\u7c7b\u611f\u77e5\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u884c\u4e3a\u6570\u636e\u4f18\u5316ANN\u6a21\u578b\uff0c\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u611f\u77e5\u6a21\u5f0f\u7684\u5bf9\u9f50\u3002", "motivation": "\u60c5\u611f\u8ba4\u77e5\u79d1\u5b66\u7684\u6838\u5fc3\u6311\u6218\u662f\u5efa\u7acb\u5916\u90e8\u60c5\u611f\u523a\u6fc0\u4e0e\u4eba\u7c7b\u5185\u90e8\u4f53\u9a8c\u7684\u51c6\u786e\u6a21\u578b\u3002\u5c3d\u7ba1ANN\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u4eba\u7c7b\u611f\u77e5\u4e2a\u4f53\u5dee\u5f02\u7684\u5efa\u6a21\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u611f\u77e5\u8fb9\u754c\u91c7\u6837\u65b9\u6cd5\uff0c\u751f\u6210\u4f4d\u4e8eANN\u51b3\u7b56\u8fb9\u754c\u7684\u9762\u90e8\u8868\u60c5\u523a\u6fc0\uff0c\u6784\u5efavarEmotion\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5206\u6790\u663e\u793a\uff0cANN\u5206\u7c7b\u6a21\u7cca\u7684\u523a\u6fc0\u540c\u6837\u5f15\u53d1\u4eba\u7c7b\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u60c5\u611f\u611f\u77e5\u7684\u5171\u4eab\u8ba1\u7b97\u539f\u5219\u3002\u901a\u8fc7\u884c\u4e3a\u6570\u636e\u5fae\u8c03ANN\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u53ca\u4e2a\u4f53\u611f\u77e5\u6a21\u5f0f\u7684\u5339\u914d\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86ANN\u51b3\u7b56\u8fb9\u754c\u4e0e\u4eba\u7c7b\u611f\u77e5\u53d8\u5f02\u6027\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u8054\u7cfb\uff0c\u4e3a\u60c5\u611f\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.14353", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14353", "abs": "https://arxiv.org/abs/2507.14353", "authors": ["Harsh Nilesh Pathak", "Randy Paffenroth"], "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers", "comment": null, "summary": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge.", "AI": {"tldr": "Solo Connection\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u89e3\u7801\u5668\u5757\u7ea7\u522b\u7684\u8868\u793a\u800c\u975e\u5355\u4e2a\u6743\u91cd\u77e9\u9635\uff0c\u4f18\u4e8eLoRA\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6539\u8fdb\u73b0\u6709PEFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u957f\u8df3\u8dc3\u8fde\u63a5\u548c\u57fa\u4e8e\u540c\u4f26\u7406\u8bba\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7a33\u5b9a\u7684\u6a21\u578b\u9002\u5e94\u3002", "method": "\u63d0\u51faSolo Connection\u65b9\u6cd5\uff0c\u5229\u7528\u89e3\u7801\u5668\u5757\u7ea7\u522b\u7684\u957f\u8df3\u8dc3\u8fde\u63a5\u548c\u7ebf\u6027\u53d8\u6362\uff0c\u9010\u6b65\u63d2\u503c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u3002", "result": "Solo Connection\u5728E2E\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8eLoRA\uff0c\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1159%\uff08\u76f8\u6bd4LoRA\uff09\u548c99%\u4ee5\u4e0a\uff08\u76f8\u6bd4\u5168\u5fae\u8c03\uff09\u3002", "conclusion": "Solo Connection\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u591a\u5c42\u6a21\u578b\u7684\u4efb\u52a1\u9002\u5e94\u3002"}}
{"id": "2507.15058", "categories": ["cs.CR", "cs.LG", "cs.SE", "D.2.5; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.15058", "abs": "https://arxiv.org/abs/2507.15058", "authors": ["Ian Hardgrove", "John D. Hastings"], "title": "LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries", "comment": "6 pages, 2 figures, 1 table, 2 listings", "summary": "A fundamental problem in cybersecurity and computer science is determining\nwhether a program is free of bugs and vulnerabilities. Fuzzing, a popular\napproach to discovering vulnerabilities in programs, has several advantages\nover alternative strategies, although it has investment costs in the form of\ninitial setup and continuous maintenance. The choice of fuzzing is further\ncomplicated when only a binary library is available, such as the case of\nclosed-source and proprietary software. In response, we introduce LibLMFuzz, a\nframework that reduces costs associated with fuzzing closed-source libraries by\npairing an agentic Large Language Model (LLM) with a lightweight tool-chain\n(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan\nfuzz strategies, generate drivers, and iteratively self-repair build or runtime\nerrors. Tested on four widely-used Linux libraries, LibLMFuzz produced\nsyntactically correct drivers for all 558 fuzz-able API functions, achieving\n100% API coverage with no human intervention. Across the 1601 synthesized\ndrivers, 75.52% were nominally correct on first execution. The results show\nthat LLM-augmented middleware holds promise in reducing the costs of fuzzing\nblack box components and provides a foundation for future research efforts.\nFuture opportunities exist for research in branch coverage.", "AI": {"tldr": "LibLMFuzz\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u8f7b\u91cf\u7ea7\u5de5\u5177\u94fe\uff0c\u81ea\u52a8\u5316\u5206\u6790\u95ed\u6e90\u5e93\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u51cf\u5c11\u6a21\u7cca\u6d4b\u8bd5\u7684\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u95ed\u6e90\u548c\u4e13\u6709\u8f6f\u4ef6\u4e2d\u6a21\u7cca\u6d4b\u8bd5\u7684\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528LLM\u548c\u5de5\u5177\u94fe\uff08\u53cd\u6c47\u7f16\u5668/\u7f16\u8bd1\u5668/\u6a21\u7cca\u6d4b\u8bd5\u5668\uff09\u81ea\u4e3b\u5206\u6790\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u751f\u6210\u9a71\u52a8\u7a0b\u5e8f\u5e76\u81ea\u6211\u4fee\u590d\u9519\u8bef\u3002", "result": "\u5728\u56db\u4e2aLinux\u5e93\u4e0a\u6d4b\u8bd5\uff0c\u8986\u76d6100%\u7684API\u51fd\u6570\uff0c75.52%\u7684\u9a71\u52a8\u7a0b\u5e8f\u9996\u6b21\u6267\u884c\u6b63\u786e\u3002", "conclusion": "LLM\u589e\u5f3a\u7684\u4e2d\u95f4\u4ef6\u6709\u671b\u964d\u4f4e\u9ed1\u76d2\u7ec4\u4ef6\u7684\u6a21\u7cca\u6d4b\u8bd5\u6210\u672c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.14688", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14688", "abs": "https://arxiv.org/abs/2507.14688", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86Hugging Face Hub\u4e0a\u516c\u5f00\u7684\u963f\u62c9\u4f2f\u8bed\u540e\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ece\u56db\u4e2a\u7ef4\u5ea6\uff08LLM\u80fd\u529b\u3001\u53ef\u64cd\u63a7\u6027\u3001\u5bf9\u9f50\u6027\u548c\u9c81\u68d2\u6027\uff09\u8bc4\u4f30\u5176\u8d28\u91cf\uff0c\u53d1\u73b0\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\u3001\u6587\u6863\u7f3a\u5931\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u540e\u8bad\u7ec3\u662f\u63d0\u5347\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u6280\u672f\uff0c\u800c\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u5bf9\u540e\u8bad\u7ec3\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u73b0\u6709\u963f\u62c9\u4f2f\u8bed\u540e\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u73b0\u72b6\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\uff08LLM\u80fd\u529b\u3001\u53ef\u64cd\u63a7\u6027\u3001\u5bf9\u9f50\u6027\u548c\u9c81\u68d2\u6027\uff09\u5bf9Hugging Face Hub\u4e0a\u7684\u963f\u62c9\u4f2f\u8bed\u540e\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8003\u5bdf\u5176\u6d41\u884c\u5ea6\u3001\u5b9e\u7528\u6027\u3001\u6587\u6863\u8d28\u91cf\u7b49\u6307\u6807\u3002", "result": "\u53d1\u73b0\u963f\u62c9\u4f2f\u8bed\u540e\u8bad\u7ec3\u6570\u636e\u96c6\u5b58\u5728\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\u3001\u6587\u6863\u548c\u6807\u6ce8\u4e0d\u4e00\u81f4\u3001\u793e\u533a\u91c7\u7528\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u6307\u51fa\u4e86\u963f\u62c9\u4f2f\u8bed\u540e\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u5177\u4f53\u6539\u8fdb\u5efa\u8bae\uff0c\u4ee5\u4fc3\u8fdb\u963f\u62c9\u4f2f\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002"}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u673a\u5f15\u5bfc\u7cfb\u7edf\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u548c\u53bb\u9664\u7167\u7247\u4e2d\u7684\u6742\u4e71\u5143\u7d20\uff0c\u63d0\u5347\u7167\u7247\u7f8e\u5b66\u8d28\u91cf\u3002", "motivation": "\u7167\u7247\u4e2d\u7684\u6742\u4e71\u5143\u7d20\u4f1a\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5f71\u54cd\u60c5\u611f\u6216\u6545\u4e8b\u7684\u4f20\u8fbe\uff0c\u5c24\u5176\u662f\u4e1a\u4f59\u6444\u5f71\u5e08\u5e38\u56e0\u758f\u5ffd\u6216\u7ecf\u9a8c\u4e0d\u8db3\u800c\u672a\u80fd\u907f\u514d\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u7b97\u6cd5\u8bc4\u4f30\u5bf9\u8c61\u5bf9\u7167\u7247\u7f8e\u5b66\u548c\u5185\u5bb9\u7684\u8d21\u732e\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u6742\u4e71\u8bc6\u522b\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4fee\u590d\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u8bc6\u522b\u5e72\u6270\u7269\u5e76\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7075\u6d3b\u7684\u754c\u9762\u548c\u7cbe\u51c6\u7684\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6444\u5f71\u4f5c\u54c1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.14387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14387", "abs": "https://arxiv.org/abs/2507.14387", "authors": ["Arun Vignesh Malarkkan", "Dongjie Wang", "Haoyue Bai", "Yanjie Fu"], "title": "Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures", "comment": "12 pages, 5 figures, 3 Tables, under review in IEEE Transactions on\n  Big Data", "summary": "The escalating threat of cyberattacks on real-time critical infrastructures\nposes serious risks to public safety, demanding detection methods that\neffectively capture complex system interdependencies and adapt to evolving\nattack patterns. Traditional real-time anomaly detection techniques often\nsuffer from excessive false positives due to their statistical sensitivity to\nhigh data variance and class imbalance. To address these limitations, recent\nresearch has explored modeling causal relationships among system components.\nHowever, prior work mainly focuses on offline causal graph-based approaches\nthat require static historical data and fail to generalize to real-time\nsettings. These methods are fundamentally constrained by: (1) their inability\nto adapt to dynamic shifts in data distribution without retraining, and (2) the\nrisk of catastrophic forgetting when lacking timely supervision in live\nsystems. To overcome these challenges, we propose INCADET, a novel framework\nfor incremental causal graph learning tailored to real-time cyberattack\ndetection. INCADET dynamically captures evolving system behavior by\nincrementally updating causal graphs across streaming time windows. The\nframework comprises three modules: 1) Early Symptom Detection: Detects\ntransitions in system status using divergence in edge-weight distributions\nacross sequential causal graphs. 2) Incremental Causal Graph Learning:\nLeverages experience replay and edge reinforcement to continually refine causal\nstructures while preserving prior knowledge. 3) Causal Graph Classification:\nEmploys Graph Convolutional Networks (GCNs) to classify system status using the\nlearned causal graphs. Extensive experiments on real-world critical\ninfrastructure datasets demonstrate that INCADET achieves superior accuracy,\nrobustness, and adaptability compared to both static causal and deep temporal\nbaselines in evolving attack scenarios.", "AI": {"tldr": "INCADET\u662f\u4e00\u79cd\u9488\u5bf9\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u7684\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u56e0\u679c\u56fe\u6765\u9002\u5e94\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u7f51\u7edc\u653b\u51fb\u5bf9\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u56e0\u9ad8\u6570\u636e\u65b9\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u8bef\u62a5\u7387\u9ad8\uff0c\u4e14\u73b0\u6709\u56e0\u679c\u56fe\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u52a8\u6001\u53d8\u5316\u3002", "method": "INCADET\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u65e9\u671f\u75c7\u72b6\u68c0\u6d4b\u3001\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u548c\u56e0\u679c\u56fe\u5206\u7c7b\uff0c\u7ed3\u5408\u7ecf\u9a8c\u91cd\u653e\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u52a8\u6001\u66f4\u65b0\u56e0\u679c\u56fe\u3002", "result": "\u5728\u771f\u5b9e\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cINCADET\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u9759\u6001\u56e0\u679c\u548c\u6df1\u5ea6\u65f6\u5e8f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "INCADET\u4e3a\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15219", "abs": "https://arxiv.org/abs/2507.15219", "authors": ["Tianneng Shi", "Kaijie Zhu", "Zhun Wang", "Yuqi Jia", "Will Cai", "Weida Liang", "Haonan Wang", "Hend Alzahrani", "Joshua Lu", "Kenji Kawaguchi", "Basel Alomair", "Xuandong Zhao", "William Yang Wang", "Neil Gong", "Wenbo Guo", "Dawn Song"], "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses", "comment": null, "summary": "Despite their potential, recent research has demonstrated that LLM agents are\nvulnerable to prompt injection attacks, where malicious prompts are injected\ninto the agent's input, causing it to perform an attacker-specified task rather\nthan the intended task provided by the user. In this paper, we present\nPromptArmor, a simple yet effective defense against prompt injection attacks.\nSpecifically, PromptArmor prompts an off-the-shelf LLM to detect and remove\npotential injected prompts from the input before the agent processes it. Our\nresults show that PromptArmor can accurately identify and remove injected\nprompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves\nboth a false positive rate and a false negative rate below 1% on the AgentDojo\nbenchmark. Moreover, after removing injected prompts with PromptArmor, the\nattack success rate drops to below 1%. We also demonstrate PromptArmor's\neffectiveness against adaptive attacks and explore different strategies for\nprompting an LLM. We recommend that PromptArmor be adopted as a standard\nbaseline for evaluating new defenses against prompt injection attacks.", "AI": {"tldr": "PromptArmor\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u79fb\u9664LLM\u4ee3\u7406\u4e2d\u7684\u6076\u610f\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "LLM\u4ee3\u7406\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5bfc\u81f4\u6267\u884c\u6076\u610f\u4efb\u52a1\u800c\u975e\u7528\u6237\u610f\u56fe\u3002", "method": "PromptArmor\u5229\u7528\u73b0\u6210LLM\u68c0\u6d4b\u5e76\u79fb\u9664\u8f93\u5165\u4e2d\u7684\u6f5c\u5728\u6ce8\u5165\u63d0\u793a\u3002", "result": "\u5728AgentDojo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPromptArmor\u7684\u8bef\u62a5\u7387\u548c\u6f0f\u62a5\u7387\u5747\u4f4e\u4e8e1%\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u81f31%\u4ee5\u4e0b\u3002", "conclusion": "PromptArmor\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u65b0\u9632\u5fa1\u65b9\u6cd5\u7684\u6807\u51c6\u57fa\u7ebf\u3002"}}
{"id": "2507.14693", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14693", "abs": "https://arxiv.org/abs/2507.14693", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u571f\u8033\u5176\u8bed\u81ea\u6740\u610f\u5ff5\u68c0\u6d4b\u8bed\u6599\u5e93\uff0c\u5e76\u901a\u8fc7\u591a\u6807\u6ce8\u8005\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6846\u67b6\u89e3\u51b3\u4e86\u6807\u6ce8\u53ef\u9760\u6027\u95ee\u9898\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u81ea\u6740\u610f\u5ff5\u68c0\u6d4b\u9762\u4e34\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u548c\u6807\u6ce8\u4e0d\u53ef\u9760\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u4f4e\u8d28\u91cf\u6216\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86AI\u5728\u5168\u7403\u81ea\u6740\u9884\u9632\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u571f\u8033\u5176\u8bed\u793e\u4ea4\u5a92\u4f53\u8bed\u6599\u5e93\uff0c\u5f15\u5165\u591a\u6807\u6ce8\u8005\u548cLLMs\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u6807\u6ce8\u548c\u6a21\u578b\u53ef\u9760\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u547c\u5401\u5728\u5fc3\u7406\u5065\u5eb7NLP\u4e2d\u91c7\u7528\u66f4\u4e25\u683c\u3001\u8bed\u8a00\u5305\u5bb9\u7684\u6807\u6ce8\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u6570\u636e\u548c\u6a21\u578b\u7684\u900f\u660e\u5ea6\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Descrip3D\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u663e\u5f0f\u7f16\u7801\u5bf9\u8c61\u95f4\u5173\u7cfb\uff0c\u63d0\u53473D\u573a\u666f\u7406\u89e3\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f-\u8bed\u8a00\u6a21\u578b\u5728\u5173\u7cfb\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u5d4c\u5165\u65e0\u6cd5\u5145\u5206\u8868\u8fbe\u5bf9\u8c61\u89d2\u8272\u548c\u4ea4\u4e92\u3002", "method": "Descrip3D\u4e3a\u6bcf\u4e2a\u5bf9\u8c61\u6dfb\u52a0\u6587\u672c\u63cf\u8ff0\uff0c\u6355\u6349\u5176\u5c5e\u6027\u548c\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u878d\u5408\u548c\u63d0\u793a\u7ea7\u6ce8\u5165\u5b9e\u73b0\u53cc\u7ea7\u96c6\u6210\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDescrip3D\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u5173\u7cfb\u8868\u793a\u5bf9\u590d\u6742\u5ba4\u5185\u573a\u666f\u7406\u89e3\u6709\u6548\u3002"}}
{"id": "2507.14419", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14419", "abs": "https://arxiv.org/abs/2507.14419", "authors": ["Guojun Wu"], "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "comment": null, "summary": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u6548\u679c\u6765\u81ea\u9650\u5236\u6700\u5927\u957f\u5ea6\uff0c\u800c\u901a\u8fc7\u8ffd\u52a0\u201cWait\u201d\u8fdb\u884c\u6269\u5c55\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u3002o1\u7c7b\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u7136\u6269\u5c55\u8ba1\u7b97\u80fd\u529b\uff0c\u6027\u80fd\u8d85\u8d8a\u5cf0\u503c\u3002", "motivation": "\u7814\u7a76\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\u53ca\u5176\u4e0eo1\u7c7b\u6a21\u578b\u7684\u533a\u522b\uff0c\u4ee5\u660e\u786e\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u7684\u771f\u6b63\u76ee\u6807\u3002", "method": "\u5206\u6790\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u5305\u62ec\u9650\u5236\u6700\u5927\u957f\u5ea6\u548c\u8ffd\u52a0\u201cWait\u201d\u6269\u5c55\uff0c\u5e76\u4e0eo1\u7c7b\u6a21\u578b\uff08\u5982DeepSeek-R1@\uff09\u7684\u6269\u5c55\u884c\u4e3a\u5bf9\u6bd4\u3002", "result": "\u9650\u5236\u6700\u5927\u957f\u5ea6\u662f\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u4e3b\u8981\u6548\u679c\u6765\u6e90\uff0c\u800c\u8ffd\u52a0\u201cWait\u201d\u4f1a\u5bfc\u81f4\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u7684\u632f\u8361\u3002o1\u7c7b\u6a21\u578b\u901a\u8fc7\u81ea\u7136\u6269\u5c55\u8ba1\u7b97\u80fd\u529b\u5b9e\u73b0\u66f4\u9ad8\u6027\u80fd\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u7684\u76ee\u6807\u5e94\u662f\u89e3\u9501\u66f4\u9ad8\u6027\u80fd\uff0c\u800c\u975e\u4ec5\u590d\u5236\u7f29\u653e\u884c\u4e3a\u7684\u5916\u89c2\u3002o1\u7c7b\u6a21\u578b\u7684\u81ea\u7136\u6269\u5c55\u80fd\u529b\u4f18\u4e8e\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u3002"}}
{"id": "2507.15377", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15377", "abs": "https://arxiv.org/abs/2507.15377", "authors": ["Magali Bardet", "Charles Brion", "Philippe Gaborit", "Mercedes Haiech", "Romaric Neveu"], "title": "The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head", "comment": null, "summary": "Nowadays, equivalence problems are widely used in cryptography, most notably\nto establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as\nthe most recent ones. However, in the context of matrix codes, only the code\nequivalence problem has been studied, while the subcode equivalence is\nwell-defined in the Hamming metric. In this work, we introduce two new\nproblems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted\nKernel Problem, to which we apply the MPCitH paradigm to build a signature\nscheme. These new problems, closely related to the Matrix Code Equivalence\nproblem, ask to find an isometry given a code $C$ and a subcode $D$.\nFurthermore, we prove that the Matrix Subcode Equivalence problem reduces to\nthe Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus\nintroducing the matrix code version of the Permuted Kernel Problem. We also\nadapt the combinatorial and algebraic algorithms for the Matrix Code\nEquivalence problem to the subcode case, and we analyze their complexities. We\nfind with this analysis that the algorithms perform much worse than in the code\nequivalence case, which is the same as what happens in the Hamming metric.\nFinally, our analysis of the attacks allows us to take parameters much smaller\nthan in the Matrix Code Equivalence case. Coupled with the effectiveness of\n\\textit{Threshold-Computation-in-the-Head} or \\textit{VOLE-in-the-Head}, we\nobtain a signature size of $\\approx$ 4 800 Bytes, with a public key of\n$\\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings\ndiversity in the landscape of post-quantum signature schemes, by relying on a\nnew hard problem. In particular, this new signature scheme performs better than\nSPHINCS+, with a smaller size of public key + signature. Our signature compares\nalso well with other signature schemes: compared to MEDS, the signature is\nsmaller, and we reduced the size of the sum of signature and public key by a\nfactor close to 5. We also obtain a signature size that is almost half the size\nof the CROSS signature scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u77e9\u9635\u7801\u7b49\u4ef7\u95ee\u9898\uff0c\u5e76\u57fa\u4e8eMPCitH\u8303\u5f0f\u6784\u5efa\u4e86\u4e00\u4e2a\u7b7e\u540d\u65b9\u6848\uff0c\u5176\u7b7e\u540d\u548c\u516c\u94a5\u5c3a\u5bf8\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u5728\u77e9\u9635\u7801\u9886\u57df\uff0c\u4ec5\u7814\u7a76\u4e86\u7801\u7b49\u4ef7\u95ee\u9898\uff0c\u800c\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u5728\u6c49\u660e\u5ea6\u91cf\u4e2d\u5df2\u6709\u5b9a\u4e49\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u7b7e\u540d\u65b9\u6848\u3002", "method": "\u5f15\u5165\u77e9\u9635\u5b50\u7801\u7b49\u4ef7\u95ee\u9898\u548c\u77e9\u9635\u7801\u7f6e\u6362\u6838\u95ee\u9898\uff0c\u5e94\u7528MPCitH\u8303\u5f0f\u6784\u5efa\u7b7e\u540d\u65b9\u6848\uff0c\u5e76\u5206\u6790\u7b97\u6cd5\u7684\u590d\u6742\u6027\u3002", "result": "\u65b0\u65b9\u6848\u7684\u7b7e\u540d\u5c3a\u5bf8\u7ea6\u4e3a4800\u5b57\u8282\uff0c\u516c\u94a5\u5c3a\u5bf8\u7ea6\u4e3a275\u5b57\u8282\uff0c\u4f18\u4e8eSPHINCS+\u3001MEDS\u548cCROSS\u7b49\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u4f9d\u8d56\u65b0\u7684\u96be\u9898\uff0c\u8be5\u7b7e\u540d\u65b9\u6848\u5728\u6027\u80fd\u548c\u591a\u6837\u6027\u4e0a\u4e3a\u540e\u91cf\u5b50\u7b7e\u540d\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u9009\u62e9\u3002"}}
{"id": "2507.14741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14741", "abs": "https://arxiv.org/abs/2507.14741", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u540c\u884c\u8bc4\u5ba1\u4e2d\u5b58\u5728\u8bed\u8a00\u504f\u89c1\uff0c\u533f\u540d\u6027\u5bf9\u516c\u5e73\u6027\u7684\u5f71\u54cd\u4e0e\u4f20\u7edf\u5047\u8bbe\u4e0d\u540c\u3002", "motivation": "\u63a2\u8ba8\u540c\u884c\u8bc4\u5ba1\u4e2d\u8bed\u8a00\u5982\u4f55\u5f3a\u5316\u4e0d\u5e73\u7b49\uff0c\u63ed\u793a\u9690\u85cf\u7684\u504f\u89c1\u3002", "method": "\u5bf98\u4e07\u591a\u7bc7\u8bc4\u5ba1\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u7edf\u8ba1\u5efa\u6a21\uff0c\u5206\u6790\u4f5c\u8005\u6027\u522b\u3001\u79cd\u65cf\u548c\u673a\u6784\u80cc\u666f\u5bf9\u8bc4\u5ba1\u8bed\u8a00\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8bc4\u5ba1\u8bed\u6c14\u3001\u60c5\u611f\u548c\u652f\u6301\u6027\u8bed\u8a00\u56e0\u4f5c\u8005\u80cc\u666f\u800c\u5f02\uff0c\u533f\u540d\u6027\u62ab\u9732\u5f71\u54cd\u8bc4\u5ba1\u8bed\u8a00\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u533f\u540d\u6027\u4fc3\u8fdb\u516c\u5e73\u7684\u5047\u8bbe\uff0c\u4e3a\u5b66\u672f\u51fa\u7248\u6539\u9769\u63d0\u4f9b\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2507.14559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLEAD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4f18\u5316\u8fc7\u7a0b\u548c\u975e\u7ebf\u6027\u6f14\u5316\uff0c\u6709\u6548\u9884\u6d4b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u6570\u91cf\u6fc0\u589e\uff0c\u5982\u4f55\u9ad8\u6548\u9009\u62e9\u9002\u5408\u4e0b\u6e38\u4efb\u52a1\u7684\u6a21\u578b\u6210\u4e3a\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u51c6\u786e\u6355\u6349\u5fae\u8c03\u52a8\u6001\u7684\u975e\u7ebf\u6027\u7279\u6027\u3002", "method": "LEAD\u57fa\u4e8e\u7f51\u7edc\u8f93\u51fa\u7684logits\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5efa\u6a21\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u7528ODE\u63cf\u8ff0\u975e\u7ebf\u6027\u6f14\u5316\uff0c\u8bbe\u8ba1\u7c7b\u611f\u77e5\u5206\u89e3\u65b9\u6cd5\u3002", "result": "\u572824\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c10\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793aLEAD\u6027\u80fd\u4f18\u5f02\uff0c\u9002\u5e94\u6027\u5f3a\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u3002", "conclusion": "LEAD\u901a\u8fc7\u4f18\u5316\u76ee\u6807\u5bf9\u9f50\u548c\u975e\u7ebf\u6027\u5efa\u6a21\uff0c\u5355\u6b65\u89e3\u51b3\u4f18\u5316\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u9009\u62e9\u6548\u7387\u3002"}}
{"id": "2507.14446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14446", "abs": "https://arxiv.org/abs/2507.14446", "authors": ["Feng Liu", "Ying Liu", "Carson Eisenach"], "title": "Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness", "comment": null, "summary": "In this work, we study how to efficiently apply reinforcement learning (RL)\nfor solving large-scale stochastic optimization problems by leveraging\nintervention models. The key of the proposed methodology is to better explore\nthe solution space by simulating and composing the stochastic processes using\npre-trained deep learning (DL) models. We demonstrate our approach on a\nchallenging real-world application, the multi-sourcing multi-period inventory\nmanagement problem in supply chain optimization. In particular, we employ deep\nRL models for learning and forecasting the stochastic supply chain processes\nunder a range of assumptions. Moreover, we also introduce a constraint\ncoordination mechanism, designed to forecast dual costs given the\ncross-products constraints in the inventory network. We highlight that instead\nof directly modeling the complex physical constraints into the RL optimization\nproblem and solving the stochastic problem as a whole, our approach breaks down\nthose supply chain processes into scalable and composable DL modules, leading\nto improved performance on large real-world datasets. We also outline open\nproblems for future research to further investigate the efficacy of such\nmodels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f9b\u5e94\u94fe\u4e2d\u7684\u591a\u6e90\u591a\u5468\u671f\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5982\u4f55\u9ad8\u6548\u5730\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u590d\u6742\u7684\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6a21\u62df\u548c\u7ec4\u5408\u968f\u673a\u8fc7\u7a0b\uff0c\u4ee5\u66f4\u597d\u5730\u63a2\u7d22\u89e3\u7a7a\u95f4\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5b66\u4e60\u548c\u9884\u6d4b\u968f\u673a\u4f9b\u5e94\u94fe\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u7ea6\u675f\u534f\u8c03\u673a\u5236\u6765\u9884\u6d4b\u5e93\u5b58\u7f51\u7edc\u4e2d\u7684\u53cc\u91cd\u6210\u672c\u3002\u901a\u8fc7\u5c06\u590d\u6742\u7684\u7269\u7406\u7ea6\u675f\u5206\u89e3\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u7ec4\u5408\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u5efa\u6a21\u7684\u590d\u6742\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u578b\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u4ee5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u6b64\u7c7b\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.15393", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15393", "abs": "https://arxiv.org/abs/2507.15393", "authors": ["Ruofan Liu", "Yun Lin", "Silas Yeo Shuen Yu", "Xiwen Teoh", "Zhenkai Liang", "Jin Song Dong"], "title": "PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants", "comment": null, "summary": "Phishing emails are a critical component of the cybercrime kill chain due to\ntheir wide reach and low cost. Their ever-evolving nature renders traditional\nrule-based and feature-engineered detectors ineffective in the ongoing arms\nrace between attackers and defenders. The rise of large language models (LLMs)\nfurther exacerbates the threat, enabling attackers to craft highly convincing\nphishing emails at minimal cost.\n  This work demonstrates that LLMs can generate psychologically persuasive\nphishing emails tailored to victim profiles, successfully bypassing nearly all\ncommercial and academic detectors. To defend against such threats, we propose\nPiMRef, the first reference-based phishing email detector that leverages\nknowledge-based invariants. Our core insight is that persuasive phishing emails\noften contain disprovable identity claims, which contradict real-world facts.\nPiMRef reframes phishing detection as an identity fact-checking task. Given an\nemail, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the\nlegitimacy of the sender's domain against a predefined knowledge base, and\n(iii) detects call-to-action prompts that push user engagement. Contradictory\nclaims are flagged as phishing indicators and serve as human-understandable\nexplanations.\n  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,\nPiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks\nlike Nazario and PhishPot. In a real-world evaluation of 10,183 emails across\nfive university accounts over three years, PiMRef achieved 92.1% precision,\n87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art\nin both effectiveness and efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPiMRef\uff0c\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u4e0d\u53d8\u91cf\u7684\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u53d1\u4ef6\u4eba\u8eab\u4efd\u7684\u771f\u5b9e\u6027\u548c\u68c0\u6d4b\u53ef\u7591\u884c\u4e3a\uff0c\u6709\u6548\u5bf9\u6297\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u9493\u9c7c\u90ae\u4ef6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u548c\u7279\u5f81\u5de5\u7a0b\u7684\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u653b\u51fb\u8005\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u9ad8\u8bf4\u670d\u529b\u9493\u9c7c\u90ae\u4ef6\uff0c\u4e9f\u9700\u65b0\u7684\u9632\u5fa1\u624b\u6bb5\u3002", "method": "PiMRef\u5c06\u9493\u9c7c\u68c0\u6d4b\u91cd\u6784\u4e3a\u8eab\u4efd\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\uff0c\u901a\u8fc7\u63d0\u53d6\u53d1\u4ef6\u4eba\u8eab\u4efd\u3001\u9a8c\u8bc1\u57df\u540d\u5408\u6cd5\u6027\u53ca\u68c0\u6d4b\u7528\u6237\u4e92\u52a8\u63d0\u793a\u6765\u8bc6\u522b\u9493\u9c7c\u90ae\u4ef6\u3002", "result": "PiMRef\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u7cbe\u5ea68.8%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u8fbe\u523092.1%\u7684\u7cbe\u5ea6\u548c87.9%\u7684\u53ec\u56de\u7387\uff0c\u8fd0\u884c\u6548\u7387\u9ad8\u3002", "conclusion": "PiMRef\u901a\u8fc7\u77e5\u8bc6\u57fa\u4e0d\u53d8\u91cf\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u9632\u5fa1\u9ad8\u7ea7\u9493\u9c7c\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.14749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14749", "abs": "https://arxiv.org/abs/2507.14749", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u673a\u5668\u5b66\u4e60\u5982\u4f55\u901a\u8fc7\u6a21\u62df\u513f\u7ae5\u8bed\u8a00\u8f93\u5165\u73af\u5883\u6765\u7406\u89e3\u8bed\u8a00\u4e60\u5f97\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u795e\u7ecf\u7f51\u7edc\u5728\u6709\u9650\u6570\u636e\u4e0b\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u662f\u5426\u80fd\u4ece\u7c7b\u4f3c\u513f\u7ae5\u7684\u8bed\u8a00\u8f93\u5165\u4e2d\u5b66\u4e60\uff0c\u4ee5\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u7684\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u8bed\u97f3\u8f6c\u5f55\u65b9\u6cd5\u5904\u7406SAYCam\u6570\u636e\u96c6\uff0c\u751f\u6210\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u914d\u7f6e\u3002", "result": "\u7f51\u7edc\u80fd\u4ece\u6bcf\u4e2a\u513f\u7ae5\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u6cdb\u5316\u8bcd\u6c47-\u6307\u4ee3\u6620\u5c04\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5b66\u4e60\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\u3002", "conclusion": "\u591a\u6a21\u6001\u795e\u7ecf\u7f51\u7edc\u5728\u6709\u9650\u6570\u636e\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u5b66\u4e60\u6a21\u5f0f\u53d7\u4e2a\u4f53\u8f93\u5165\u5dee\u5f02\u5f71\u54cd\u3002"}}
{"id": "2507.14575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\uff08FM\uff09\u6280\u672f\u5728T1w\u5230T2w MRI\u56fe\u50cf\u8f6c\u6362\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GAN-based Pix2Pix\u6a21\u578b\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u51cf\u5c11MRI\u626b\u63cf\u65f6\u95f4\u548c\u6210\u672c\uff0c\u901a\u8fc7\u8ba1\u7b97\u5408\u6210\u7f3a\u5931\u7684\u5bf9\u6bd4\u5ea6\u56fe\u50cf\u3002", "method": "\u4f7f\u7528GANs\u3001\u6269\u6563\u6a21\u578b\u548cFM\u6280\u672f\u8fdb\u884cT1w\u5230T2w\u76842D MRI\u56fe\u50cf\u8f6c\u6362\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "GAN-based Pix2Pix\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u6269\u6563\u548cFM\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "conclusion": "GANs\u66f4\u9002\u5408\u5f53\u524dMRI\u56fe\u50cf\u5408\u6210\u4efb\u52a1\uff0c\u672a\u6765\u7814\u7a76\u53ef\u63a2\u7d22\u66f4\u591a\u6570\u636e\u4ee5\u63d0\u5347FM\u6027\u80fd\u3002"}}
{"id": "2507.14484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14484", "abs": "https://arxiv.org/abs/2507.14484", "authors": ["Yule Li", "Yifeng Lu", "Zhen Wang", "Zhewei Wei", "Yaliang Li", "Bolin Ding"], "title": "ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions", "comment": null, "summary": "In recent years, graph neural networks (GNN) have achieved unprecedented\nsuccesses in node classification tasks. Although GNNs inherently encode\nspecific inductive biases (e.g., acting as low-pass or high-pass filters), most\nexisting methods implicitly assume conditional independence among node labels\nin their optimization objectives. While this assumption is suitable for\ntraditional classification tasks such as image recognition, it contradicts the\nintuitive observation that node labels in graphs remain correlated, even after\nconditioning on the graph structure. To make structured predictions for node\nlabels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for\nStructured node Classification. ReDiSC estimates the joint distribution of node\nlabels using a reparameterized masked diffusion model, which is learned through\nthe variational expectation-maximization (EM) framework. Our theoretical\nanalysis shows the efficiency advantage of ReDiSC in the E-step compared to\nDPM-SNC, a state-of-the-art model that relies on a manifold-constrained\ndiffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's\nM-step objective to popular GNN and label propagation hybrid approaches.\nExtensive experiments demonstrate that ReDiSC achieves superior or highly\ncompetitive performance compared to state-of-the-art GNN, label propagation,\nand diffusion-based baselines across both homophilic and heterophilic graphs of\nvarying sizes. Notably, ReDiSC scales effectively to large-scale datasets on\nwhich previous structured diffusion methods fail due to computational\nconstraints, highlighting its significant practical advantage in structured\nnode classification tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReDiSC\uff0c\u4e00\u79cd\u57fa\u4e8e\u91cd\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u56fe\u8282\u70b9\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u8282\u70b9\u6807\u7b7e\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u65b9\u6cd5\u5047\u8bbe\u8282\u70b9\u6807\u7b7e\u6761\u4ef6\u72ec\u7acb\uff0c\u4f46\u5b9e\u9645\u4e2d\u6807\u7b7e\u4ecd\u76f8\u5173\u3002ReDiSC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u9884\u6d4b\u3002", "method": "ReDiSC\u901a\u8fc7\u53d8\u5206\u671f\u671b\u6700\u5927\u5316\u6846\u67b6\u5b66\u4e60\u91cd\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u4f30\u8ba1\u8282\u70b9\u6807\u7b7e\u7684\u8054\u5408\u5206\u5e03\u3002", "result": "ReDiSC\u5728\u591a\u79cd\u56fe\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u9ad8\u6548\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "ReDiSC\u5728\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15419", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15419", "abs": "https://arxiv.org/abs/2507.15419", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "title": "PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation", "comment": "Accepted by EAI ICDF2C 2025", "summary": "Phishing websites remain a major cybersecurity threat, yet existing methods\nprimarily focus on detection, while the recognition of underlying malicious\nintentions remains largely unexplored. To address this gap, we propose\nPhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework\nthat uncovers phishing intentions from website screenshots. Leveraging the\nvisual-language capabilities of large language models (LLMs), our framework\nidentifies four key phishing objectives: Credential Theft, Financial Fraud,\nMalware Distribution, and Personal Information Harvesting. We construct and\nrelease the first phishing intention ground truth dataset (~2K samples) and\nevaluate the framework using four commercial LLMs. Experimental results show\nthat PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and\nsignificantly outperforms the single-agent baseline with a ~95% improvement in\nmicro-precision. Compared to the previous work, it achieves 0.8545 precision\nfor credential theft, marking a ~4% improvement. Additionally, we generate a\nlarger dataset of ~9K samples for large-scale phishing intention profiling\nacross sectors. This work provides a scalable and interpretable solution for\nintention-aware phishing analysis.", "AI": {"tldr": "\u63d0\u51faPhishIntentionLLM\u6846\u67b6\uff0c\u5229\u7528\u591a\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u4ece\u7f51\u7ad9\u622a\u56fe\u8bc6\u522b\u9493\u9c7c\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9493\u9c7c\u7f51\u7ad9\u68c0\u6d4b\uff0c\u800c\u5bf9\u6076\u610f\u610f\u56fe\u7684\u8bc6\u522b\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u89c6\u89c9-\u8bed\u8a00\u80fd\u529b\uff0c\u8bbe\u8ba1\u591a\u4ee3\u7406RAG\u6846\u67b6\uff0c\u8bc6\u522b\u56db\u79cd\u9493\u9c7c\u76ee\u6807\u3002", "result": "\u5728GPT-4o\u4e0a\u8fbe\u52300.7895\u7684\u5fae\u7cbe\u5ea6\uff0c\u6bd4\u5355\u4ee3\u7406\u57fa\u7ebf\u63d0\u5347\u7ea695%\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u9493\u9c7c\u610f\u56fe\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14758", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14758", "abs": "https://arxiv.org/abs/2507.14758", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "AI": {"tldr": "GRACE\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165Chain-of-Thought\uff08CoT\uff09\u6807\u8bb0\u5316\u548cJourney-Aware\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u591a\u884c\u4e3a\u63a8\u8350\u4e2d\u7684\u6548\u7387\u548c\u4fe1\u606f\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u6807\u8bb0\u63a8\u7406\u4fe1\u606f\u4e0d\u8db3\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u591a\u5c3a\u5ea6\u5efa\u6a21\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "GRACE\u91c7\u7528\u6df7\u5408Chain-of-Thought\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ea7\u54c1\u77e5\u8bc6\u56fe\u8c31\u5c5e\u6027\uff0c\u5e76\u8bbe\u8ba1Journey-Aware\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cGRACE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0cHR@10\u548cNDCG@10\u63d0\u5347\u6700\u9ad8\u8fbe106.9%\u548c106.7%\uff0c\u540c\u65f6\u51cf\u5c1148%\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "conclusion": "GRACE\u901a\u8fc7\u521b\u65b0\u7684\u6807\u8bb0\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u591a\u884c\u4e3a\u63a8\u8350\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u751f\u6210\u5f0f\u63a8\u8350\u3002"}}
{"id": "2507.14587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Be\u0107irovi\u0107", "Amina Kurtovi\u0107", "Nordin Smajlovi\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86TensorFlow\u3001PyTorch\u548cJAX\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u8840\u6db2\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u63a8\u7406\u65f6\u95f4\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u8840\u6db2\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u6846\u67b6\u6027\u80fd\u7684\u8be6\u7ec6\u5206\u6790\u3002", "method": "\u4f7f\u7528BloodMNIST\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e09\u79cd\u6846\u67b6\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u63a8\u7406\u65f6\u95f4\u548c\u56fe\u50cf\u5927\u5c0f\u7684\u5f71\u54cd\u3002", "result": "JAX\u548cPyTorch\u7684\u5206\u7c7b\u51c6\u786e\u6027\u63a5\u8fd1\u5f53\u524d\u57fa\u51c6\uff0c\u6027\u80fd\u53d7\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u6846\u67b6\u4f18\u5316\u5f71\u54cd\u3002", "conclusion": "JAX\u548cPyTorch\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u9ad8\u6548\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14487", "abs": "https://arxiv.org/abs/2507.14487", "authors": ["Ukjo Hwang", "Songnam Hong"], "title": "Federated Reinforcement Learning in Heterogeneous Environments", "comment": null, "summary": "We investigate a Federated Reinforcement Learning with Environment\nHeterogeneity (FRL-EH) framework, where local environments exhibit statistical\nheterogeneity. Within this framework, agents collaboratively learn a global\npolicy by aggregating their collective experiences while preserving the privacy\nof their local trajectories. To better reflect real-world scenarios, we\nintroduce a robust FRL-EH framework by presenting a novel global objective\nfunction. This function is specifically designed to optimize a global policy\nthat ensures robust performance across heterogeneous local environments and\ntheir plausible perturbations. We propose a tabular FRL algorithm named FedRQ\nand theoretically prove its asymptotic convergence to an optimal policy for the\nglobal objective function. Furthermore, we extend FedRQ to environments with\ncontinuous state space through the use of expectile loss, addressing the key\nchallenge of minimizing a value function over a continuous subset of the state\nspace. This advancement facilitates the seamless integration of the principles\nof FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive\nempirical evaluations validate the effectiveness and robustness of our FRL\nalgorithms across diverse heterogeneous environments, consistently achieving\nsuperior performance over the existing state-of-the-art FRL algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6FRL-EH\uff0c\u89e3\u51b3\u5c40\u90e8\u73af\u5883\u7edf\u8ba1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u5408\u7ecf\u9a8c\u5b66\u4e60\u5168\u5c40\u7b56\u7565\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5c40\u90e8\u73af\u5883\u5b58\u5728\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u9700\u8bbe\u8ba1\u9c81\u68d2\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4ee5\u4f18\u5316\u5168\u5c40\u7b56\u7565\u3002", "method": "\u63d0\u51faFedRQ\u7b97\u6cd5\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6e10\u8fd1\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u671f\u671b\u635f\u5931\u6269\u5c55\u81f3\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1FedRQ\u5728\u5f02\u8d28\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FRL-EH\u6846\u67b6\u53caFedRQ\u7b97\u6cd5\u5728\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5f02\u8d28\u73af\u5883\u3002"}}
{"id": "2507.15449", "categories": ["cs.CR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.15449", "abs": "https://arxiv.org/abs/2507.15449", "authors": ["Alessio Caminata", "Elisa Gorla", "Madison Mabe", "Martina Vigorito", "Irene Villa"], "title": "Cryptanalysis of a multivariate CCZ scheme", "comment": "are welcome!", "summary": "We consider the multivariate scheme Pesto, which was introduced by Calderini,\nCaminata, and Villa. In this scheme, the public polynomials are obtained by\napplying a CCZ transformation to a set of quadratic secret polynomials. As a\nconsequence, the public key consists of polynomials of degree 4. In this work,\nwe show that the public degree 4 polynomial system can be efficiently reduced\nto a system of quadratic polynomials. This seems to suggest that the CCZ\ntransformation may not offer a significant increase in security, contrary to\nwhat was initially believed.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Pesto\u65b9\u6848\uff0c\u53d1\u73b0\u5176CCZ\u53d8\u6362\u540e\u76844\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\u53ef\u9ad8\u6548\u964d\u4e3a2\u6b21\uff0c\u8d28\u7591\u5176\u5b89\u5168\u6027\u63d0\u5347\u3002", "motivation": "\u63a2\u8ba8Pesto\u65b9\u6848\u4e2dCCZ\u53d8\u6362\u662f\u5426\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u5c06CCZ\u53d8\u6362\u540e\u76844\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\u964d\u4e3a2\u6b21\u7cfb\u7edf\u3002", "result": "\u53d1\u73b0\u964d\u9636\u6548\u7387\u9ad8\uff0cCCZ\u53d8\u6362\u53ef\u80fd\u672a\u663e\u8457\u589e\u5f3a\u5b89\u5168\u6027\u3002", "conclusion": "CCZ\u53d8\u6362\u5728Pesto\u65b9\u6848\u4e2d\u7684\u5b89\u5168\u6027\u63d0\u5347\u6548\u679c\u5b58\u7591\u3002"}}
{"id": "2507.14815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14815", "abs": "https://arxiv.org/abs/2507.14815", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.", "AI": {"tldr": "FastLongSpeech\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u878d\u5408\u548c\u52a8\u6001\u538b\u7f29\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86LSLMs\u5904\u7406\u957f\u8bed\u97f3\u7684\u6311\u6218\uff0c\u65e0\u9700\u4e13\u7528\u957f\u8bed\u97f3\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5728LongSpeech-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709LSLMs\u5728\u957f\u8bed\u97f3\u5904\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u957f\u8bed\u97f3\u8bad\u7ec3\u6570\u636e\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5f15\u5165FastLongSpeech\u6846\u67b6\uff0c\u91c7\u7528\u8fed\u4ee3\u878d\u5408\u7b56\u7565\u538b\u7f29\u957f\u8bed\u97f3\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u538b\u7f29\u8bad\u7ec3\u9002\u5e94LSLMs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastLongSpeech\u5728\u957f\u8bed\u97f3\u548c\u77ed\u8bed\u97f3\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "conclusion": "FastLongSpeech\u4e3aLSLMs\u9ad8\u6548\u5904\u7406\u957f\u8bed\u97f3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D\u662f\u4e00\u79cd\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f00\u653e\u8bcd\u6c47\u5f15\u5bfc\u76843D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728\u573a\u666f\u548c\u7528\u6237\u67e5\u8be2\u9002\u5e94\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u6216\u573a\u666f\u5185\u5bb9\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u573a\u666f\u548c\u7528\u6237\u67e5\u8be2\u7684\u9700\u6c42\u3002DiSCO-3D\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u573a\u8868\u793a\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f31\u5f00\u653e\u8bcd\u6c47\u5f15\u5bfc\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u76d1\u7763\u5206\u5272\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DiSCO-3D\u4e3a3D\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.14492", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14492", "abs": "https://arxiv.org/abs/2507.14492", "authors": ["Satyankar Chandra", "Ashutosh Gupta", "Kaushik Mallik", "Krishna Shankaranarayanan", "Namrita Varshney"], "title": "Glitches in Decision Tree Ensemble Models", "comment": null, "summary": "Many critical decision-making tasks are now delegated to machine-learned\nmodels, and it is imperative that their decisions are trustworthy and reliable,\nand their outputs are consistent across similar inputs. We identify a new\nsource of unreliable behaviors-called glitches-which may significantly impair\nthe reliability of AI models having steep decision boundaries. Roughly\nspeaking, glitches are small neighborhoods in the input space where the model's\noutput abruptly oscillates with respect to small changes in the input. We\nprovide a formal definition of glitches, and use well-known models and datasets\nfrom the literature to demonstrate that they have widespread existence and\nargue they usually indicate potential model inconsistencies in the neighborhood\nof where they are found. We proceed to the algorithmic search of glitches for\nwidely used gradient-boosted decision tree (GBDT) models. We prove that the\nproblem of detecting glitches is NP-complete for tree ensembles, already for\ntrees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP\nencoding of the problem, and its effectiveness and computational feasibility\nare demonstrated on a set of widely used GBDT benchmarks taken from the\nliterature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u53ef\u9760\u884c\u4e3a\u6765\u6e90\u2014\u2014glitches\uff08\u6545\u969c\u70b9\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBDT\uff09\u6a21\u578b\u4e2d\u7684\u5e7f\u6cdb\u5b58\u5728\u3002\u901a\u8fc7\u5f62\u5f0f\u5316\u5b9a\u4e49\u548c\u7b97\u6cd5\u641c\u7d22\uff0c\u5c55\u793a\u4e86\u68c0\u6d4bglitches\u7684NP\u5b8c\u5168\u6027\u53ca\u5176\u5bf9\u6a21\u578b\u4e00\u81f4\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5173\u952e\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u8f93\u51fa\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u8bc6\u522b\u5e76\u89e3\u51b3\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\u9661\u5ced\u65f6\u53ef\u80fd\u51fa\u73b0\u7684glitches\u95ee\u9898\u3002", "method": "\u8bba\u6587\u9996\u5148\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86glitches\uff0c\u5e76\u901a\u8fc7\u6587\u732e\u4e2d\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u666e\u904d\u5b58\u5728\u3002\u63a5\u7740\uff0c\u9488\u5bf9GBDT\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u7684glitch\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0c\u5bf9\u4e8e\u6df1\u5ea6\u4e3a4\u7684\u6811\u96c6\u6210\u6a21\u578b\uff0c\u68c0\u6d4bglitches\u662fNP\u5b8c\u5168\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cglitch\u641c\u7d22\u7b97\u6cd5\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684GBDT\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5177\u6709\u6709\u6548\u6027\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "glitches\u662f\u5f71\u54cdAI\u6a21\u578b\u53ef\u9760\u6027\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u68c0\u6d4b\u548c\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.15613", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15613", "abs": "https://arxiv.org/abs/2507.15613", "authors": ["Andrii Balashov", "Olena Ponomarova", "Xiaohua Zhai"], "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems", "comment": "26 pages", "summary": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f01\u4e1a\u73af\u5883\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9762\u4e34\u7684\u591a\u9636\u6bb5\u63d0\u793a\u63a8\u7406\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u6a21\u578b\u548c\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u4f01\u4e1a\u90e8\u7f72\u7684LLM\u9762\u4e34\u65b0\u578b\u5b89\u5168\u5a01\u80c1\uff0c\u5982\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u63a8\u7406\u653b\u51fb\u9010\u6b65\u63d0\u53d6\u673a\u5bc6\u6570\u636e\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u653b\u51fb\u573a\u666f\u3001\u6982\u7387\u7406\u8bba\u3001\u4f18\u5316\u6846\u67b6\u548c\u4fe1\u606f\u6cc4\u6f0f\u8fb9\u754c\u5206\u6790\u591a\u9636\u6bb5\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u9632\u5fa1\u63aa\u65bd\u3002", "result": "\u653b\u51fb\u80fd\u53ef\u9760\u5730\u63d0\u53d6\u654f\u611f\u4fe1\u606f\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u5f02\u5e38\u68c0\u6d4b\u3001\u8bbf\u95ee\u63a7\u5236\u7b49\uff09\u6709\u6548\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u4f01\u4e1aLLM\u5b89\u5168\u9700\u4ece\u5355\u8f6e\u63d0\u793a\u8fc7\u6ee4\u8f6c\u5411\u591a\u9636\u6bb5\u653b\u51fb\u4e0e\u9632\u5fa1\u7684\u6574\u4f53\u89c6\u89d2\u3002"}}
{"id": "2507.14819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14819", "abs": "https://arxiv.org/abs/2507.14819", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u4ece\u957f\u6587\u6863\u751f\u6210\u56fe\u8868\u7684\u65e0\u76d1\u7763\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u51c6\u786e\u6027\u548c\u56fe\u8868\u7c7b\u578b\u5339\u914d\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u4ece\u957f\u6587\u6863\u6839\u636e\u7528\u6237\u610f\u56fe\u751f\u6210\u56fe\u8868\uff0c\u9700\u8981\u7528\u6237\u9884\u5148\u624b\u52a8\u9009\u62e9\u76f8\u5173\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1aLLM\u5206\u89e3\u610f\u56fe\u5e76\u63d0\u53d6\u6570\u636e\uff0c\u542f\u53d1\u5f0f\u6a21\u5757\u9009\u62e9\u56fe\u8868\u7c7b\u578b\u540e\u751f\u6210\u4ee3\u7801\u3002", "result": "\u5728\u91d1\u878d\u548c\u79d1\u5b66\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u6570\u636e\u51c6\u786e\u6027\u548c\u56fe\u8868\u7c7b\u578b\u5339\u914d\u5ea6\u4e0a\u5206\u522b\u6bd4\u57fa\u7ebf\u9ad89\u548c17\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u957f\u6587\u6863\u57fa\u4e8e\u610f\u56fe\u751f\u6210\u56fe\u8868\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u65b0\u6307\u6807\u9a8c\u8bc1\u4e86\u6570\u636e\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14608", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "Exp-Graph\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u5efa\u6a21\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9Transformer\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u90e8\u5c5e\u6027\u7684\u7ed3\u6784\u53d8\u5316\u9700\u8981\u88ab\u6709\u6548\u6355\u6349\u4ee5\u63d0\u9ad8\u8bc6\u522b\u6548\u679c\u3002", "method": "\u4f7f\u7528\u9762\u90e8\u5173\u952e\u70b9\u4f5c\u4e3a\u56fe\u7684\u9876\u70b9\uff0c\u57fa\u4e8e\u90bb\u8fd1\u6027\u548c\u5c40\u90e8\u5916\u89c2\u76f8\u4f3c\u6027\u786e\u5b9a\u8fb9\uff0c\u7ed3\u5408\u89c6\u89c9Transformer\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u6355\u6349\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728Oulu-CASIA\u3001eNTERFACE05\u548cAFEW\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.09%\u300179.01%\u548c56.39%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Exp-Graph\u5728\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14503", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14503", "abs": "https://arxiv.org/abs/2507.14503", "authors": ["Jiequan Cui", "Beier Zhu", "Qingshan Xu", "Xiaogang Xu", "Pengguang Chen", "Xiaojuan Qi", "Bei Yu", "Hanwang Zhang", "Richang Hong"], "title": "Generative Distribution Distillation", "comment": "Technique report", "summary": "In this paper, we formulate the knowledge distillation (KD) as a conditional\ngenerative problem and propose the \\textit{Generative Distribution Distillation\n(GenDD)} framework. A naive \\textit{GenDD} baseline encounters two major\nchallenges: the curse of high-dimensional optimization and the lack of semantic\nsupervision from labels. To address these issues, we introduce a \\textit{Split\nTokenization} strategy, achieving stable and effective unsupervised KD.\nAdditionally, we develop the \\textit{Distribution Contraction} technique to\nintegrate label supervision into the reconstruction objective. Our theoretical\nproof demonstrates that \\textit{GenDD} with \\textit{Distribution Contraction}\nserves as a gradient-level surrogate for multi-task learning, realizing\nefficient supervised training without explicit classification loss on\nmulti-step sampling image representations. To evaluate the effectiveness of our\nmethod, we conduct experiments on balanced, imbalanced, and unlabeled data.\nExperimental results show that \\textit{GenDD} performs competitively in the\nunsupervised setting, significantly surpassing KL baseline by \\textbf{16.29\\%}\non ImageNet validation set. With label supervision, our ResNet-50 achieves\n\\textbf{82.28\\%} top-1 accuracy on ImageNet in 600 epochs training,\nestablishing a new state-of-the-art.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u95ee\u9898\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6GenDD\uff0c\u901a\u8fc7Split Tokenization\u548cDistribution Contraction\u6280\u672f\u89e3\u51b3\u4e86\u9ad8\u7ef4\u4f18\u5316\u548c\u6807\u7b7e\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u65e0\u76d1\u7763\u548c\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u901a\u5e38\u9762\u4e34\u9ad8\u7ef4\u4f18\u5316\u548c\u6807\u7b7e\u76d1\u7763\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u6539\u8fdbKD\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faGenDD\u6846\u67b6\uff0c\u7ed3\u5408Split Tokenization\u5b9e\u73b0\u7a33\u5b9a\u7684\u65e0\u76d1\u7763KD\uff0c\u5e76\u901a\u8fc7Distribution Contraction\u6280\u672f\u5f15\u5165\u6807\u7b7e\u76d1\u7763\u3002", "result": "\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cGenDD\u6bd4KL\u57fa\u7ebf\u63d0\u534716.29%\uff1b\u5728\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cResNet-50\u5728ImageNet\u4e0a\u8fbe\u523082.28%\u7684top-1\u51c6\u786e\u7387\u3002", "conclusion": "GenDD\u6846\u67b6\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65e0\u76d1\u7763\u548c\u76d1\u7763\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2507.15660", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.15660", "abs": "https://arxiv.org/abs/2507.15660", "authors": ["Rohit Negi", "Amit Negi", "Manish Sharma", "S. Venkatesan", "Prem Kumar", "Sandeep K. Shukla"], "title": "Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls", "comment": "11 pages, 11 tables", "summary": "Mega events such as the Olympics, World Cup tournaments, G-20 Summit,\nreligious events such as MahaKumbh are increasingly digitalized. From event\nticketing, vendor booth or lodging reservations, sanitation, event scheduling,\ncustomer service, crime reporting, media streaming and messaging on digital\ndisplay boards, surveillance, crowd control, traffic control and many other\nservices are based on mobile and web applications, wired and wireless\nnetworking, network of Closed-Circuit Television (CCTV) cameras, specialized\ncontrol room with network and video-feed monitoring. Consequently, cyber\nthreats directed at such digital infrastructure are common. Starting from hobby\nhackers, hacktivists, cyber crime gangs, to the nation state actors, all target\nsuch infrastructure to unleash chaos on an otherwise smooth operation, and\noften the cyber threat actors attempt to embarrass the organizing country or\nthe organizers. Unlike long-standing organizations such as a corporate or a\ngovernment department, the infrastructure of mega-events is temporary,\nconstructed over a short time span in expediency, and often shortcuts are taken\nto make the deadline for the event. As a result, securing such an elaborate yet\ntemporary infrastructure requires a different approach than securing a standard\norganizational digital infrastructure. In this paper, we describe our approach\nto securing MahaKumbh 2025, a 600 million footfall event for 45 days in\nPrayagraj, India, as a cyber security assessment and risk management oversight\nteam. We chronicle the scope, process, methodology, and outcome of our team's\neffort to secure this mega event. It should be noted that none of the cyber\nattacks during the 45-day event was successful. Our goal is to put on record\nthe methodology and discuss what we would do differently in case we work on\nsimilar future mega event.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u6d3b\u52a8\uff08\u5982\u5965\u8fd0\u4f1a\u3001\u4e16\u754c\u676f\u7b49\uff09\u6570\u5b57\u5316\u57fa\u7840\u8bbe\u65bd\u9762\u4e34\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u5e76\u4ee5MahaKumbh 2025\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u4e86\u5176\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u548c\u98ce\u9669\u7ba1\u7406\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u6d3b\u52a8\u7684\u6570\u5b57\u5316\u57fa\u7840\u8bbe\u65bd\u901a\u5e38\u662f\u4e34\u65f6\u6784\u5efa\u7684\uff0c\u5bb9\u6613\u6210\u4e3a\u9ed1\u5ba2\u653b\u51fb\u76ee\u6807\uff0c\u56e0\u6b64\u9700\u8981\u72ec\u7279\u7684\u7f51\u7edc\u5b89\u5168\u65b9\u6cd5\u3002", "method": "\u4f5c\u4e3a\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u548c\u98ce\u9669\u7ba1\u7406\u56e2\u961f\uff0c\u4f5c\u8005\u8be6\u7ec6\u63cf\u8ff0\u4e86MahaKumbh 2025\u7684\u5b89\u5168\u8bc4\u4f30\u8303\u56f4\u3001\u6d41\u7a0b\u548c\u65b9\u6cd5\u3002", "result": "\u572845\u5929\u7684\u6d3b\u52a8\u4e2d\uff0c\u6240\u6709\u7f51\u7edc\u653b\u51fb\u5747\u672a\u6210\u529f\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u65b9\u6cd5\u8bba\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7c7b\u4f3c\u5927\u578b\u6d3b\u52a8\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.14849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14849", "abs": "https://arxiv.org/abs/2507.14849", "authors": ["Yifei Wang"], "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.", "AI": {"tldr": "\u63a8\u7406\u84b8\u998f\u80fd\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bf9\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u7406\u89e3\u7684\u5f71\u54cd\u5c1a\u672a\u7814\u7a76\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u84b8\u998f\u663e\u8457\u6539\u5584\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u89e3\u51b3\u201c\u4e2d\u95f4\u8ff7\u5931\u201d\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5927\u89c4\u6a21\u63a8\u7406\u84b8\u998f\u5bf9\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u7406\u89e3\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528\u4eceDeepseek-R1\u84b8\u998f\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u53d6\u548c\u6574\u5408\u80fd\u529b\u3002", "result": "\u84b8\u998f\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4fc3\u8fdb\u66f4\u8be6\u7ec6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u7f13\u89e3\u201c\u4e2d\u95f4\u8ff7\u5931\u201d\u95ee\u9898\u3002", "conclusion": "\u63a8\u7406\u84b8\u998f\u4e0d\u4ec5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u80fd\u6709\u6548\u6539\u5584\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2507.14613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "DD-SAM2\u662f\u4e00\u4e2a\u9ad8\u6548\u9002\u914dSAM2\u7684\u6846\u67b6\uff0c\u901a\u8fc7Depthwise-Dilated Adapter\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u5206\u5272\u4e0e\u8ddf\u8e2a\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u591a\u4e3a\u6a21\u6001\u7279\u5b9a\u8bbe\u8ba1\uff0c\u9002\u5e94\u6027\u5dee\uff0c\u4e14SAM2\u7b49\u6a21\u578b\u5728\u533b\u5b66\u89c6\u9891\u573a\u666f\u4e2d\u9700\u5927\u89c4\u6a21\u6570\u636e\u96c6\u91cd\u65b0\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faDD-SAM2\u6846\u67b6\uff0c\u7ed3\u5408Depthwise-Dilated Adapter\uff0c\u4ee5\u6700\u5c0f\u53c2\u6570\u91cf\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u5c0f\u6570\u636e\u5fae\u8c03\u3002", "result": "\u5728TrackRad2025\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230Dice\u5206\u65700.93\u548c0.97\u3002", "conclusion": "DD-SAM2\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u57fa\u4e8e\u9002\u914d\u5668\u7684SAM2\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e3a\u533b\u5b66\u89c6\u9891\u5206\u5272\u4e0e\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14516", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14516", "abs": "https://arxiv.org/abs/2507.14516", "authors": ["Jeyoung Lee", "Hochul Kang"], "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning", "comment": null, "summary": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u5ea6\u91cf\u51fd\u6570SDSC\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ddd\u79bb\u76ee\u6807\uff08\u5982MSE\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u5982MSE\uff09\u5bf9\u5e45\u5ea6\u654f\u611f\u3001\u5bf9\u6ce2\u5f62\u6781\u6027\u4e0d\u53d8\u4e14\u5c3a\u5ea6\u65e0\u754c\uff0c\u963b\u788d\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "SDSC\u57fa\u4e8eDice\u76f8\u4f3c\u7cfb\u6570\uff0c\u91cf\u5316\u65f6\u95f4\u4fe1\u53f7\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u5206\u8fd1\u4f3c\u5b9e\u73b0\u68af\u5ea6\u4f18\u5316\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SDSC\u4e0eMSE\u7684\u6df7\u5408\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDSC\u5728\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0eMSE\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u9886\u57df\u5185\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5ea6\u91cf\uff08\u5982SDSC\uff09\u80fd\u63d0\u5347\u4fe1\u53f7\u8868\u793a\u7684\u8bed\u4e49\u8d28\u91cf\uff0c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u8ddd\u79bb\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.14871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14871", "abs": "https://arxiv.org/abs/2507.14871", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "title": "Tiny language models", "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5c0f\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\uff08TLMs\uff09\uff0c\u9884\u8bad\u7ec3\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u6027\u80fd\u5dee\u8ddd\u968f\u9884\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u4efb\u52a1\u76f8\u5173\u6027\u589e\u52a0\u3002", "motivation": "\u7531\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9884\u8bad\u7ec3\u9700\u8981\u5de8\u5927\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u7814\u7a76\u53c2\u4e0e\uff0c\u56e0\u6b64\u63a2\u7d22\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08TLMs\uff09\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u80fd\u529b\u6210\u4e3a\u5173\u952e\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3BERT-6\u53ca\u5176\u53d8\u4f53\uff08\u5982BERT-1\uff09\u5728Wikipedia\u5b50\u96c6\u4e0a\uff0c\u5e76\u5728FewRel\u3001AGNews\u548cDBPedia\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u9884\u8bad\u7ec3\u7684TLMs\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u672a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e14\u6027\u80fd\u5dee\u8ddd\u968f\u9884\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u4efb\u52a1\u76f8\u5173\u6027\u589e\u52a0\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8f6f\u59d4\u5458\u4f1a\u673a\u5236\uff0c\u591a\u4e2a\u6d45\u5c42\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u590d\u73b0\u6df1\u5c42\u6a21\u578b\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "TLMs\u9884\u8bad\u7ec3\u5728\u5c0f\u89c4\u6a21\u4e0b\u4ecd\u6709\u6548\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u673a\u5236\uff0c\u5c24\u5176\u662f\u5728\u751f\u7269\u542f\u53d1\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.14632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "BusterX++\u662f\u4e00\u4e2a\u65b0\u578b\u8de8\u6a21\u6001\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u89e3\u91ca\u5408\u6210\u5a92\u4f53\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u8fdb\u6b65\u589e\u52a0\u4e86\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u73b0\u6709\u5355\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u591a\u6a21\u6001\u5408\u6210\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u601d\u7ef4\u5956\u52b1\u548c\u6df7\u5408\u63a8\u7406\u3002", "result": "BusterX++\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u901a\u8fc7GenBuster++\u57fa\u51c6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "BusterX++\u4e3a\u89e3\u51b3\u8de8\u6a21\u6001\u5408\u6210\u5a92\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u73b0\u5b9e\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14528", "abs": "https://arxiv.org/abs/2507.14528", "authors": ["Ilias Tsoumas", "Dimitrios Bormpoudakis", "Vasileios Sitokonstantinou", "Athanasios Askitopoulos", "Andreas Kalogeras", "Charalampos Kontoes", "Ioannis Athanasiadis"], "title": "Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference", "comment": "Accepted at KDD 2025 Workshop on Causal Inference and Machine\n  Learning in Practice", "summary": "In causal inference, whether through randomized controlled trials or\nobservational studies, access to both treated and control units is essential\nfor estimating the effect of a treatment on an outcome of interest. When\ntreatment assignment is random, the average treatment effect (ATE) can be\nestimated directly by comparing outcomes between groups. In non-randomized\nsettings, various techniques are employed to adjust for confounding and\napproximate the counterfactual scenario to recover an unbiased ATE. A common\nchallenge, especially in observational studies, is the absence of units clearly\nlabeled as controls-that is, units known not to have received the treatment. To\naddress this, we propose positive-unlabeled (PU) learning as a framework for\nidentifying, with high confidence, control units from a pool of unlabeled ones,\nusing only the available treated (positive) units. We evaluate this approach\nusing both simulated and real-world data. We construct a causal graph with\ndiverse relationships and use it to generate synthetic data under various\nscenarios, assessing how reliably the method recovers control groups that allow\nestimates of true ATE. We also apply our approach to real-world data on optimal\nsowing and fertilizer treatments in sustainable agriculture. Our findings show\nthat PU learning can successfully identify control (negative) units from\nunlabeled data based only on treated units and, through the resulting control\ngroup, estimate an ATE that closely approximates the true value. This work has\nimportant implications for observational causal inference, especially in fields\nwhere randomized experiments are difficult or costly. In domains such as earth,\nenvironmental, and agricultural sciences, it enables a plethora of\nquasi-experiments by leveraging available earth observation and climate data,\nparticularly when treated units are available but control units are lacking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u672a\u6807\u8bb0\uff08PU\uff09\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\u8bc6\u522b\u63a7\u5236\u7ec4\uff0c\u4ece\u800c\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u3002", "motivation": "\u5728\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\uff0c\u7f3a\u4e4f\u660e\u786e\u6807\u8bb0\u7684\u63a7\u5236\u7ec4\u662f\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u4f30\u8ba1\u65e0\u504f\u7684ATE\u3002", "method": "\u91c7\u7528PU\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u57fa\u4e8e\u5df2\u5904\u7406\u7684\uff08\u6b63\uff09\u5355\u5143\uff0c\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u8bc6\u522b\u63a7\u5236\u5355\u5143\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u8bc6\u522b\u63a7\u5236\u7ec4\uff0c\u5e76\u4f30\u8ba1\u51fa\u63a5\u8fd1\u771f\u5b9e\u503c\u7684ATE\u3002", "conclusion": "PU\u5b66\u4e60\u4e3a\u89c2\u5bdf\u6027\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u96be\u4ee5\u8fdb\u884c\u968f\u673a\u5b9e\u9a8c\u7684\u9886\u57df\u3002"}}
{"id": "2507.14887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14887", "abs": "https://arxiv.org/abs/2507.14887", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.", "AI": {"tldr": "MEKiT\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u591a\u6e90\u5f02\u6784\u77e5\u8bc6\uff08\u5185\u90e8\u60c5\u611f\u77e5\u8bc6\u548c\u5916\u90e8\u56e0\u679c\u77e5\u8bc6\uff09\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f-\u539f\u56e0\u5bf9\u63d0\u53d6\u4efb\u52a1\uff08ECPE\uff09\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u60c5\u611f-\u539f\u56e0\u5bf9\u63d0\u53d6\u4efb\u52a1\uff08ECPE\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u8f85\u52a9\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u60c5\u611f\u611f\u77e5\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMEKiT\u65b9\u6cd5\uff0c\u7ed3\u5408\u6307\u4ee4\u6a21\u677f\u548c\u6570\u636e\u6df7\u5408\u7684\u6307\u4ee4\u8c03\u4f18\uff0c\u5206\u522b\u4f18\u5316\u60c5\u611f\u8bc6\u522b\u548c\u56e0\u679c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMEKiT\u5728ECPE\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u6027\u80fd\u3002", "conclusion": "MEKiT\u4e3aECPE\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u591a\u5149\u8c31\u7279\u5f81\u878d\u5408\u6846\u67b6MS2Fusion\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u4e92\u8865\u7279\u5f81\u504f\u597d\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u5149\u8c31\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u5c40\u90e8\u4e92\u8865\u7279\u5f81\u800c\u5ffd\u89c6\u8de8\u6a21\u6001\u5171\u4eab\u8bed\u4e49\uff0c\u4e14\u5b58\u5728\u611f\u53d7\u91ce\u5927\u5c0f\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "MS2Fusion\u91c7\u7528\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\uff1a\u4e00\u6761\u8def\u5f84\u901a\u8fc7\u8de8\u6a21\u6001\u9690\u85cf\u72b6\u6001\u89e3\u7801\u6316\u6398\u4e92\u8865\u4fe1\u606f\uff0c\u53e6\u4e00\u6761\u8def\u5f84\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u63a2\u7d22\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u4e24\u8005\u5728SSM\u4e2d\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728FLIR\u3001M3FD\u548cLLVIP\u7b49\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMS2Fusion\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5728RGB-T\u8bed\u4e49\u5206\u5272\u548cRGBT\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "MS2Fusion\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u4e86\u529f\u80fd\u4e92\u8865\u548c\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.14529", "categories": ["cs.LG", "math.OC", "91A16, 68T05, 49N45, 93E20, 46E22"], "pdf": "https://arxiv.org/pdf/2507.14529", "abs": "https://arxiv.org/abs/2507.14529", "authors": ["Berkay Anahtarci", "Can Deha Kariksiz", "Naci Saldi"], "title": "Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games", "comment": null, "summary": "We consider the maximum causal entropy inverse reinforcement learning problem\nfor infinite-horizon stationary mean-field games, in which we model the unknown\nreward function within a reproducing kernel Hilbert space. This allows the\ninference of rich and potentially nonlinear reward structures directly from\nexpert demonstrations, in contrast to most existing inverse reinforcement\nlearning approaches for mean-field games that typically restrict the reward\nfunction to a linear combination of a fixed finite set of basis functions. We\nalso focus on the infinite-horizon cost structure, whereas prior studies\nprimarily rely on finite-horizon formulations. We introduce a Lagrangian\nrelaxation to this maximum causal entropy inverse reinforcement learning\nproblem that enables us to reformulate it as an unconstrained log-likelihood\nmaximization problem, and obtain a solution \\lk{via} a gradient ascent\nalgorithm. To illustrate the theoretical consistency of the algorithm, we\nestablish the smoothness of the log-likelihood objective by proving the\nFr\\'echet differentiability of the related soft Bellman operators with respect\nto the parameters in the reproducing kernel Hilbert space. We demonstrate the\neffectiveness of our method on a mean-field traffic routing game, where it\naccurately recovers expert behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u56e0\u679c\u71b5\u7684\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u9650\u65f6\u57df\u5e73\u7a33\u5747\u503c\u573a\u535a\u5f08\uff0c\u901a\u8fc7\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5efa\u6a21\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5956\u52b1\u51fd\u6570\u7ebf\u6027\u7ec4\u5408\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5747\u503c\u573a\u535a\u5f08\u4e2d\u901a\u5e38\u9650\u5236\u5956\u52b1\u51fd\u6570\u4e3a\u56fa\u5b9a\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u4e14\u591a\u57fa\u4e8e\u6709\u9650\u65f6\u57df\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63a8\u65ad\u66f4\u4e30\u5bcc\u7684\u975e\u7ebf\u6027\u5956\u52b1\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u6cd5\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u5bf9\u6570\u4f3c\u7136\u6700\u5927\u5316\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u6c42\u89e3\u3002\u8bc1\u660e\u4e86\u8f6f\u8d1d\u5c14\u66fc\u7b97\u5b50\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u53c2\u6570\u4e0b\u7684Fr'echet\u53ef\u5fae\u6027\u3002", "result": "\u5728\u5747\u503c\u573a\u4ea4\u901a\u8def\u7531\u535a\u5f08\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u6062\u590d\u4e13\u5bb6\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u65e0\u9650\u65f6\u57df\u5747\u503c\u573a\u535a\u5f08\u4e2d\u6210\u529f\u63a8\u65ad\u975e\u7ebf\u6027\u5956\u52b1\u51fd\u6570\uff0c\u6269\u5c55\u4e86\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.15112", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15112", "abs": "https://arxiv.org/abs/2507.15112", "authors": ["Youssef Allouah", "Rachid Guerraoui", "Sanmi Koyejo"], "title": "Distributional Unlearning: Forgetting Distributions, Not Just Samples", "comment": null, "summary": "Machine unlearning seeks to remove unwanted information from trained models,\ninitially at the individual-sample level, but increasingly at the level of\nentire sub-populations. In many deployments, models must delete whole topical\ndomains to satisfy privacy, legal, or quality requirements, e.g., removing\nseveral users' posts under GDPR or copyrighted web content. Existing unlearning\ntools remain largely sample-oriented, and straightforward point deletion often\nleaves enough residual signal for downstream learners to recover the unwanted\ndomain. We introduce distributional unlearning, a data-centric, model-agnostic\nframework that asks: Given examples from an unwanted distribution and a\nretained distribution, what is the smallest set of points whose removal makes\nthe edited dataset far from the unwanted domain yet close to the retained one?\nUsing Kullback-Leibler divergence to quantify removal and preservation, we\nderive the exact Pareto frontier in the Gaussian case and prove that any model\nretrained on the edited data incurs log-loss shifts bounded by the divergence\nthresholds. We propose a simple distance-based selection rule satisfying these\nconstraints with a quadratic reduction in deletion budget compared to random\nremoval. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,\nand CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on\nretained performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7684\u9057\u5fd8\u6846\u67b6\uff0c\u65e8\u5728\u9ad8\u6548\u79fb\u9664\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u5206\u5e03\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u6570\u636e\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u5de5\u5177\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u6837\u672c\uff0c\u96be\u4ee5\u6ee1\u8db3\u9690\u79c1\u3001\u6cd5\u5f8b\u6216\u8d28\u91cf\u9700\u6c42\u4e2d\u5bf9\u6574\u4e2a\u4e3b\u9898\u57df\u7684\u5220\u9664\u8981\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8eKullback-Leibler\u6563\u5ea6\u7684\u5206\u5e03\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u7684Pareto\u524d\u6cbf\u548c\u8ddd\u79bb\u9009\u62e9\u89c4\u5219\u51cf\u5c11\u5220\u9664\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u968f\u673a\u5220\u9664\u51cf\u5c1115-72%\u7684\u5220\u9664\u91cf\uff0c\u4e14\u5bf9\u4fdd\u7559\u6570\u636e\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u5206\u5e03\u9057\u5fd8\u6846\u67b6\u4e3a\u9ad8\u6548\u79fb\u9664\u7279\u5b9a\u5206\u5e03\u6570\u636e\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14894", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14894", "abs": "https://arxiv.org/abs/2507.14894", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSASFT\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u5e76\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u610f\u5916\u4ee3\u7801\u5207\u6362\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u610f\u5916\u4ee3\u7801\u5207\u6362\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u54cd\u5e94\u53ef\u8bfb\u6027\u548c\u53ef\u7528\u6027\u4e0b\u964d\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u673a\u5236\u5206\u6790\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u4ee3\u7801\u5207\u6362\u95ee\u9898\uff0c\u63d0\u51faSASFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u63a7\u5236\u8bed\u8a00\u7279\u5f81\u7684\u9884\u6fc0\u6d3b\u503c\u3002", "result": "\u5728\u4e94\u79cd\u6a21\u578b\u548c\u4e09\u79cd\u8bed\u8a00\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSASFT\u5c06\u610f\u5916\u4ee3\u7801\u5207\u6362\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u5e76\u5728\u56db\u79cd\u60c5\u51b5\u4e0b\u5b8c\u5168\u6d88\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\u3002", "conclusion": "SASFT\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u5207\u6362\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2507.14657", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "FST.ai\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4f53\u80b2\u88c1\u5224\u7684\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8dc6\u62f3\u9053\u4e2d\u5b9e\u65f6\u5934\u90e8\u8e22\u51fb\u68c0\u6d4b\u548c\u8bc4\u5206\u3002", "motivation": "\u4f20\u7edf\u88c1\u5224\u7cfb\u7edf\u5b58\u5728\u5ef6\u8fdf\u3001\u4e3b\u89c2\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u516c\u5e73\u6027\u548c\u8fd0\u52a8\u5458\u4fe1\u4efb\u3002", "method": "\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u8fb9\u7f18\u63a8\u7406\uff0c\u81ea\u52a8\u5316\u52a8\u4f5c\u8bc6\u522b\u4e0e\u5206\u7c7b\u3002", "result": "\u663e\u8457\u51cf\u5c11\u51b3\u7b56\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "FST.ai\u6846\u67b6\u5177\u6709\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u9879\u76ee\u3002"}}
